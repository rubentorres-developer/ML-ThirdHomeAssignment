{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190fb39a",
   "metadata": {},
   "source": [
    "# Machine Learning 2023/2024\n",
    "\n",
    "## Third Home Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df32ad",
   "metadata": {},
   "source": [
    "**Group Number:** 10\n",
    "\n",
    "**Group Elements:**\n",
    "- André Santos (fc53323)\n",
    "- Filipe Santos (fc53304)\n",
    "- João Martins (fc62532)\n",
    "- Rúben Torres (fc62531)\n",
    "\n",
    "**Hours Worked:**\n",
    "- André Santos (10h)\n",
    "- Filipe Santos (10h)\n",
    "- João Martins (10h)\n",
    "- Rúben Torres (10h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c477d4",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccfa3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "#from statsmodels.api import OLS, add_constant\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, max_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import (PowerTransformer)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da94a3b",
   "metadata": {},
   "source": [
    "### Loading and understanding the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598351a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df = pd.read_pickle(\"drd2_data.pickle\")\n",
    "#unpickled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d77ed14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7337, 2132)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(unpickled_df[0]))\n",
    "unpickled_df[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e8122",
   "metadata": {},
   "source": [
    "Data is splitted upon loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0cb614",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_ivs, y_train, col_names = pickle.load(open(\"drd2_data.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc145a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in (col_names):\n",
    "#    print(\"coluna: \", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130a27dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7337, 2132)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63e19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.tofile('X_train.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64ef73d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(816, 2132)\n"
     ]
    }
   ],
   "source": [
    "print(X_ivs.shape)\n",
    "#X_ivs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431538c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7337,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec9d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_to_discard = []\n",
    "#for col in range (X_train.shape[1]):\n",
    "#    print(\"coluna: \", col_names[col])\n",
    "#    proportion = (X_train[: col] == 0).mean() * 100\n",
    "#    if proportion > 0:\n",
    "#            print(f\"Proportion of missing values in column { col }: { round(proportion, 2) }%\")\n",
    "#        cols_to_discard.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51bc4e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2123</th>\n",
       "      <th>2124</th>\n",
       "      <th>2125</th>\n",
       "      <th>2126</th>\n",
       "      <th>2127</th>\n",
       "      <th>2128</th>\n",
       "      <th>2129</th>\n",
       "      <th>2130</th>\n",
       "      <th>2131</th>\n",
       "      <th>2132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.654947</td>\n",
       "      <td>541.280138</td>\n",
       "      <td>541.656</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.649995</td>\n",
       "      <td>426.197714</td>\n",
       "      <td>426.582</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.154947</td>\n",
       "      <td>348.183778</td>\n",
       "      <td>348.446</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.616176</td>\n",
       "      <td>1455.763803</td>\n",
       "      <td>1456.831</td>\n",
       "      <td>27.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.359725</td>\n",
       "      <td>387.151368</td>\n",
       "      <td>387.886</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>467.149047</td>\n",
       "      <td>467.513</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7333</th>\n",
       "      <td>0.002193</td>\n",
       "      <td>240.162649</td>\n",
       "      <td>240.350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7334</th>\n",
       "      <td>0.293481</td>\n",
       "      <td>510.317874</td>\n",
       "      <td>510.802</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7335</th>\n",
       "      <td>0.596804</td>\n",
       "      <td>393.187483</td>\n",
       "      <td>393.556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7336</th>\n",
       "      <td>0.776976</td>\n",
       "      <td>484.056123</td>\n",
       "      <td>485.462</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7337 rows × 2133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0            1         2     3     4     5     6     7      8     \\\n",
       "0     0.654947   541.280138   541.656  10.0   1.0   8.0   1.0  10.0   40.0   \n",
       "1     0.649995   426.197714   426.582   5.0   1.0   9.0   1.0   4.0   30.0   \n",
       "2     0.154947   348.183778   348.446   4.0   0.0   3.0   0.0   3.0   26.0   \n",
       "3     0.616176  1455.763803  1456.831  27.0  19.0  23.0  17.0  16.0  105.0   \n",
       "4     0.359725   387.151368   387.886   4.0   0.0   4.0   0.0   4.0   27.0   \n",
       "...        ...          ...       ...   ...   ...   ...   ...   ...    ...   \n",
       "7332  0.000000   467.149047   467.513   6.0   0.0   6.0   0.0   5.0   32.0   \n",
       "7333  0.002193   240.162649   240.350   2.0   0.0   3.0   0.0   2.0   18.0   \n",
       "7334  0.293481   510.317874   510.802   4.0   0.0  10.0   0.0   4.0   37.0   \n",
       "7335  0.596804   393.187483   393.556   4.0   2.0   5.0   1.0   5.0   28.0   \n",
       "7336  0.776976   484.056123   485.462   6.0   1.0   7.0   1.0   6.0   30.0   \n",
       "\n",
       "       9     ...  2123  2124  2125  2126  2127  2128  2129  2130  2131  2132  \n",
       "0      75.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1      60.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2      50.0  ...   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3     206.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0  \n",
       "4      50.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...     ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "7332   56.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7333   38.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "7334   79.0  ...   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7335   55.0  ...   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7336   52.0  ...   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[7337 rows x 2133 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N,M=X_train.shape\n",
    "N,M\n",
    "v=np.hstack((y_train.reshape((N,1)), X_train))\n",
    "pd.DataFrame(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd6c76-8b8d-4e79-b6a7-2808c2faaae9",
   "metadata": {},
   "source": [
    "<h3>Data Treatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "272f6f3c-e1e3-4b37-aa43-ad4cfd28c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#powerscaler not ideal for this dataset since it deletes outliers and we believe that the outliers shouldn't be removed from this dataset\n",
    "#scaler = PowerTransformer().fit(X_train)\n",
    "\n",
    "sScaler = StandardScaler()\n",
    "X_train_scaled = sScaler.fit_transform(X_train)\n",
    "X_ivs_scaled = sScaler.transform(X_ivs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc9cff66-ac2c-4074-826c-12ef2947c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.unique(X_train_scaled[: 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90cfa218-0b72-4752-8506-41bb13e457f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_scaled.tofile('X_train_scaled.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82245a31-69ad-46bd-993d-b827fc1cefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pode não fazer sentido fazer o scale dos dados cortados. Talvez faça mais sentido fazer o corte sobre os dados já \"scaled\"\n",
    "#X_train_cut_scaled = sScaler.fit_transform(X_train_cut)\n",
    "#X_ivs_cut_scaled = sScaler.transform(X_ivs_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf635db9-0490-45b9-9fe7-0c60bf8c3e47",
   "metadata": {},
   "source": [
    "<h3>selecting features by droping the features with correlation score bellow 0.05 to the dependant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaa85ca2-8d13-4e1f-a9e7-aaa4b10d9923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2123</th>\n",
       "      <th>2124</th>\n",
       "      <th>2125</th>\n",
       "      <th>2126</th>\n",
       "      <th>2127</th>\n",
       "      <th>2128</th>\n",
       "      <th>2129</th>\n",
       "      <th>2130</th>\n",
       "      <th>2131</th>\n",
       "      <th>2132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.654947</td>\n",
       "      <td>0.609921</td>\n",
       "      <td>0.608648</td>\n",
       "      <td>1.098414</td>\n",
       "      <td>-0.081854</td>\n",
       "      <td>0.246156</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>1.908209</td>\n",
       "      <td>0.737657</td>\n",
       "      <td>0.614996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.649995</td>\n",
       "      <td>-0.023790</td>\n",
       "      <td>-0.024513</td>\n",
       "      <td>-0.103427</td>\n",
       "      <td>-0.081854</td>\n",
       "      <td>0.413306</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>-0.243126</td>\n",
       "      <td>-0.038629</td>\n",
       "      <td>0.032423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.154947</td>\n",
       "      <td>-0.453381</td>\n",
       "      <td>-0.454433</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.589590</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.601682</td>\n",
       "      <td>-0.349144</td>\n",
       "      <td>-0.355960</td>\n",
       "      <td>...</td>\n",
       "      <td>1.234467</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.616176</td>\n",
       "      <td>5.645607</td>\n",
       "      <td>5.644129</td>\n",
       "      <td>5.184672</td>\n",
       "      <td>7.039406</td>\n",
       "      <td>2.753396</td>\n",
       "      <td>7.153978</td>\n",
       "      <td>4.059544</td>\n",
       "      <td>5.783518</td>\n",
       "      <td>5.702803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>5.029661</td>\n",
       "      <td>3.091413</td>\n",
       "      <td>5.116060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.359725</td>\n",
       "      <td>-0.238802</td>\n",
       "      <td>-0.237426</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.422441</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.243126</td>\n",
       "      <td>-0.271515</td>\n",
       "      <td>-0.355960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201712</td>\n",
       "      <td>0.200698</td>\n",
       "      <td>0.136941</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.088142</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>0.115430</td>\n",
       "      <td>0.116628</td>\n",
       "      <td>-0.122930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7333</th>\n",
       "      <td>0.002193</td>\n",
       "      <td>-1.048209</td>\n",
       "      <td>-1.049199</td>\n",
       "      <td>-0.824531</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.589590</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.960238</td>\n",
       "      <td>-0.970173</td>\n",
       "      <td>-0.822018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>3.091413</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7334</th>\n",
       "      <td>0.293481</td>\n",
       "      <td>0.439425</td>\n",
       "      <td>0.438883</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>0.580455</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.243126</td>\n",
       "      <td>0.504771</td>\n",
       "      <td>0.770349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>3.210153</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7335</th>\n",
       "      <td>0.596804</td>\n",
       "      <td>-0.205564</td>\n",
       "      <td>-0.206229</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>0.313772</td>\n",
       "      <td>-0.255292</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>0.115430</td>\n",
       "      <td>-0.193886</td>\n",
       "      <td>-0.161769</td>\n",
       "      <td>...</td>\n",
       "      <td>1.234467</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7336</th>\n",
       "      <td>0.776976</td>\n",
       "      <td>0.294812</td>\n",
       "      <td>0.299457</td>\n",
       "      <td>0.136941</td>\n",
       "      <td>-0.081854</td>\n",
       "      <td>0.079007</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>0.473986</td>\n",
       "      <td>-0.038629</td>\n",
       "      <td>-0.278283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>5.116060</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7337 rows × 2133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.654947  0.609921  0.608648  1.098414 -0.081854  0.246156 -0.058057   \n",
       "1     0.649995 -0.023790 -0.024513 -0.103427 -0.081854  0.413306 -0.058057   \n",
       "2     0.154947 -0.453381 -0.454433 -0.343795 -0.477479 -0.589590 -0.508809   \n",
       "3     0.616176  5.645607  5.644129  5.184672  7.039406  2.753396  7.153978   \n",
       "4     0.359725 -0.238802 -0.237426 -0.343795 -0.477479 -0.422441 -0.508809   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7332  0.000000  0.201712  0.200698  0.136941 -0.477479 -0.088142 -0.508809   \n",
       "7333  0.002193 -1.048209 -1.049199 -0.824531 -0.477479 -0.589590 -0.508809   \n",
       "7334  0.293481  0.439425  0.438883 -0.343795 -0.477479  0.580455 -0.508809   \n",
       "7335  0.596804 -0.205564 -0.206229 -0.343795  0.313772 -0.255292 -0.058057   \n",
       "7336  0.776976  0.294812  0.299457  0.136941 -0.081854  0.079007 -0.058057   \n",
       "\n",
       "          7         8         9     ...      2123      2124      2125  \\\n",
       "0     1.908209  0.737657  0.614996  ... -0.810066 -0.195463 -0.179087   \n",
       "1    -0.243126 -0.038629  0.032423  ... -0.810066 -0.195463 -0.179087   \n",
       "2    -0.601682 -0.349144 -0.355960  ...  1.234467 -0.195463 -0.179087   \n",
       "3     4.059544  5.783518  5.702803  ... -0.810066 -0.195463 -0.179087   \n",
       "4    -0.243126 -0.271515 -0.355960  ... -0.810066 -0.195463 -0.179087   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7332  0.115430  0.116628 -0.122930  ... -0.810066 -0.195463 -0.179087   \n",
       "7333 -0.960238 -0.970173 -0.822018  ... -0.810066 -0.195463 -0.179087   \n",
       "7334 -0.243126  0.504771  0.770349  ... -0.810066 -0.195463 -0.179087   \n",
       "7335  0.115430 -0.193886 -0.161769  ...  1.234467 -0.195463 -0.179087   \n",
       "7336  0.473986 -0.038629 -0.278283  ... -0.810066  5.116060 -0.179087   \n",
       "\n",
       "          2126      2127      2128      2129      2130      2131      2132  \n",
       "0    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "1    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "2    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "3    -0.311512 -0.168687 -0.105656 -0.136394  5.029661  3.091413  5.116060  \n",
       "4    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7332 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "7333 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821  3.091413 -0.195463  \n",
       "7334  3.210153 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "7335 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "7336 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "\n",
       "[7337 rows x 2133 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new correlation matrix with scaled data\n",
    "v_scaled=np.hstack((y_train.reshape((N,1)), X_train_scaled))\n",
    "pd.DataFrame(v_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4854909b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2123</th>\n",
       "      <th>2124</th>\n",
       "      <th>2125</th>\n",
       "      <th>2126</th>\n",
       "      <th>2127</th>\n",
       "      <th>2128</th>\n",
       "      <th>2129</th>\n",
       "      <th>2130</th>\n",
       "      <th>2131</th>\n",
       "      <th>2132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.160182</td>\n",
       "      <td>0.160216</td>\n",
       "      <td>0.130117</td>\n",
       "      <td>0.084939</td>\n",
       "      <td>0.151968</td>\n",
       "      <td>0.095929</td>\n",
       "      <td>0.126690</td>\n",
       "      <td>0.157545</td>\n",
       "      <td>0.165189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011360</td>\n",
       "      <td>0.095138</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>-0.021728</td>\n",
       "      <td>-0.012977</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.031884</td>\n",
       "      <td>-0.027774</td>\n",
       "      <td>0.058370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.160182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.919389</td>\n",
       "      <td>0.772557</td>\n",
       "      <td>0.888252</td>\n",
       "      <td>0.784495</td>\n",
       "      <td>0.856242</td>\n",
       "      <td>0.992766</td>\n",
       "      <td>0.973177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213406</td>\n",
       "      <td>0.019559</td>\n",
       "      <td>0.127229</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>-0.017119</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.212881</td>\n",
       "      <td>0.342831</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.219727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.160216</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919128</td>\n",
       "      <td>0.772379</td>\n",
       "      <td>0.888072</td>\n",
       "      <td>0.784320</td>\n",
       "      <td>0.855980</td>\n",
       "      <td>0.992623</td>\n",
       "      <td>0.972956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213435</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.127309</td>\n",
       "      <td>0.117351</td>\n",
       "      <td>-0.017201</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.212889</td>\n",
       "      <td>0.342785</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>0.219646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130117</td>\n",
       "      <td>0.919389</td>\n",
       "      <td>0.919128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838103</td>\n",
       "      <td>0.863232</td>\n",
       "      <td>0.843652</td>\n",
       "      <td>0.931528</td>\n",
       "      <td>0.922469</td>\n",
       "      <td>0.905146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196534</td>\n",
       "      <td>-0.008557</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>0.064926</td>\n",
       "      <td>-0.026235</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.181134</td>\n",
       "      <td>0.332638</td>\n",
       "      <td>0.137266</td>\n",
       "      <td>0.205303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.084939</td>\n",
       "      <td>0.772557</td>\n",
       "      <td>0.772379</td>\n",
       "      <td>0.838103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.704868</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>0.653117</td>\n",
       "      <td>0.774714</td>\n",
       "      <td>0.769215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185163</td>\n",
       "      <td>-0.032325</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>0.022735</td>\n",
       "      <td>-0.013150</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.110449</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.210139</td>\n",
       "      <td>0.231743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.036037</td>\n",
       "      <td>0.047958</td>\n",
       "      <td>0.038387</td>\n",
       "      <td>0.059813</td>\n",
       "      <td>0.061989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018420</td>\n",
       "      <td>-0.020652</td>\n",
       "      <td>-0.018922</td>\n",
       "      <td>-0.009945</td>\n",
       "      <td>-0.009870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063520</td>\n",
       "      <td>0.047193</td>\n",
       "      <td>0.032638</td>\n",
       "      <td>0.027846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.212881</td>\n",
       "      <td>0.212889</td>\n",
       "      <td>0.181134</td>\n",
       "      <td>0.110449</td>\n",
       "      <td>0.254921</td>\n",
       "      <td>0.115041</td>\n",
       "      <td>0.202605</td>\n",
       "      <td>0.213051</td>\n",
       "      <td>0.218212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>-0.015847</td>\n",
       "      <td>0.075295</td>\n",
       "      <td>-0.020981</td>\n",
       "      <td>-0.004391</td>\n",
       "      <td>0.063520</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>0.000372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>0.031884</td>\n",
       "      <td>0.342831</td>\n",
       "      <td>0.342785</td>\n",
       "      <td>0.332638</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.276091</td>\n",
       "      <td>0.410854</td>\n",
       "      <td>0.233237</td>\n",
       "      <td>0.342548</td>\n",
       "      <td>0.342609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.017914</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>-0.004214</td>\n",
       "      <td>-0.020504</td>\n",
       "      <td>0.047193</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.123067</td>\n",
       "      <td>0.127682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>-0.027774</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>0.137266</td>\n",
       "      <td>0.210139</td>\n",
       "      <td>0.177572</td>\n",
       "      <td>0.216401</td>\n",
       "      <td>0.081396</td>\n",
       "      <td>0.184924</td>\n",
       "      <td>0.190152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>-0.026145</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>0.051670</td>\n",
       "      <td>-0.029027</td>\n",
       "      <td>0.032638</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>0.123067</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>0.058370</td>\n",
       "      <td>0.219727</td>\n",
       "      <td>0.219646</td>\n",
       "      <td>0.205303</td>\n",
       "      <td>0.231743</td>\n",
       "      <td>0.119145</td>\n",
       "      <td>0.240242</td>\n",
       "      <td>0.151050</td>\n",
       "      <td>0.226627</td>\n",
       "      <td>0.218560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>-0.025196</td>\n",
       "      <td>-0.024145</td>\n",
       "      <td>0.027846</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.127682</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2133 rows × 2133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     1.000000  0.160182  0.160216  0.130117  0.084939  0.151968  0.095929   \n",
       "1     0.160182  1.000000  0.999998  0.919389  0.772557  0.888252  0.784495   \n",
       "2     0.160216  0.999998  1.000000  0.919128  0.772379  0.888072  0.784320   \n",
       "3     0.130117  0.919389  0.919128  1.000000  0.838103  0.863232  0.843652   \n",
       "4     0.084939  0.772557  0.772379  0.838103  1.000000  0.704868  0.993448   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2128  0.001865  0.053576  0.053498  0.043314  0.045537  0.036037  0.047958   \n",
       "2129  0.012468  0.212881  0.212889  0.181134  0.110449  0.254921  0.115041   \n",
       "2130  0.031884  0.342831  0.342785  0.332638  0.409722  0.276091  0.410854   \n",
       "2131 -0.027774  0.173077  0.173016  0.137266  0.210139  0.177572  0.216401   \n",
       "2132  0.058370  0.219727  0.219646  0.205303  0.231743  0.119145  0.240242   \n",
       "\n",
       "          7         8         9     ...      2123      2124      2125  \\\n",
       "0     0.126690  0.157545  0.165189  ...  0.011360  0.095138  0.078900   \n",
       "1     0.856242  0.992766  0.973177  ...  0.213406  0.019559  0.127229   \n",
       "2     0.855980  0.992623  0.972956  ...  0.213435  0.019583  0.127309   \n",
       "3     0.931528  0.922469  0.905146  ...  0.196534 -0.008557  0.095891   \n",
       "4     0.653117  0.774714  0.769215  ...  0.185163 -0.032325  0.083849   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2128  0.038387  0.059813  0.061989  ...  0.018420 -0.020652 -0.018922   \n",
       "2129  0.202605  0.213051  0.218212  ...  0.022700 -0.015847  0.075295   \n",
       "2130  0.233237  0.342548  0.342609  ...  0.006494  0.017914  0.009569   \n",
       "2131  0.081396  0.184924  0.190152  ...  0.004409 -0.026145  0.009127   \n",
       "2132  0.151050  0.226627  0.218560  ...  0.004474  0.000246 -0.018317   \n",
       "\n",
       "          2126      2127      2128      2129      2130      2131      2132  \n",
       "0    -0.021728 -0.012977  0.001865  0.012468  0.031884 -0.027774  0.058370  \n",
       "1     0.117371 -0.017119  0.053576  0.212881  0.342831  0.173077  0.219727  \n",
       "2     0.117351 -0.017201  0.053498  0.212889  0.342785  0.173016  0.219646  \n",
       "3     0.064926 -0.026235  0.043314  0.181134  0.332638  0.137266  0.205303  \n",
       "4     0.022735 -0.013150  0.045537  0.110449  0.409722  0.210139  0.231743  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2128 -0.009945 -0.009870  1.000000  0.063520  0.047193  0.032638  0.027846  \n",
       "2129 -0.020981 -0.004391  0.063520  1.000000  0.026101  0.049729  0.000372  \n",
       "2130 -0.004214 -0.020504  0.047193  0.026101  1.000000  0.123067  0.127682  \n",
       "2131  0.051670 -0.029027  0.032638  0.049729  0.123067  1.000000  0.082630  \n",
       "2132 -0.025196 -0.024145  0.027846  0.000372  0.127682  0.082630  1.000000  \n",
       "\n",
       "[2133 rows x 2133 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.corrcoef(v_scaled.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b92368f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = pd.DataFrame(np.corrcoef(v_scaled.T))\n",
    "\n",
    "# Initialize an empty list to store line numbers and values\n",
    "filtered_values_per_line = []\n",
    "\n",
    "# Iterate through the DataFrame and store line numbers and values\n",
    "#for i, row in enumerate(corr_matrix.values):\n",
    "#    line_values = [(j, value) for j, value in enumerate(row) if abs(value) > 0. and abs(value) < 1]\n",
    "#    if line_values:\n",
    "#        filtered_values_per_line.append(line_values)\n",
    "\n",
    "# Display the filtered values for each line\n",
    "#for line_number, values in enumerate(filtered_values_per_line):\n",
    "#    print(f\"Line {line_number}: {values}\")\n",
    "   \n",
    "for row, value in enumerate(corr_matrix.values[0, 1:]):\n",
    "    if value > 0.05:\n",
    "        line_values = (row, value)\n",
    "        filtered_values_per_line.append(line_values)\n",
    "\n",
    "#for values, line_number in enumerate(filtered_values_per_line):\n",
    "    #print(f\"Line {line_number}: {values}\")\n",
    "\n",
    "len(filtered_values_per_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08cfb4a-f46b-48d0-a0f4-841ab9f71337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1758"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list to store line numbers and values\n",
    "filtered_values_per_line = []\n",
    "\n",
    "for row, value in enumerate(corr_matrix.values[0, 1:]):\n",
    "    if value < 0.05: #0.05 to have a total of 374 features\n",
    "        line_values = (row, value)\n",
    "        filtered_values_per_line.append(line_values)\n",
    "\n",
    "features_to_remove = []\n",
    "for values, line_number in enumerate(filtered_values_per_line):\n",
    "    #print(f\"Feature {line_number}: {values}\")\n",
    "    features_to_remove.append(line_number[0])\n",
    "\n",
    "len(filtered_values_per_line)\n",
    "#print (features_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8435ada5-e330-4eb6-9c6b-97fb6da18e5f",
   "metadata": {},
   "source": [
    "<h4>New X-train with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a5bda67-9628-4370-a4d8-e34edf6cc899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7337, 374)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cut= np.delete(X_train_scaled, features_to_remove, 1)\n",
    "\n",
    "N_cut,M_cut=X_train_cut.shape\n",
    "X_train_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "574ceca3-4ba3-45e9-9ea4-bb69f7108427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(816, 374)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ivs_cut= np.delete(X_ivs_scaled, features_to_remove, 1)\n",
    "\n",
    "X_ivs_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "084e4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.unique(X_train[: 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f5b72",
   "metadata": {},
   "source": [
    "<h3>Data Treatment redundant (ignore this section, to delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cf213e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#powerscaler not ideal for this dataset since it deletes outliers and we believe that the outliers shouldn't be removed from this dataset\n",
    "#scaler = PowerTransformer().fit(X_train)\n",
    "\n",
    "#sScaler = StandardScaler()\n",
    "#X_train_scaled = sScaler.fit_transform(X_train)\n",
    "#X_ivs_scaled = sScaler.transform(X_ivs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd0b7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.unique(X_train_scaled[: 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad84ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_scaled.tofile('X_train_scaled.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b05a620-75fb-443c-9752-c3bea6e038da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pode não fazer sentido fazer o scale dos dados cortados. Talvez faça mais sentido fazer o corte sobre os dados já \"scaled\"\n",
    "#X_train_cut_scaled = sScaler.fit_transform(X_train_cut)\n",
    "#X_ivs_cut_scaled = sScaler.transform(X_ivs_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0975c",
   "metadata": {},
   "source": [
    "<h3>Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e949059",
   "metadata": {},
   "source": [
    "<h4>Stepwise fowards feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "745da56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Discard. This approach of feature selection takes to long on big datasets (7000+, 300+)\n",
    "\n",
    "#N,M=X_train_cut.shape\n",
    "\n",
    "#using linear regression for sequential feature selection\n",
    "#lmr=LinearRegression()\n",
    "#sfs = SequentialFeatureSelector(lmr, n_features_to_select=50, n_jobs=16)\n",
    "#sfs.fit(X_train_cut, y_train)\n",
    "\n",
    "#get the relevant columns\n",
    "#features=sfs.get_support()\n",
    "#Features_selected =np.arange(M)[features]\n",
    "#print(\"The features selected are columns: \", Features_selected)\n",
    "\n",
    "#X_train_cut_sfs=sfs.transform(X_train_cut)\n",
    "#X_test_cut_sfs=sfs.transform(X_ivs_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ca76c",
   "metadata": {},
   "source": [
    "<h4>RFs for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3399dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rffeatureSelect(x_train, x_ivs, n_features, t):\n",
    "    rfr=RandomForestRegressor(random_state=0, n_jobs=6) #n_jobs=8\n",
    "    rfr.fit(x_train, y_train)\n",
    "    #for i, imp in enumerate(rfr.feature_importances_):\n",
    "    #    print(\"Feature\", i, \"Importance:\", imp )\n",
    "    \n",
    "    sel = SelectFromModel(estimator=rfr, threshold= t) #Change the threshold! See what happens!\n",
    "    sel.fit(x_train, y_train)\n",
    "    \n",
    "    #print(\"Importances: \", sel.estimator.feature_importances_)\n",
    "    \n",
    "    #print(\"Default threshold: \", sel.threshold_)\n",
    "    \n",
    "    features=sel.get_support()\n",
    "    Features_selected =np.arange(n_features)[features]\n",
    "    print(\"The features selected are columns: \", Features_selected,\".\\n Number of features:\", len(Features_selected))\n",
    "    \n",
    "    X_train_rffs=sel.transform(x_train)\n",
    "    X_test_rffs=sel.transform(x_ivs)\n",
    "    return X_train_rffs, X_test_rffs\n",
    "#naif_model_testingR(nX_train_cut, nX_test_cut, y_train, y_ivs) # y_ivs doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80b03c65-0ab8-449e-b591-ea796349dee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features selected are columns:  [   0    1    2    3    4    5    6    8    9   11   12   16   22   23\n",
      "   24   25   26   27   28   29   30   31   32   33   34   35   36   37\n",
      "   38   39   40   41   42  216  246  287  316  336  348  353  362  364\n",
      "  444  494  561  695  819  839  853  886  891  909  939  959  981  982\n",
      " 1013 1054 1087 1147 1161 1167 1176 1285 1440 1454 1499 1527 1598 1667\n",
      " 1687 1709 1841 1868 1921 1922 1968] .\n",
      " Number of features: 77\n"
     ]
    }
   ],
   "source": [
    "#takes around 5 min to execute\n",
    "X_train_rffs, X_test_rffs = rffeatureSelect(X_train, X_ivs, M, .002)#.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4356313-917f-4f30-be4c-a31afc006403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features selected are columns:  [   0    1    2    3    4    5    6    8    9   11   12   16   22   23\n",
      "   24   25   26   27   28   29   30   31   32   33   34   35   36   37\n",
      "   38   39   40   41   42  216  246  287  316  336  348  353  362  364\n",
      "  444  494  561  819  839  853  886  891  909  939  959  981  982 1013\n",
      " 1054 1087 1147 1161 1167 1176 1285 1440 1454 1499 1527 1598 1667 1687\n",
      " 1709 1841 1868 1921 1922 1968] .\n",
      " Number of features: 76\n"
     ]
    }
   ],
   "source": [
    "#takes around 5 min to execute\n",
    "#Delete? probably not, since its possible to feature select in unscaled dataset, but it makes no sense to scale it after the feature selection is made.\n",
    "X_train_scaled_rffs, X_test_scaled_rffs = rffeatureSelect(X_train_scaled, X_ivs_scaled, M, .002)#.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b6a5f93-2c17-4180-ad10-6a5e1004d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features selected are columns:  [  0   1   3   4   5   6   8   9  11  16  19  20  21  22  23  24  25  26\n",
      "  27  28  29  30  31  32  33  34  35  36  50  68  74  78  79  90  92 102\n",
      " 140 161 167 178 187 192 201 240 267 272 298 299 306 327 343] .\n",
      " Number of features: 51\n"
     ]
    }
   ],
   "source": [
    "X_train_cut_rffs, X_test_cut_rffs= rffeatureSelect(X_train_cut, X_ivs_cut, M_cut, .005)#.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1580a5",
   "metadata": {},
   "source": [
    "<h4>Principal component analisys (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49f9a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(x_train, n_comps):\n",
    "    pca = PCA(n_components=n_comps)\n",
    "    pca.fit(x_train)\n",
    "    tve=0\n",
    "    for i, ve in enumerate(pca.explained_variance_ratio_):\n",
    "        tve+=ve\n",
    "        print(\"PC%d - Variance explained: %7.4f - Total Variance: %7.4f\" % (i, ve, tve) )\n",
    "    #print()\n",
    "    #print(\"Actual Eigenvalues:\", pca.singular_values_)\n",
    "    #for i,comp in enumerate(pca.components_):\n",
    "    #    print(\"PC\",i, \"-->\", comp)    \n",
    "    return pca\n",
    "\n",
    "def kpca(x_train, n_comps):\n",
    "    kpca = KernelPCA(n_components=n_comps, kernel='rbf')#, gamma=3)\n",
    "    kpca.fit(x_train)\n",
    "    X_train_kpca = kpca.transform(x_train)\n",
    "    return X_train_kpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6254d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.9869 - Total Variance:  0.9869\n",
      "PC1 - Variance explained:  0.0102 - Total Variance:  0.9971\n",
      "PC2 - Variance explained:  0.0014 - Total Variance:  0.9985\n",
      "PC3 - Variance explained:  0.0002 - Total Variance:  0.9988\n",
      "PC4 - Variance explained:  0.0001 - Total Variance:  0.9989\n",
      "PC5 - Variance explained:  0.0001 - Total Variance:  0.9990\n",
      "PC6 - Variance explained:  0.0001 - Total Variance:  0.9990\n",
      "PC7 - Variance explained:  0.0000 - Total Variance:  0.9991\n",
      "PC8 - Variance explained:  0.0000 - Total Variance:  0.9991\n",
      "PC9 - Variance explained:  0.0000 - Total Variance:  0.9991\n",
      "PC10 - Variance explained:  0.0000 - Total Variance:  0.9992\n",
      "PC11 - Variance explained:  0.0000 - Total Variance:  0.9992\n",
      "PC12 - Variance explained:  0.0000 - Total Variance:  0.9992\n",
      "PC13 - Variance explained:  0.0000 - Total Variance:  0.9992\n",
      "PC14 - Variance explained:  0.0000 - Total Variance:  0.9992\n",
      "PC15 - Variance explained:  0.0000 - Total Variance:  0.9992\n",
      "PC16 - Variance explained:  0.0000 - Total Variance:  0.9992\n",
      "PC17 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC18 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC19 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC20 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC21 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC22 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC23 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC24 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC25 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC26 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC27 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC28 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC29 - Variance explained:  0.0000 - Total Variance:  0.9993\n",
      "PC30 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC31 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC32 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC33 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC34 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC35 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC36 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC37 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC38 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC39 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC40 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC41 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC42 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC43 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC44 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC45 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC46 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC47 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC48 - Variance explained:  0.0000 - Total Variance:  0.9994\n",
      "PC49 - Variance explained:  0.0000 - Total Variance:  0.9994\n"
     ]
    }
   ],
   "source": [
    "pca1 = pca(X_train, 50) \n",
    "X_train_pca = pca1.transform(X_train)\n",
    "#X_test_pca=pca.transform(X_ivs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2aa0f9cf-c6d8-41cd-8a9a-b2e71f11fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_kpca =kpca(X_train,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "092add44-4336-4e85-bd1b-ad012cd63cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.0259 - Total Variance:  0.0259\n",
      "PC1 - Variance explained:  0.0131 - Total Variance:  0.0389\n",
      "PC2 - Variance explained:  0.0126 - Total Variance:  0.0515\n",
      "PC3 - Variance explained:  0.0116 - Total Variance:  0.0631\n",
      "PC4 - Variance explained:  0.0099 - Total Variance:  0.0730\n",
      "PC5 - Variance explained:  0.0077 - Total Variance:  0.0807\n",
      "PC6 - Variance explained:  0.0074 - Total Variance:  0.0881\n",
      "PC7 - Variance explained:  0.0068 - Total Variance:  0.0949\n",
      "PC8 - Variance explained:  0.0066 - Total Variance:  0.1016\n",
      "PC9 - Variance explained:  0.0064 - Total Variance:  0.1079\n",
      "PC10 - Variance explained:  0.0062 - Total Variance:  0.1141\n",
      "PC11 - Variance explained:  0.0059 - Total Variance:  0.1200\n",
      "PC12 - Variance explained:  0.0057 - Total Variance:  0.1258\n",
      "PC13 - Variance explained:  0.0054 - Total Variance:  0.1312\n",
      "PC14 - Variance explained:  0.0052 - Total Variance:  0.1364\n",
      "PC15 - Variance explained:  0.0051 - Total Variance:  0.1415\n",
      "PC16 - Variance explained:  0.0051 - Total Variance:  0.1465\n",
      "PC17 - Variance explained:  0.0048 - Total Variance:  0.1513\n",
      "PC18 - Variance explained:  0.0048 - Total Variance:  0.1561\n",
      "PC19 - Variance explained:  0.0047 - Total Variance:  0.1608\n",
      "PC20 - Variance explained:  0.0047 - Total Variance:  0.1655\n",
      "PC21 - Variance explained:  0.0046 - Total Variance:  0.1700\n",
      "PC22 - Variance explained:  0.0045 - Total Variance:  0.1745\n",
      "PC23 - Variance explained:  0.0044 - Total Variance:  0.1788\n",
      "PC24 - Variance explained:  0.0043 - Total Variance:  0.1832\n",
      "PC25 - Variance explained:  0.0043 - Total Variance:  0.1874\n",
      "PC26 - Variance explained:  0.0042 - Total Variance:  0.1916\n",
      "PC27 - Variance explained:  0.0042 - Total Variance:  0.1958\n",
      "PC28 - Variance explained:  0.0040 - Total Variance:  0.1997\n",
      "PC29 - Variance explained:  0.0040 - Total Variance:  0.2037\n",
      "PC30 - Variance explained:  0.0039 - Total Variance:  0.2077\n",
      "PC31 - Variance explained:  0.0038 - Total Variance:  0.2115\n",
      "PC32 - Variance explained:  0.0037 - Total Variance:  0.2152\n",
      "PC33 - Variance explained:  0.0037 - Total Variance:  0.2189\n",
      "PC34 - Variance explained:  0.0037 - Total Variance:  0.2226\n",
      "PC35 - Variance explained:  0.0036 - Total Variance:  0.2262\n",
      "PC36 - Variance explained:  0.0035 - Total Variance:  0.2297\n",
      "PC37 - Variance explained:  0.0035 - Total Variance:  0.2332\n",
      "PC38 - Variance explained:  0.0035 - Total Variance:  0.2367\n",
      "PC39 - Variance explained:  0.0034 - Total Variance:  0.2401\n",
      "PC40 - Variance explained:  0.0033 - Total Variance:  0.2434\n",
      "PC41 - Variance explained:  0.0033 - Total Variance:  0.2467\n",
      "PC42 - Variance explained:  0.0033 - Total Variance:  0.2500\n",
      "PC43 - Variance explained:  0.0032 - Total Variance:  0.2532\n",
      "PC44 - Variance explained:  0.0031 - Total Variance:  0.2563\n",
      "PC45 - Variance explained:  0.0031 - Total Variance:  0.2594\n",
      "PC46 - Variance explained:  0.0031 - Total Variance:  0.2625\n",
      "PC47 - Variance explained:  0.0030 - Total Variance:  0.2655\n",
      "PC48 - Variance explained:  0.0030 - Total Variance:  0.2685\n",
      "PC49 - Variance explained:  0.0030 - Total Variance:  0.2715\n",
      "PC50 - Variance explained:  0.0029 - Total Variance:  0.2744\n",
      "PC51 - Variance explained:  0.0029 - Total Variance:  0.2773\n",
      "PC52 - Variance explained:  0.0028 - Total Variance:  0.2801\n",
      "PC53 - Variance explained:  0.0028 - Total Variance:  0.2830\n",
      "PC54 - Variance explained:  0.0028 - Total Variance:  0.2857\n",
      "PC55 - Variance explained:  0.0027 - Total Variance:  0.2885\n",
      "PC56 - Variance explained:  0.0027 - Total Variance:  0.2912\n",
      "PC57 - Variance explained:  0.0027 - Total Variance:  0.2939\n",
      "PC58 - Variance explained:  0.0027 - Total Variance:  0.2966\n",
      "PC59 - Variance explained:  0.0026 - Total Variance:  0.2992\n",
      "PC60 - Variance explained:  0.0026 - Total Variance:  0.3019\n",
      "PC61 - Variance explained:  0.0026 - Total Variance:  0.3044\n",
      "PC62 - Variance explained:  0.0026 - Total Variance:  0.3070\n",
      "PC63 - Variance explained:  0.0025 - Total Variance:  0.3095\n",
      "PC64 - Variance explained:  0.0025 - Total Variance:  0.3120\n",
      "PC65 - Variance explained:  0.0025 - Total Variance:  0.3145\n",
      "PC66 - Variance explained:  0.0025 - Total Variance:  0.3170\n",
      "PC67 - Variance explained:  0.0024 - Total Variance:  0.3194\n",
      "PC68 - Variance explained:  0.0024 - Total Variance:  0.3219\n",
      "PC69 - Variance explained:  0.0024 - Total Variance:  0.3243\n",
      "PC70 - Variance explained:  0.0024 - Total Variance:  0.3266\n",
      "PC71 - Variance explained:  0.0024 - Total Variance:  0.3290\n",
      "PC72 - Variance explained:  0.0023 - Total Variance:  0.3313\n",
      "PC73 - Variance explained:  0.0023 - Total Variance:  0.3337\n",
      "PC74 - Variance explained:  0.0023 - Total Variance:  0.3360\n",
      "PC75 - Variance explained:  0.0023 - Total Variance:  0.3383\n",
      "PC76 - Variance explained:  0.0023 - Total Variance:  0.3406\n",
      "PC77 - Variance explained:  0.0023 - Total Variance:  0.3428\n",
      "PC78 - Variance explained:  0.0022 - Total Variance:  0.3451\n",
      "PC79 - Variance explained:  0.0022 - Total Variance:  0.3473\n",
      "PC80 - Variance explained:  0.0022 - Total Variance:  0.3495\n",
      "PC81 - Variance explained:  0.0022 - Total Variance:  0.3517\n",
      "PC82 - Variance explained:  0.0022 - Total Variance:  0.3539\n",
      "PC83 - Variance explained:  0.0022 - Total Variance:  0.3561\n",
      "PC84 - Variance explained:  0.0021 - Total Variance:  0.3582\n",
      "PC85 - Variance explained:  0.0021 - Total Variance:  0.3603\n",
      "PC86 - Variance explained:  0.0021 - Total Variance:  0.3624\n",
      "PC87 - Variance explained:  0.0021 - Total Variance:  0.3645\n",
      "PC88 - Variance explained:  0.0021 - Total Variance:  0.3666\n",
      "PC89 - Variance explained:  0.0021 - Total Variance:  0.3687\n",
      "PC90 - Variance explained:  0.0021 - Total Variance:  0.3708\n",
      "PC91 - Variance explained:  0.0020 - Total Variance:  0.3728\n",
      "PC92 - Variance explained:  0.0020 - Total Variance:  0.3748\n",
      "PC93 - Variance explained:  0.0020 - Total Variance:  0.3768\n",
      "PC94 - Variance explained:  0.0020 - Total Variance:  0.3788\n",
      "PC95 - Variance explained:  0.0020 - Total Variance:  0.3808\n",
      "PC96 - Variance explained:  0.0020 - Total Variance:  0.3828\n",
      "PC97 - Variance explained:  0.0020 - Total Variance:  0.3848\n",
      "PC98 - Variance explained:  0.0020 - Total Variance:  0.3868\n",
      "PC99 - Variance explained:  0.0019 - Total Variance:  0.3887\n",
      "PC100 - Variance explained:  0.0019 - Total Variance:  0.3906\n",
      "PC101 - Variance explained:  0.0019 - Total Variance:  0.3925\n",
      "PC102 - Variance explained:  0.0019 - Total Variance:  0.3945\n",
      "PC103 - Variance explained:  0.0019 - Total Variance:  0.3963\n",
      "PC104 - Variance explained:  0.0019 - Total Variance:  0.3982\n",
      "PC105 - Variance explained:  0.0018 - Total Variance:  0.4001\n",
      "PC106 - Variance explained:  0.0018 - Total Variance:  0.4019\n",
      "PC107 - Variance explained:  0.0018 - Total Variance:  0.4037\n",
      "PC108 - Variance explained:  0.0018 - Total Variance:  0.4056\n",
      "PC109 - Variance explained:  0.0018 - Total Variance:  0.4074\n",
      "PC110 - Variance explained:  0.0018 - Total Variance:  0.4091\n",
      "PC111 - Variance explained:  0.0018 - Total Variance:  0.4109\n",
      "PC112 - Variance explained:  0.0018 - Total Variance:  0.4127\n",
      "PC113 - Variance explained:  0.0018 - Total Variance:  0.4144\n",
      "PC114 - Variance explained:  0.0017 - Total Variance:  0.4162\n",
      "PC115 - Variance explained:  0.0017 - Total Variance:  0.4179\n",
      "PC116 - Variance explained:  0.0017 - Total Variance:  0.4197\n",
      "PC117 - Variance explained:  0.0017 - Total Variance:  0.4214\n",
      "PC118 - Variance explained:  0.0017 - Total Variance:  0.4231\n",
      "PC119 - Variance explained:  0.0017 - Total Variance:  0.4248\n",
      "PC120 - Variance explained:  0.0017 - Total Variance:  0.4265\n",
      "PC121 - Variance explained:  0.0017 - Total Variance:  0.4281\n",
      "PC122 - Variance explained:  0.0017 - Total Variance:  0.4298\n",
      "PC123 - Variance explained:  0.0017 - Total Variance:  0.4315\n",
      "PC124 - Variance explained:  0.0016 - Total Variance:  0.4331\n",
      "PC125 - Variance explained:  0.0016 - Total Variance:  0.4347\n",
      "PC126 - Variance explained:  0.0016 - Total Variance:  0.4364\n",
      "PC127 - Variance explained:  0.0016 - Total Variance:  0.4380\n",
      "PC128 - Variance explained:  0.0016 - Total Variance:  0.4396\n",
      "PC129 - Variance explained:  0.0016 - Total Variance:  0.4412\n",
      "PC130 - Variance explained:  0.0016 - Total Variance:  0.4428\n",
      "PC131 - Variance explained:  0.0016 - Total Variance:  0.4444\n",
      "PC132 - Variance explained:  0.0016 - Total Variance:  0.4460\n",
      "PC133 - Variance explained:  0.0016 - Total Variance:  0.4475\n",
      "PC134 - Variance explained:  0.0016 - Total Variance:  0.4491\n",
      "PC135 - Variance explained:  0.0016 - Total Variance:  0.4507\n",
      "PC136 - Variance explained:  0.0015 - Total Variance:  0.4522\n",
      "PC137 - Variance explained:  0.0015 - Total Variance:  0.4537\n",
      "PC138 - Variance explained:  0.0015 - Total Variance:  0.4553\n",
      "PC139 - Variance explained:  0.0015 - Total Variance:  0.4568\n",
      "PC140 - Variance explained:  0.0015 - Total Variance:  0.4583\n",
      "PC141 - Variance explained:  0.0015 - Total Variance:  0.4598\n",
      "PC142 - Variance explained:  0.0015 - Total Variance:  0.4613\n",
      "PC143 - Variance explained:  0.0015 - Total Variance:  0.4628\n",
      "PC144 - Variance explained:  0.0015 - Total Variance:  0.4643\n",
      "PC145 - Variance explained:  0.0015 - Total Variance:  0.4657\n",
      "PC146 - Variance explained:  0.0015 - Total Variance:  0.4672\n",
      "PC147 - Variance explained:  0.0014 - Total Variance:  0.4686\n",
      "PC148 - Variance explained:  0.0014 - Total Variance:  0.4701\n",
      "PC149 - Variance explained:  0.0014 - Total Variance:  0.4715\n",
      "PC150 - Variance explained:  0.0014 - Total Variance:  0.4729\n",
      "PC151 - Variance explained:  0.0014 - Total Variance:  0.4743\n",
      "PC152 - Variance explained:  0.0014 - Total Variance:  0.4757\n",
      "PC153 - Variance explained:  0.0014 - Total Variance:  0.4771\n",
      "PC154 - Variance explained:  0.0014 - Total Variance:  0.4785\n",
      "PC155 - Variance explained:  0.0014 - Total Variance:  0.4799\n",
      "PC156 - Variance explained:  0.0014 - Total Variance:  0.4813\n",
      "PC157 - Variance explained:  0.0014 - Total Variance:  0.4826\n",
      "PC158 - Variance explained:  0.0014 - Total Variance:  0.4840\n",
      "PC159 - Variance explained:  0.0014 - Total Variance:  0.4853\n",
      "PC160 - Variance explained:  0.0013 - Total Variance:  0.4867\n",
      "PC161 - Variance explained:  0.0013 - Total Variance:  0.4880\n",
      "PC162 - Variance explained:  0.0013 - Total Variance:  0.4894\n",
      "PC163 - Variance explained:  0.0013 - Total Variance:  0.4907\n",
      "PC164 - Variance explained:  0.0013 - Total Variance:  0.4920\n",
      "PC165 - Variance explained:  0.0013 - Total Variance:  0.4933\n",
      "PC166 - Variance explained:  0.0013 - Total Variance:  0.4947\n",
      "PC167 - Variance explained:  0.0013 - Total Variance:  0.4960\n",
      "PC168 - Variance explained:  0.0013 - Total Variance:  0.4973\n",
      "PC169 - Variance explained:  0.0013 - Total Variance:  0.4986\n",
      "PC170 - Variance explained:  0.0013 - Total Variance:  0.4998\n",
      "PC171 - Variance explained:  0.0013 - Total Variance:  0.5011\n",
      "PC172 - Variance explained:  0.0013 - Total Variance:  0.5024\n",
      "PC173 - Variance explained:  0.0013 - Total Variance:  0.5036\n",
      "PC174 - Variance explained:  0.0013 - Total Variance:  0.5049\n",
      "PC175 - Variance explained:  0.0013 - Total Variance:  0.5062\n",
      "PC176 - Variance explained:  0.0013 - Total Variance:  0.5074\n",
      "PC177 - Variance explained:  0.0012 - Total Variance:  0.5087\n",
      "PC178 - Variance explained:  0.0012 - Total Variance:  0.5099\n",
      "PC179 - Variance explained:  0.0012 - Total Variance:  0.5111\n",
      "PC180 - Variance explained:  0.0012 - Total Variance:  0.5124\n",
      "PC181 - Variance explained:  0.0012 - Total Variance:  0.5136\n",
      "PC182 - Variance explained:  0.0012 - Total Variance:  0.5148\n",
      "PC183 - Variance explained:  0.0012 - Total Variance:  0.5160\n",
      "PC184 - Variance explained:  0.0012 - Total Variance:  0.5172\n",
      "PC185 - Variance explained:  0.0012 - Total Variance:  0.5184\n",
      "PC186 - Variance explained:  0.0012 - Total Variance:  0.5196\n",
      "PC187 - Variance explained:  0.0012 - Total Variance:  0.5208\n",
      "PC188 - Variance explained:  0.0012 - Total Variance:  0.5220\n",
      "PC189 - Variance explained:  0.0012 - Total Variance:  0.5231\n",
      "PC190 - Variance explained:  0.0012 - Total Variance:  0.5243\n",
      "PC191 - Variance explained:  0.0012 - Total Variance:  0.5255\n",
      "PC192 - Variance explained:  0.0012 - Total Variance:  0.5266\n",
      "PC193 - Variance explained:  0.0012 - Total Variance:  0.5278\n",
      "PC194 - Variance explained:  0.0012 - Total Variance:  0.5290\n",
      "PC195 - Variance explained:  0.0011 - Total Variance:  0.5301\n",
      "PC196 - Variance explained:  0.0011 - Total Variance:  0.5312\n",
      "PC197 - Variance explained:  0.0011 - Total Variance:  0.5324\n",
      "PC198 - Variance explained:  0.0011 - Total Variance:  0.5335\n",
      "PC199 - Variance explained:  0.0011 - Total Variance:  0.5346\n",
      "PC200 - Variance explained:  0.0011 - Total Variance:  0.5357\n",
      "PC201 - Variance explained:  0.0011 - Total Variance:  0.5368\n",
      "PC202 - Variance explained:  0.0011 - Total Variance:  0.5379\n",
      "PC203 - Variance explained:  0.0011 - Total Variance:  0.5390\n",
      "PC204 - Variance explained:  0.0011 - Total Variance:  0.5401\n",
      "PC205 - Variance explained:  0.0011 - Total Variance:  0.5412\n",
      "PC206 - Variance explained:  0.0011 - Total Variance:  0.5423\n",
      "PC207 - Variance explained:  0.0011 - Total Variance:  0.5434\n",
      "PC208 - Variance explained:  0.0011 - Total Variance:  0.5444\n",
      "PC209 - Variance explained:  0.0011 - Total Variance:  0.5455\n",
      "PC210 - Variance explained:  0.0011 - Total Variance:  0.5465\n",
      "PC211 - Variance explained:  0.0011 - Total Variance:  0.5476\n",
      "PC212 - Variance explained:  0.0010 - Total Variance:  0.5486\n",
      "PC213 - Variance explained:  0.0010 - Total Variance:  0.5497\n",
      "PC214 - Variance explained:  0.0010 - Total Variance:  0.5507\n",
      "PC215 - Variance explained:  0.0010 - Total Variance:  0.5517\n",
      "PC216 - Variance explained:  0.0010 - Total Variance:  0.5528\n",
      "PC217 - Variance explained:  0.0010 - Total Variance:  0.5538\n",
      "PC218 - Variance explained:  0.0010 - Total Variance:  0.5548\n",
      "PC219 - Variance explained:  0.0010 - Total Variance:  0.5558\n",
      "PC220 - Variance explained:  0.0010 - Total Variance:  0.5568\n",
      "PC221 - Variance explained:  0.0010 - Total Variance:  0.5578\n",
      "PC222 - Variance explained:  0.0010 - Total Variance:  0.5588\n",
      "PC223 - Variance explained:  0.0010 - Total Variance:  0.5598\n",
      "PC224 - Variance explained:  0.0010 - Total Variance:  0.5608\n",
      "PC225 - Variance explained:  0.0010 - Total Variance:  0.5618\n",
      "PC226 - Variance explained:  0.0010 - Total Variance:  0.5628\n",
      "PC227 - Variance explained:  0.0010 - Total Variance:  0.5637\n",
      "PC228 - Variance explained:  0.0010 - Total Variance:  0.5647\n",
      "PC229 - Variance explained:  0.0010 - Total Variance:  0.5657\n",
      "PC230 - Variance explained:  0.0010 - Total Variance:  0.5666\n",
      "PC231 - Variance explained:  0.0010 - Total Variance:  0.5676\n",
      "PC232 - Variance explained:  0.0009 - Total Variance:  0.5685\n",
      "PC233 - Variance explained:  0.0009 - Total Variance:  0.5695\n",
      "PC234 - Variance explained:  0.0009 - Total Variance:  0.5704\n",
      "PC235 - Variance explained:  0.0009 - Total Variance:  0.5713\n",
      "PC236 - Variance explained:  0.0009 - Total Variance:  0.5722\n",
      "PC237 - Variance explained:  0.0009 - Total Variance:  0.5732\n",
      "PC238 - Variance explained:  0.0009 - Total Variance:  0.5741\n",
      "PC239 - Variance explained:  0.0009 - Total Variance:  0.5750\n",
      "PC240 - Variance explained:  0.0009 - Total Variance:  0.5759\n",
      "PC241 - Variance explained:  0.0009 - Total Variance:  0.5768\n",
      "PC242 - Variance explained:  0.0009 - Total Variance:  0.5777\n",
      "PC243 - Variance explained:  0.0009 - Total Variance:  0.5786\n",
      "PC244 - Variance explained:  0.0009 - Total Variance:  0.5794\n",
      "PC245 - Variance explained:  0.0009 - Total Variance:  0.5803\n",
      "PC246 - Variance explained:  0.0009 - Total Variance:  0.5812\n",
      "PC247 - Variance explained:  0.0009 - Total Variance:  0.5821\n",
      "PC248 - Variance explained:  0.0009 - Total Variance:  0.5829\n",
      "PC249 - Variance explained:  0.0009 - Total Variance:  0.5838\n"
     ]
    }
   ],
   "source": [
    "pca_scaled = pca(X_train_scaled, 250) \n",
    "X_train_scaled_pca = pca_scaled.transform(X_train_scaled)\n",
    "#X_test_scaled_pca=pca_scaled.transform(X_ivs_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20afd6f6-5429-45c6-946b-2406a4e5bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_kpca =kpca(X_train_scaled,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67ec5b00-3398-4861-a1b2-afe4355c2ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.0862 - Total Variance:  0.0862\n",
      "PC1 - Variance explained:  0.0406 - Total Variance:  0.1268\n",
      "PC2 - Variance explained:  0.0323 - Total Variance:  0.1591\n",
      "PC3 - Variance explained:  0.0293 - Total Variance:  0.1884\n",
      "PC4 - Variance explained:  0.0272 - Total Variance:  0.2156\n",
      "PC5 - Variance explained:  0.0256 - Total Variance:  0.2412\n",
      "PC6 - Variance explained:  0.0245 - Total Variance:  0.2657\n",
      "PC7 - Variance explained:  0.0191 - Total Variance:  0.2848\n",
      "PC8 - Variance explained:  0.0169 - Total Variance:  0.3017\n",
      "PC9 - Variance explained:  0.0163 - Total Variance:  0.3180\n",
      "PC10 - Variance explained:  0.0137 - Total Variance:  0.3317\n",
      "PC11 - Variance explained:  0.0133 - Total Variance:  0.3450\n",
      "PC12 - Variance explained:  0.0124 - Total Variance:  0.3573\n",
      "PC13 - Variance explained:  0.0112 - Total Variance:  0.3685\n",
      "PC14 - Variance explained:  0.0109 - Total Variance:  0.3794\n",
      "PC15 - Variance explained:  0.0107 - Total Variance:  0.3901\n",
      "PC16 - Variance explained:  0.0099 - Total Variance:  0.4000\n",
      "PC17 - Variance explained:  0.0095 - Total Variance:  0.4095\n",
      "PC18 - Variance explained:  0.0089 - Total Variance:  0.4183\n",
      "PC19 - Variance explained:  0.0086 - Total Variance:  0.4269\n",
      "PC20 - Variance explained:  0.0083 - Total Variance:  0.4352\n",
      "PC21 - Variance explained:  0.0079 - Total Variance:  0.4431\n",
      "PC22 - Variance explained:  0.0074 - Total Variance:  0.4504\n",
      "PC23 - Variance explained:  0.0073 - Total Variance:  0.4577\n",
      "PC24 - Variance explained:  0.0071 - Total Variance:  0.4648\n",
      "PC25 - Variance explained:  0.0069 - Total Variance:  0.4718\n",
      "PC26 - Variance explained:  0.0066 - Total Variance:  0.4784\n",
      "PC27 - Variance explained:  0.0065 - Total Variance:  0.4849\n",
      "PC28 - Variance explained:  0.0063 - Total Variance:  0.4912\n",
      "PC29 - Variance explained:  0.0061 - Total Variance:  0.4973\n",
      "PC30 - Variance explained:  0.0060 - Total Variance:  0.5033\n",
      "PC31 - Variance explained:  0.0059 - Total Variance:  0.5092\n",
      "PC32 - Variance explained:  0.0056 - Total Variance:  0.5148\n",
      "PC33 - Variance explained:  0.0054 - Total Variance:  0.5202\n",
      "PC34 - Variance explained:  0.0052 - Total Variance:  0.5254\n",
      "PC35 - Variance explained:  0.0051 - Total Variance:  0.5305\n",
      "PC36 - Variance explained:  0.0051 - Total Variance:  0.5355\n",
      "PC37 - Variance explained:  0.0049 - Total Variance:  0.5405\n",
      "PC38 - Variance explained:  0.0047 - Total Variance:  0.5452\n",
      "PC39 - Variance explained:  0.0047 - Total Variance:  0.5498\n",
      "PC40 - Variance explained:  0.0046 - Total Variance:  0.5544\n",
      "PC41 - Variance explained:  0.0045 - Total Variance:  0.5589\n",
      "PC42 - Variance explained:  0.0043 - Total Variance:  0.5633\n",
      "PC43 - Variance explained:  0.0042 - Total Variance:  0.5675\n",
      "PC44 - Variance explained:  0.0042 - Total Variance:  0.5717\n",
      "PC45 - Variance explained:  0.0042 - Total Variance:  0.5758\n",
      "PC46 - Variance explained:  0.0040 - Total Variance:  0.5798\n",
      "PC47 - Variance explained:  0.0040 - Total Variance:  0.5838\n",
      "PC48 - Variance explained:  0.0039 - Total Variance:  0.5877\n",
      "PC49 - Variance explained:  0.0039 - Total Variance:  0.5916\n",
      "PC50 - Variance explained:  0.0038 - Total Variance:  0.5954\n",
      "PC51 - Variance explained:  0.0038 - Total Variance:  0.5991\n",
      "PC52 - Variance explained:  0.0037 - Total Variance:  0.6028\n",
      "PC53 - Variance explained:  0.0037 - Total Variance:  0.6065\n",
      "PC54 - Variance explained:  0.0036 - Total Variance:  0.6101\n",
      "PC55 - Variance explained:  0.0036 - Total Variance:  0.6138\n",
      "PC56 - Variance explained:  0.0035 - Total Variance:  0.6173\n",
      "PC57 - Variance explained:  0.0034 - Total Variance:  0.6207\n",
      "PC58 - Variance explained:  0.0034 - Total Variance:  0.6242\n",
      "PC59 - Variance explained:  0.0034 - Total Variance:  0.6276\n",
      "PC60 - Variance explained:  0.0033 - Total Variance:  0.6309\n",
      "PC61 - Variance explained:  0.0033 - Total Variance:  0.6342\n",
      "PC62 - Variance explained:  0.0033 - Total Variance:  0.6375\n",
      "PC63 - Variance explained:  0.0032 - Total Variance:  0.6407\n",
      "PC64 - Variance explained:  0.0032 - Total Variance:  0.6439\n",
      "PC65 - Variance explained:  0.0032 - Total Variance:  0.6471\n",
      "PC66 - Variance explained:  0.0031 - Total Variance:  0.6502\n",
      "PC67 - Variance explained:  0.0031 - Total Variance:  0.6533\n",
      "PC68 - Variance explained:  0.0030 - Total Variance:  0.6563\n",
      "PC69 - Variance explained:  0.0030 - Total Variance:  0.6593\n",
      "PC70 - Variance explained:  0.0030 - Total Variance:  0.6623\n",
      "PC71 - Variance explained:  0.0029 - Total Variance:  0.6652\n",
      "PC72 - Variance explained:  0.0029 - Total Variance:  0.6681\n",
      "PC73 - Variance explained:  0.0029 - Total Variance:  0.6710\n",
      "PC74 - Variance explained:  0.0029 - Total Variance:  0.6739\n",
      "PC75 - Variance explained:  0.0028 - Total Variance:  0.6767\n",
      "PC76 - Variance explained:  0.0028 - Total Variance:  0.6796\n",
      "PC77 - Variance explained:  0.0028 - Total Variance:  0.6824\n",
      "PC78 - Variance explained:  0.0027 - Total Variance:  0.6851\n",
      "PC79 - Variance explained:  0.0027 - Total Variance:  0.6878\n",
      "PC80 - Variance explained:  0.0027 - Total Variance:  0.6905\n",
      "PC81 - Variance explained:  0.0027 - Total Variance:  0.6932\n",
      "PC82 - Variance explained:  0.0026 - Total Variance:  0.6958\n",
      "PC83 - Variance explained:  0.0026 - Total Variance:  0.6985\n",
      "PC84 - Variance explained:  0.0026 - Total Variance:  0.7011\n",
      "PC85 - Variance explained:  0.0026 - Total Variance:  0.7036\n",
      "PC86 - Variance explained:  0.0026 - Total Variance:  0.7062\n",
      "PC87 - Variance explained:  0.0025 - Total Variance:  0.7087\n",
      "PC88 - Variance explained:  0.0025 - Total Variance:  0.7112\n",
      "PC89 - Variance explained:  0.0025 - Total Variance:  0.7137\n",
      "PC90 - Variance explained:  0.0025 - Total Variance:  0.7162\n",
      "PC91 - Variance explained:  0.0024 - Total Variance:  0.7186\n",
      "PC92 - Variance explained:  0.0024 - Total Variance:  0.7211\n",
      "PC93 - Variance explained:  0.0024 - Total Variance:  0.7234\n",
      "PC94 - Variance explained:  0.0024 - Total Variance:  0.7258\n",
      "PC95 - Variance explained:  0.0024 - Total Variance:  0.7282\n",
      "PC96 - Variance explained:  0.0023 - Total Variance:  0.7305\n",
      "PC97 - Variance explained:  0.0023 - Total Variance:  0.7328\n",
      "PC98 - Variance explained:  0.0023 - Total Variance:  0.7351\n",
      "PC99 - Variance explained:  0.0023 - Total Variance:  0.7374\n",
      "PC100 - Variance explained:  0.0022 - Total Variance:  0.7396\n",
      "PC101 - Variance explained:  0.0022 - Total Variance:  0.7419\n",
      "PC102 - Variance explained:  0.0022 - Total Variance:  0.7441\n",
      "PC103 - Variance explained:  0.0022 - Total Variance:  0.7463\n",
      "PC104 - Variance explained:  0.0022 - Total Variance:  0.7485\n",
      "PC105 - Variance explained:  0.0022 - Total Variance:  0.7506\n",
      "PC106 - Variance explained:  0.0021 - Total Variance:  0.7528\n",
      "PC107 - Variance explained:  0.0021 - Total Variance:  0.7549\n",
      "PC108 - Variance explained:  0.0021 - Total Variance:  0.7570\n",
      "PC109 - Variance explained:  0.0021 - Total Variance:  0.7591\n",
      "PC110 - Variance explained:  0.0021 - Total Variance:  0.7612\n",
      "PC111 - Variance explained:  0.0021 - Total Variance:  0.7632\n",
      "PC112 - Variance explained:  0.0020 - Total Variance:  0.7653\n",
      "PC113 - Variance explained:  0.0020 - Total Variance:  0.7673\n",
      "PC114 - Variance explained:  0.0020 - Total Variance:  0.7693\n",
      "PC115 - Variance explained:  0.0020 - Total Variance:  0.7713\n",
      "PC116 - Variance explained:  0.0020 - Total Variance:  0.7732\n",
      "PC117 - Variance explained:  0.0020 - Total Variance:  0.7752\n",
      "PC118 - Variance explained:  0.0019 - Total Variance:  0.7772\n",
      "PC119 - Variance explained:  0.0019 - Total Variance:  0.7791\n",
      "PC120 - Variance explained:  0.0019 - Total Variance:  0.7810\n",
      "PC121 - Variance explained:  0.0019 - Total Variance:  0.7829\n",
      "PC122 - Variance explained:  0.0019 - Total Variance:  0.7847\n",
      "PC123 - Variance explained:  0.0018 - Total Variance:  0.7866\n",
      "PC124 - Variance explained:  0.0018 - Total Variance:  0.7884\n",
      "PC125 - Variance explained:  0.0018 - Total Variance:  0.7903\n",
      "PC126 - Variance explained:  0.0018 - Total Variance:  0.7921\n",
      "PC127 - Variance explained:  0.0018 - Total Variance:  0.7939\n",
      "PC128 - Variance explained:  0.0018 - Total Variance:  0.7957\n",
      "PC129 - Variance explained:  0.0018 - Total Variance:  0.7975\n",
      "PC130 - Variance explained:  0.0018 - Total Variance:  0.7992\n",
      "PC131 - Variance explained:  0.0017 - Total Variance:  0.8010\n",
      "PC132 - Variance explained:  0.0017 - Total Variance:  0.8027\n",
      "PC133 - Variance explained:  0.0017 - Total Variance:  0.8044\n",
      "PC134 - Variance explained:  0.0017 - Total Variance:  0.8061\n",
      "PC135 - Variance explained:  0.0017 - Total Variance:  0.8078\n",
      "PC136 - Variance explained:  0.0017 - Total Variance:  0.8095\n",
      "PC137 - Variance explained:  0.0017 - Total Variance:  0.8112\n",
      "PC138 - Variance explained:  0.0016 - Total Variance:  0.8128\n",
      "PC139 - Variance explained:  0.0016 - Total Variance:  0.8145\n",
      "PC140 - Variance explained:  0.0016 - Total Variance:  0.8161\n",
      "PC141 - Variance explained:  0.0016 - Total Variance:  0.8177\n",
      "PC142 - Variance explained:  0.0016 - Total Variance:  0.8193\n",
      "PC143 - Variance explained:  0.0016 - Total Variance:  0.8208\n",
      "PC144 - Variance explained:  0.0016 - Total Variance:  0.8224\n",
      "PC145 - Variance explained:  0.0015 - Total Variance:  0.8239\n",
      "PC146 - Variance explained:  0.0015 - Total Variance:  0.8255\n",
      "PC147 - Variance explained:  0.0015 - Total Variance:  0.8270\n",
      "PC148 - Variance explained:  0.0015 - Total Variance:  0.8285\n",
      "PC149 - Variance explained:  0.0015 - Total Variance:  0.8300\n"
     ]
    }
   ],
   "source": [
    "pca_cut = pca(X_train_cut, 150)\n",
    "X_train_cut_pca = pca_cut.transform(X_train_cut)\n",
    "#X_test_cut_pca=pca_cut.transform(X_ivs_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8fee1ee-28b4-4594-8249-2c974586ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cut_kpca =kpca(X_train_cut,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6327c040-ab6c-4f7b-904f-5faa56139899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.9878 - Total Variance:  0.9878\n",
      "PC1 - Variance explained:  0.0102 - Total Variance:  0.9980\n",
      "PC2 - Variance explained:  0.0014 - Total Variance:  0.9994\n",
      "PC3 - Variance explained:  0.0002 - Total Variance:  0.9996\n",
      "PC4 - Variance explained:  0.0001 - Total Variance:  0.9998\n",
      "PC5 - Variance explained:  0.0001 - Total Variance:  0.9998\n",
      "PC6 - Variance explained:  0.0000 - Total Variance:  0.9999\n",
      "PC7 - Variance explained:  0.0000 - Total Variance:  0.9999\n",
      "PC8 - Variance explained:  0.0000 - Total Variance:  0.9999\n",
      "PC9 - Variance explained:  0.0000 - Total Variance:  0.9999\n",
      "PC10 - Variance explained:  0.0000 - Total Variance:  0.9999\n",
      "PC11 - Variance explained:  0.0000 - Total Variance:  0.9999\n",
      "PC12 - Variance explained:  0.0000 - Total Variance:  0.9999\n",
      "PC13 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC14 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC15 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC16 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC17 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC18 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC19 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC20 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC21 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC22 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC23 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC24 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC25 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC26 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC27 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC28 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC29 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC30 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC31 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC32 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC33 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC34 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC35 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC36 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC37 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC38 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC39 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC40 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC41 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC42 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC43 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC44 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC45 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC46 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC47 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC48 - Variance explained:  0.0000 - Total Variance:  1.0000\n",
      "PC49 - Variance explained:  0.0000 - Total Variance:  1.0000\n"
     ]
    }
   ],
   "source": [
    "pca_rffs = pca(X_train_rffs, 50)\n",
    "X_train_rffs_pca = pca_rffs.transform(X_train_rffs)\n",
    "#X_test_rffs_pca=pca_rffs.transform(X_test_rffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2ffb0a2-051e-4669-9941-3ba3b78b1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rffs_kpca =kpca(X_train_rffs,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3bdee35-ff0a-423d-930c-551ad7b0d17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.3320 - Total Variance:  0.3320\n",
      "PC1 - Variance explained:  0.0494 - Total Variance:  0.3814\n",
      "PC2 - Variance explained:  0.0474 - Total Variance:  0.4287\n",
      "PC3 - Variance explained:  0.0409 - Total Variance:  0.4696\n",
      "PC4 - Variance explained:  0.0334 - Total Variance:  0.5030\n",
      "PC5 - Variance explained:  0.0310 - Total Variance:  0.5341\n",
      "PC6 - Variance explained:  0.0265 - Total Variance:  0.5606\n",
      "PC7 - Variance explained:  0.0240 - Total Variance:  0.5845\n",
      "PC8 - Variance explained:  0.0218 - Total Variance:  0.6063\n",
      "PC9 - Variance explained:  0.0204 - Total Variance:  0.6267\n",
      "PC10 - Variance explained:  0.0195 - Total Variance:  0.6462\n",
      "PC11 - Variance explained:  0.0184 - Total Variance:  0.6647\n",
      "PC12 - Variance explained:  0.0167 - Total Variance:  0.6814\n",
      "PC13 - Variance explained:  0.0166 - Total Variance:  0.6980\n",
      "PC14 - Variance explained:  0.0158 - Total Variance:  0.7138\n",
      "PC15 - Variance explained:  0.0149 - Total Variance:  0.7287\n",
      "PC16 - Variance explained:  0.0141 - Total Variance:  0.7429\n",
      "PC17 - Variance explained:  0.0140 - Total Variance:  0.7568\n",
      "PC18 - Variance explained:  0.0132 - Total Variance:  0.7701\n",
      "PC19 - Variance explained:  0.0127 - Total Variance:  0.7827\n",
      "PC20 - Variance explained:  0.0121 - Total Variance:  0.7949\n",
      "PC21 - Variance explained:  0.0116 - Total Variance:  0.8065\n",
      "PC22 - Variance explained:  0.0112 - Total Variance:  0.8177\n",
      "PC23 - Variance explained:  0.0108 - Total Variance:  0.8285\n",
      "PC24 - Variance explained:  0.0106 - Total Variance:  0.8391\n",
      "PC25 - Variance explained:  0.0096 - Total Variance:  0.8487\n",
      "PC26 - Variance explained:  0.0094 - Total Variance:  0.8581\n",
      "PC27 - Variance explained:  0.0089 - Total Variance:  0.8670\n",
      "PC28 - Variance explained:  0.0085 - Total Variance:  0.8755\n",
      "PC29 - Variance explained:  0.0082 - Total Variance:  0.8837\n",
      "PC30 - Variance explained:  0.0080 - Total Variance:  0.8917\n",
      "PC31 - Variance explained:  0.0077 - Total Variance:  0.8994\n",
      "PC32 - Variance explained:  0.0075 - Total Variance:  0.9069\n",
      "PC33 - Variance explained:  0.0073 - Total Variance:  0.9142\n",
      "PC34 - Variance explained:  0.0070 - Total Variance:  0.9211\n",
      "PC35 - Variance explained:  0.0066 - Total Variance:  0.9277\n",
      "PC36 - Variance explained:  0.0065 - Total Variance:  0.9342\n",
      "PC37 - Variance explained:  0.0062 - Total Variance:  0.9404\n",
      "PC38 - Variance explained:  0.0061 - Total Variance:  0.9465\n",
      "PC39 - Variance explained:  0.0058 - Total Variance:  0.9523\n"
     ]
    }
   ],
   "source": [
    "#pca_scaled_rffs\n",
    "pca_scaled_rffs = pca(X_train_scaled_rffs, 40)\n",
    "X_train_scaled_rffs_pca = pca_scaled_rffs.transform(X_train_scaled_rffs)\n",
    "#X_test_scaled_rffs_pca = pca_scaled_rffs.transform(X_test_scaled_rffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1514c36a-1da2-40a6-a0af-3043ea458c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_rffs_kpca = kpca(X_train_scaled_rffs,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bedba7e6-6974-4758-a3b2-409215c41e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.4475 - Total Variance:  0.4475\n",
      "PC1 - Variance explained:  0.0565 - Total Variance:  0.5040\n",
      "PC2 - Variance explained:  0.0497 - Total Variance:  0.5537\n",
      "PC3 - Variance explained:  0.0418 - Total Variance:  0.5955\n",
      "PC4 - Variance explained:  0.0317 - Total Variance:  0.6272\n"
     ]
    }
   ],
   "source": [
    "pca_cut_rffs = pca(X_train_cut_rffs, 5)\n",
    "X_train_cut_rffs_pca=pca_cut_rffs.transform(X_train_cut_rffs)\n",
    "#X_test_cut_rffs_pca=pca_cut.transform(X_test_cut_rffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b68e82c-1499-40a6-948c-df2d93f22835",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cut_rffs_kpca = kpca(X_train_cut_rffs,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b50ca8",
   "metadata": {},
   "source": [
    "<h3>Cross-validation. Evaluation of the (models) and data configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ba74a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAvalStat(truth, preds):\n",
    "    avalStats = []\n",
    "    print(\"The RVE is: \", explained_variance_score(truth, preds))\n",
    "    print(\"The rmse is: \", mean_squared_error(truth, preds, squared=False))\n",
    "    corr, pval=pearsonr(truth, preds)\n",
    "    print(\"The Correlation Score is: %6.4f (p-value=%e)\"%(corr,pval))\n",
    "\n",
    "    print(\"The Maximum Error is: \", max_error(truth, preds))\n",
    "    print(\"The Mean Absolute Error is:\", mean_absolute_error(truth, preds),\"\\n\")\n",
    "    avalStats.append(explained_variance_score(truth, preds))\n",
    "    avalStats.append(mean_squared_error(truth, preds, squared=False))\n",
    "    avalStats.append(pearsonr(truth, preds))\n",
    "    #avalStats.append( %(corr,pval) )\n",
    "    return avalStats\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec27ca",
   "metadata": {},
   "source": [
    "<h4>N-Fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "511ffe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfold_valid(X_train_Valids):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=23)\n",
    "    kf.get_n_splits(X_train_Valids)\n",
    "    TRUTH_nfold=None\n",
    "    PREDS_nfold=None\n",
    "    for train_index, test_index in kf.split(X_train_Valids):\n",
    "        X_train_nfold, X_ivs_nfold = X_train_Valids[train_index], X_train_Valids[test_index]\n",
    "        y_train_nfold, y_ivs_nfold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        #mdl = DecisionTreeRegressor()#max_depth = 5)\n",
    "        mdl = RandomForestRegressor(n_estimators=10, random_state=0, min_samples_leaf=3, max_depth = 8, n_jobs=6)\n",
    "        mdl.fit(X_train_nfold, y_train_nfold)\n",
    "        preds = mdl.predict(X_ivs_nfold)\n",
    "        if TRUTH_nfold is None:\n",
    "            PREDS_nfold=preds\n",
    "            TRUTH_nfold=y_ivs_nfold\n",
    "        else:\n",
    "            PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
    "            TRUTH_nfold=np.hstack((TRUTH_nfold, y_ivs_nfold))\n",
    "        \n",
    "    printAvalStat(TRUTH_nfold, PREDS_nfold)\n",
    "\n",
    "#print(\"N-fold cross validation of nXf_train, after sequencial reduction feature selection\")\n",
    "#nfold_valid(nXf_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fa3d324-7c5e-46a6-ad04-2de4dfe661dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-fold cross validation of X_train\n",
      "The RVE is:  0.4306622630357595\n",
      "The rmse is:  0.2087354179051696\n",
      "The Correlation Score is: 0.6606 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8645408990344696\n",
      "The Mean Absolute Error is: 0.165311491822563 \n",
      "\n",
      "N-fold cross validation of X_train_scaled\n",
      "The RVE is:  0.43079787092361854\n",
      "The rmse is:  0.20871057962911607\n",
      "The Correlation Score is: 0.6607 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8645408990344696\n",
      "The Mean Absolute Error is: 0.16528339261716604 \n",
      "\n",
      "N-fold cross validation of X_train_cut(already scaled!)\n",
      "The RVE is:  0.4262619972524191\n",
      "The rmse is:  0.20955020213931275\n",
      "The Correlation Score is: 0.6565 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8327437900923893\n",
      "The Mean Absolute Error is: 0.16607190021223064 \n",
      "\n",
      "N-fold cross validation of X_train_rffs\n",
      "The RVE is:  0.41724130429386386\n",
      "The rmse is:  0.21118127103081402\n",
      "The Correlation Score is: 0.6488 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8811235836490621\n",
      "The Mean Absolute Error is: 0.16644973738239066 \n",
      "\n",
      "N-fold cross validation of X_train_scaled_rffs\n",
      "The RVE is:  0.4159583325440468\n",
      "The rmse is:  0.21141373263619678\n",
      "The Correlation Score is: 0.6477 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8811235836490621\n",
      "The Mean Absolute Error is: 0.16643985052787383 \n",
      "\n",
      "N-fold cross validation of X_train_cut_rffs(already scaled!)\n",
      "The RVE is:  0.4107818261103744\n",
      "The rmse is:  0.21234864282301666\n",
      "The Correlation Score is: 0.6429 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8977471674351751\n",
      "The Mean Absolute Error is: 0.1679458154947112 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first unchanged data evaluations\n",
    "print(\"N-fold cross validation of X_train\")\n",
    "nfold_valid(X_train)\n",
    "print(\"N-fold cross validation of X_train_scaled\")\n",
    "nfold_valid(X_train_scaled)\n",
    "\n",
    "#manually cut features data evaluations\n",
    "print(\"N-fold cross validation of X_train_cut(already scaled!)\")\n",
    "nfold_valid(X_train_cut)\n",
    "\n",
    "#feature selected data evaluations\n",
    "print(\"N-fold cross validation of X_train_rffs\")\n",
    "nfold_valid(X_train_rffs)\n",
    "print(\"N-fold cross validation of X_train_scaled_rffs\")\n",
    "nfold_valid(X_train_scaled_rffs)\n",
    "\n",
    "#cut x feature selected data evaluation\n",
    "print(\"N-fold cross validation of X_train_cut_rffs(already scaled!)\")\n",
    "nfold_valid(X_train_cut_rffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccab7bff-953a-4ecb-ac4c-f34d1a6c8e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-fold cross validation of X_train_pca\n",
      "The RVE is:  0.39299224798994303\n",
      "The rmse is:  0.21553220099737946\n",
      "The Correlation Score is: 0.6356 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8392410576248104\n",
      "The Mean Absolute Error is: 0.17040092876339938 \n",
      "\n",
      "N-fold cross validation of X_train_scaled_pca\n",
      "The RVE is:  0.424286725886451\n",
      "The rmse is:  0.2099055027123239\n",
      "The Correlation Score is: 0.6571 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.7692115092281582\n",
      "The Mean Absolute Error is: 0.16682860882396786 \n",
      "\n",
      "N-fold cross validation of X_train_cut_pca(already scaled!)\n",
      "The RVE is:  0.42787835988692025\n",
      "The rmse is:  0.20927293742785102\n",
      "The Correlation Score is: 0.6601 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8038212749178815\n",
      "The Mean Absolute Error is: 0.1665195275828643 \n",
      "\n",
      "N-fold cross validation of X_train_rffs_pca\n",
      "The RVE is:  0.37750178296084635\n",
      "The rmse is:  0.21826608766673183\n",
      "The Correlation Score is: 0.6201 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8242641145047169\n",
      "The Mean Absolute Error is: 0.17277500744785562 \n",
      "\n",
      "N-fold cross validation of X_train_scaled_rffs_pca\n",
      "The RVE is:  0.4315928262708101\n",
      "The rmse is:  0.20856827135358266\n",
      "The Correlation Score is: 0.6598 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8530131145798043\n",
      "The Mean Absolute Error is: 0.16517212417838031 \n",
      "\n",
      "N-fold cross validation of X_train_cut_rffs_pca(already scaled!)\n",
      "The RVE is:  0.3257271400285424\n",
      "The rmse is:  0.22715943070718847\n",
      "The Correlation Score is: 0.5710 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8464360778434633\n",
      "The Mean Absolute Error is: 0.18009985682386184 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pca evaluations\n",
    "print(\"N-fold cross validation of X_train_pca\")\n",
    "nfold_valid(X_train_pca)\n",
    "print(\"N-fold cross validation of X_train_scaled_pca\")\n",
    "nfold_valid(X_train_scaled_pca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_cut_pca(already scaled!)\")\n",
    "nfold_valid(X_train_cut_pca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_rffs_pca\")\n",
    "nfold_valid(X_train_rffs_pca)\n",
    "print(\"N-fold cross validation of X_train_scaled_rffs_pca\")\n",
    "nfold_valid(X_train_scaled_rffs_pca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_cut_rffs_pca(already scaled!)\")\n",
    "nfold_valid(X_train_cut_rffs_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f2e13f58-251a-4684-81d2-77a478f34ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-fold cross validation of X_train_kpca\n",
      "The RVE is:  0.14801158945012416\n",
      "The rmse is:  0.25535241202946546\n",
      "The Correlation Score is: 0.3848 (p-value=1.571161e-257)\n",
      "The Maximum Error is:  0.8272060695269964\n",
      "The Mean Absolute Error is: 0.20672956807658874 \n",
      "\n",
      "N-fold cross validation of X_train_scaled_kpca\n",
      "The RVE is:  0.4036438487075611\n",
      "The rmse is:  0.21363234368750544\n",
      "The Correlation Score is: 0.6391 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.7655713324708981\n",
      "The Mean Absolute Error is: 0.16811024817903172 \n",
      "\n",
      "N-fold cross validation of X_train_cut_kpca(already scaled!)\n",
      "The RVE is:  0.46641610444987747\n",
      "The rmse is:  0.20208862280966738\n",
      "The Correlation Score is: 0.6835 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8191539898850071\n",
      "The Mean Absolute Error is: 0.15917432346466567 \n",
      "\n",
      "N-fold cross validation of X_train_rffs_kpca\n",
      "The RVE is:  0.11273638118713725\n",
      "The rmse is:  0.260578449409946\n",
      "The Correlation Score is: 0.3358 (p-value=8.420050e-193)\n",
      "The Maximum Error is:  0.8635685063724706\n",
      "The Mean Absolute Error is: 0.2123349066178758 \n",
      "\n",
      "N-fold cross validation of X_train_scaled_rffs_kpca\n",
      "The RVE is:  0.4389465993769297\n",
      "The rmse is:  0.20721374515357663\n",
      "The Correlation Score is: 0.6652 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8351108080192204\n",
      "The Mean Absolute Error is: 0.163217459979481 \n",
      "\n",
      "N-fold cross validation of X_train_cut_rffs_kpca(already scaled!)\n",
      "The RVE is:  0.35950967520797383\n",
      "The rmse is:  0.22139481986401094\n",
      "The Correlation Score is: 0.5999 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8353772162139064\n",
      "The Mean Absolute Error is: 0.17509709860542114 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Kpca evaluations\n",
    "print(\"N-fold cross validation of X_train_kpca\")\n",
    "nfold_valid(X_train_kpca)\n",
    "print(\"N-fold cross validation of X_train_scaled_kpca\")\n",
    "nfold_valid(X_train_scaled_kpca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_cut_kpca(already scaled!)\")\n",
    "nfold_valid(X_train_cut_kpca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_rffs_kpca\")\n",
    "nfold_valid(X_train_rffs_kpca)\n",
    "print(\"N-fold cross validation of X_train_scaled_rffs_kpca\")\n",
    "nfold_valid(X_train_scaled_rffs_kpca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_cut_rffs_kpca(already scaled!)\")\n",
    "nfold_valid(X_train_cut_rffs_kpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13965578-d709-4b43-9de7-35e3b72788b9",
   "metadata": {},
   "source": [
    "<h5>\n",
    "Com estes resultados concluímos que o data tretment que efetuámos não afeta consideravelmente a qualidade dos modelos resultantes.\n",
    "<br>\n",
    "De salientar que os modelos resultantes dos dados manualmente cortados, que resultou numa redução de features de 2000+ para 377, tem resultados muito semelhantes aos modelos que usam todas as 2000+ features.\n",
    "<br>\n",
    "Por isso achamos que só o uso de mais e diferentes modelos para além de random forest é que poderemos melhorar o resultado das previsões.\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1293df2-324b-41de-98e0-c2af51f4bf0f",
   "metadata": {},
   "source": [
    "#### Daqui para a frente escolhemos o dataset X_train_scaled_rffs_pca por ter melhor ratio de scores/ nº features usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f4bbaa10-1cdf-4a5a-93a0-71c74e7937e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfold_valid_mt(X_train_Valids):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=23)\n",
    "    kf.get_n_splits(X_train_Valids)\n",
    "    TRUTH_nfold=None\n",
    "    PREDS_nfold=None\n",
    "    for train_index, test_index in kf.split(X_train_Valids):\n",
    "        X_train_nfold, X_ivs_nfold = X_train_Valids[train_index], X_train_Valids[test_index]\n",
    "        y_train_nfold, y_ivs_nfold = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    gammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "    Cs = [1, 10, 100, 1e3, 1e4, 1e5]\n",
    "    param_grid = {\"gamma\": gammas, \"C\": Cs}\n",
    "    \n",
    "    #define the model and do the grid search\n",
    "    svr = SVR()\n",
    "    gs = GridSearchCV(estimator=svr, param_grid=param_grid, scoring=\"explained_variance\")\n",
    "    mdl = gs.fit(X_train_Valids, y_train)\n",
    "    preds = mdl.predict(X_ivs_nfold)\n",
    "    if TRUTH_nfold is None:\n",
    "        PREDS_nfold=preds\n",
    "        TRUTH_nfold=y_ivs_nfold\n",
    "    else:\n",
    "        PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
    "        TRUTH_nfold=np.hstack((TRUTH_nfold, y_ivs_nfold))\n",
    "        \n",
    "    printAvalStat(TRUTH_nfold, PREDS_nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72d2941c-d727-403a-a9ae-bd6990684282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nfold_valid_mt(X_train_cut_kpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8179ff-fe50-41e9-8b09-9b07389443d5",
   "metadata": {},
   "source": [
    "<h3>New Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "51b9b653-d6d6-4279-a6ab-8ec28386ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfold_evaluate(mdl ,X_train_Valids):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=23)\n",
    "    kf.get_n_splits(X_train_Valids)\n",
    "    TRUTH_nfold=None\n",
    "    PREDS_nfold=None\n",
    "    for train_index, test_index in kf.split(X_train_Valids):\n",
    "        X_train_nfold, X_ivs_nfold = X_train_Valids[train_index], X_train_Valids[test_index]\n",
    "        y_train_nfold, y_ivs_nfold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        #mdl = DecisionTreeRegressor()#max_depth = 5)\n",
    "        mdl.fit(X_train_nfold, y_train_nfold)\n",
    "        preds = mdl.predict(X_ivs_nfold)\n",
    "        if TRUTH_nfold is None:\n",
    "            PREDS_nfold=preds\n",
    "            TRUTH_nfold=y_ivs_nfold\n",
    "        else:\n",
    "            PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
    "            TRUTH_nfold=np.hstack((TRUTH_nfold, y_ivs_nfold))\n",
    "        \n",
    "    return printAvalStat(TRUTH_nfold, PREDS_nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cdbed44d-6782-4d05-8962-705a35aa3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR_mdl = RandomForestRegressor(n_estimators=10, random_state=0, min_samples_leaf=3, max_depth = 8, n_jobs=6)\n",
    "LR_mdl = LinearRegression()\n",
    "Ridge_mdl = Ridge(alpha=1, max_iter=9999999)#10\n",
    "Lasso_mdl = Lasso(alpha=1, max_iter=9999999)#10\n",
    "KNN_mdl =KNeighborsRegressor()\n",
    "SVR_mdl =SVR()\n",
    "XGB_mdl = XGBRegressor()\n",
    "ABR_mdl = AdaBoostRegressor(n_estimators=10) #n_estimators=10\n",
    "GBR_mdl = GradientBoostingRegressor(n_estimators=10, learning_rate=1.0, random_state=0) #n_estimators=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10acac-a39c-4d92-8919-33d319397176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set de controlo\n",
    "print(\"Set de controlo\")\n",
    "print(\"result of linear regression model on the X_train_scaled dataset\")\n",
    "LR_ce = nfold_evaluate(LR_mdl ,X_train_scaled)\n",
    "print(\"result of Ridge model on the X_train_scaled dataset\")\n",
    "Ridge_ce= nfold_evaluate(Ridge_mdl ,X_train_scaled)\n",
    "print(\"result of Lasso regression model on the X_train_scaled dataset\")\n",
    "Lasso_ce=nfold_evaluate(Lasso_mdl ,X_train_scaled)\n",
    "print(\"result of K-means regression model on the X_train_scaled dataset\")\n",
    "KNN_ce=nfold_evaluate(KNN_mdl ,X_train_scaled)\n",
    "print(\"result of support vector regressor model on the X_train_scaled dataset\")\n",
    "SVR_ce=nfold_evaluate(SVR_mdl ,X_train_scaled)\n",
    "print(\"result of XGBoost regression model on the X_train_scaled dataset\")\n",
    "XGB_ce=nfold_evaluate(XGB_mdl ,X_train_scaled)\n",
    "print(\"result of AdaBoost regression model on the X_train_scaled dataset\")\n",
    "ABR_ce=nfold_evaluate(ABR_mdl ,X_train_scaled)\n",
    "print(\"result of GradientBoost regression model on the X_train_scaled dataset\")\n",
    "GBR_ce=nfold_evaluate(GBR_mdl ,X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7224d971-43ff-4dd8-a8d0-94aaa8852331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of linear regression model on the X_train_cut_pca dataset\n",
      "The RVE is:  0.43377154744510427\n",
      "The rmse is:  0.20816492874850623\n",
      "The Correlation Score is: 0.6590 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8347293567988678\n",
      "The Mean Absolute Error is: 0.16525519780453973 \n",
      "\n",
      "result of Ridge model on the X_train_cut_pca dataset\n",
      "The RVE is:  0.4337769251890182\n",
      "The rmse is:  0.2081639399134176\n",
      "The Correlation Score is: 0.6590 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8347123914044519\n",
      "The Mean Absolute Error is: 0.1652550531876235 \n",
      "\n",
      "result of Lasso regression model on the X_train_cut_pca dataset\n",
      "The RVE is:  -0.0001864913956102221\n",
      "The rmse is:  0.2766632727709075\n",
      "The Correlation Score is: -0.0182 (p-value=1.189339e-01)\n",
      "The Maximum Error is:  0.613232559385074\n",
      "The Mean Absolute Error is: 0.22794516065455514 \n",
      "\n",
      "result of K-means regression model on the X_train_cut_pca dataset\n",
      "The RVE is:  0.5723094714809536\n",
      "The rmse is:  0.1816170382403425\n",
      "The Correlation Score is: 0.7588 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.95\n",
      "The Mean Absolute Error is: 0.13359628028875561 \n",
      "\n",
      "result of support vector regressor model on the X_train_cut_pca dataset\n",
      "The RVE is:  0.6146736454782425\n",
      "The rmse is:  0.1717218120943494\n",
      "The Correlation Score is: 0.7844 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8878367845582482\n",
      "The Mean Absolute Error is: 0.1315740483253072 \n",
      "\n",
      "result of XGBoost regression model on the X_train_cut_pca dataset\n",
      "The RVE is:  0.5054042508021598\n",
      "The rmse is:  0.19455468314200533\n",
      "The Correlation Score is: 0.7136 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.9033332942229194\n",
      "The Mean Absolute Error is: 0.14938183129192387 \n",
      "\n",
      "result of AdaBoost regression model on the X_train_cut_pca dataset\n",
      "The RVE is:  0.25288126702533453\n",
      "The rmse is:  0.23916834397582837\n",
      "The Correlation Score is: 0.5380 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.6822412535147975\n",
      "The Mean Absolute Error is: 0.19500135811624028 \n",
      "\n",
      "result of GradientBoost regression model on the X_train_cut_pca dataset\n",
      "The RVE is:  0.32857461360146034\n",
      "The rmse is:  0.22670718077577265\n",
      "The Correlation Score is: 0.5971 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  1.201353198636871\n",
      "The Mean Absolute Error is: 0.17746665210773113 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set para testing do X_train_scaled_rffs_pca\n",
    "print(\"result of linear regression model on the X_train_cut_pca dataset\")\n",
    "LR_me = nfold_evaluate(LR_mdl ,X_train_cut_pca)\n",
    "print(\"result of Ridge model on the X_train_cut_pca dataset\")\n",
    "Ridge_me = nfold_evaluate(Ridge_mdl ,X_train_cut_pca)\n",
    "print(\"result of Lasso regression model on the X_train_cut_pca dataset\")\n",
    "Lasso_me =nfold_evaluate(Lasso_mdl ,X_train_cut_pca)\n",
    "print(\"result of K-means regression model on the X_train_cut_pca dataset\")\n",
    "KNN_me =nfold_evaluate(KNN_mdl ,X_train_cut_pca)\n",
    "print(\"result of support vector regressor model on the X_train_cut_pca dataset\")\n",
    "SVR_me =nfold_evaluate(SVR_mdl ,X_train_cut_pca)\n",
    "print(\"result of XGBoost regression model on the X_train_cut_pca dataset\")\n",
    "XGB_me =nfold_evaluate(XGB_mdl ,X_train_cut_pca)\n",
    "print(\"result of AdaBoost regression model on the X_train_cut_pca dataset\")\n",
    "ABR_me =nfold_evaluate(ABR_mdl ,X_train_cut_pca)\n",
    "print(\"result of GradientBoost regression model on the X_train_cut_pca dataset\")\n",
    "GBR_me =nfold_evaluate(GBR_mdl ,X_train_cut_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "972dc5e0-5da7-471f-9897-32c95658b46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imputers = [\"LR\", \"Ridge\",\"Lasso\", \"KNN\", \"SVR\", \"XGBoost\", \"AdaBoost\", \"GradientBoost\"]\n",
    "RVEs = [LR_me[0], Ridge_me[0], Lasso_me[0], KNN_me[0], SVR_me[0], XGB_me[0], ABR_me[0], GBR_me[0]]\n",
    "\n",
    "plt.plot(imputers, RVEs, marker = \"o\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6d81df1-a392-4b87-b586-16d4b05946a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of support vector regressor model on the X_train_cut_pca dataset\n",
      "The RVE is:  0.6146736454782425\n",
      "The rmse is:  0.1717218120943494\n",
      "The Correlation Score is: 0.7844 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8878367845582482\n",
      "The Mean Absolute Error is: 0.1315740483253072 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVR_mdl2 = SVR(kernel= \"rbf\", gamma = 'scale')\n",
    "print(\"result of support vector regressor model on the X_train_cut_pca dataset\")\n",
    "nfold_evaluate(SVR_mdl2 ,X_train_cut_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3964d6b",
   "metadata": {},
   "source": [
    "<h4>Leave-one-out cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be3205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeaveOneOut_val(X_train_Valids):\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(X_train_Valids)\n",
    "\n",
    "    TRUTH_loo=[]\n",
    "    PREDS_loo=[]\n",
    "\n",
    "    for train_index, test_index in loo.split(X_train_Valids):\n",
    "        X_train_loo, X_test_loo = X_train_Valids[train_index], X_train_Valids[test_index]\n",
    "        y_train_loo, y_test_loo = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        mdl = DecisionTreeRegressor()#max_depth = 5)\n",
    "        mdl.fit(X_train_loo, y_train_loo)\n",
    "        pred = mdl.predict(X_test_loo)\n",
    "        PREDS_loo.append(pred)\n",
    "        TRUTH_loo.append(y_test_loo)\n",
    "\n",
    "    printAvalStat(TRUTH_nfold, PREDS_nfold)\n",
    "        \n",
    "print(\"Leave-one-out cross validation of X_train\")\n",
    "LeaveOneOut_val(X_train)\n",
    "print(\"Leave-one-out cross validation of nX_train, after PCA\")\n",
    "LeaveOneOut_val(nX_train)\n",
    "#print(\"Leave-one-out cross validation of nXf_train, after sequencial reduction feature selection\")\n",
    "#LeaveOneOut_val(nXf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb6a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "#Cs = [1, 10, 100, 1e3, 1e4, 1e5]\n",
    "#param_grid = {\"gamma\": gammas, \"C\": Cs}\n",
    "\n",
    "#svr = SVR()\n",
    "#gs = GridSearchCV(estimator=svr, param_grid=param_grid, scoring=\"explained_variance\")\n",
    "#gs=gs.fit(X_train, y_train)\n",
    "\n",
    "#res = pd.DataFrame(gs.cv_results_)\n",
    "#res = res.sort_values(by=[\"rank_test_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9671a2",
   "metadata": {},
   "source": [
    "<h4> Random Forest Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49249097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisplayRegResults(y_test, preds):\n",
    "    print(\"RVE: %7.4f\"  % explained_variance_score(y_test, preds))\n",
    "    print(\"rmse: %7.4f\" % mean_squared_error(y_test, preds, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=10, random_state=0, min_samples_leaf=3)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "preds=rf.predict(X_ivs_scaled)\n",
    "\n",
    "DisplayRegResults(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60e0db",
   "metadata": {},
   "source": [
    "<h3>Objective:<br> Produce the best regression model for y_ivs (Dependent Variable)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db0a9a",
   "metadata": {},
   "source": [
    "Decision tree Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmdl = DecisionTreeRegressor(max_depth=5)\n",
    "dmdl.fit(X_train, y_train)\n",
    "\n",
    "dtr_preds=dmdl.predict(X_test)\n",
    "\n",
    "#explained_variance_score(y_test, dtr_preds)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "r=tree.plot_tree(dmdl, filled=True)#, feature_names= df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses_i = []\n",
    "rmses_j = []\n",
    "\n",
    "#def statGraph()\n",
    "for i in range(1, 11):\n",
    "    dmdl_i = DecisionTreeRegressor(max_depth=i)\n",
    "    dmdl_i.fit(X_train, y_train)\n",
    "    \n",
    "    preds_i=dmdl_i.predict(X_test)\n",
    "    #= explained_variance_score(y_test, preds)\n",
    "    rmses_i.append(mean_squared_error(y_test, preds_i, squared=False))\n",
    "    \n",
    "for i in range(1, 41):\n",
    "    dmdl_j = DecisionTreeRegressor(max_depth=5, min_samples_leaf= i)\n",
    "    dmdl_j.fit(X_train, y_train)\n",
    "    \n",
    "    preds_j=dmdl_j.predict(X_test)\n",
    "    #= explained_variance_score(y_test, preds)\n",
    "    rmses_j.append(mean_squared_error(y_test, preds_j, squared=False))\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.plot(range(1, 11), rmses_i)\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.plot(range(1, 41), rmses_j)\n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e60aec",
   "metadata": {},
   "source": [
    "Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "lr_preds=reg.predict(X_test)\n",
    "\n",
    "#explained_variance_score(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33048246",
   "metadata": {},
   "source": [
    "Alternative linear regression model using statusmodel implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = add_constant(X_train)\n",
    "reg2=OLS(y_train,X_tr, hasconst=12).fit()\n",
    "reg2.summary()\n",
    "\n",
    "#alr_preds= reg2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab94b3",
   "metadata": {},
   "source": [
    "Regularized linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71799e7",
   "metadata": {},
   "source": [
    "Ridge Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36958bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=10, max_iter=9999999).fit(X_train, y_train)\n",
    "\n",
    "print(\"The bias is: \",  ridge.intercept_)\n",
    "print(\"The other parameters are: \")\n",
    "#for i, beta in enumerate(ridge.coef_):\n",
    "#    print(\"\\t B%02d -> %9.3f\"% (i+1, beta))\n",
    "\n",
    "ridge_preds=ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b37cde7",
   "metadata": {},
   "source": [
    "Lasso Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157655b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=10, max_iter=9999999).fit(X_train, y_train)\n",
    "\n",
    "print(\"The bias is: \",  lasso.intercept_)\n",
    "print(\"The other parameters are: \")\n",
    "#for i, beta in enumerate(ridge.coef_):\n",
    "#    print(\"\\t B%02d -> %9.3f\"% (i+1, beta))\n",
    "\n",
    "lasso_preds= lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62839ec5",
   "metadata": {},
   "source": [
    "Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawGraf(preds, title):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.scatter(preds, y_test)\n",
    "    plt.plot((0, 150), (0,150), c=\"r\")\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Truth\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#####  Gráfico linear regression #########\n",
    "drawGraf(lr_preds, \"Linear Regression\")\n",
    "\n",
    "#####  Gráfico decision tree regression #########\n",
    "drawGraf(dtr_preds, \"Decision Tree Regression\")\n",
    "\n",
    "#####  Gráfico ridge regression #########\n",
    "drawGraf(ridge_preds, \"Ridge Regression\")\n",
    "\n",
    "#####  Gráfico lasso regression #########\n",
    "drawGraf(lasso_preds, \"Lasso Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAvalStat(truth, preds):\n",
    "    print(\"The RVE is: \", explained_variance_score(truth, preds))\n",
    "    print(\"The rmse is: \", mean_squared_error(truth, preds, squared=False))\n",
    "    corr, pval=pearsonr(truth, preds)\n",
    "    print(\"The Correlation Score is: %6.4f (p-value=%e)\"%(corr,pval))\n",
    "\n",
    "    print(\"The Maximum Error is: \", max_error(truth, preds))\n",
    "    print(\"The Mean Absolute Error is:\", mean_absolute_error(truth, preds),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb11eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############   Métricas de avaliação de decision tree regression\n",
    "print(\"Métricas de avaliação de decision tree regression:\")\n",
    "printAvalStat(y_test, dtr_preds)\n",
    "\n",
    "###############   Métricas de avaliação de linear regression\n",
    "print(\"Métricas de avaliação de linear regression:\")\n",
    "printAvalStat(y_test, lr_preds)\n",
    "\n",
    "###############   Métricas de avaliação de ridge regression\n",
    "print(\"Métricas de avaliação de ridge regression:\")\n",
    "printAvalStat(y_test, ridge_preds)\n",
    "\n",
    "###############   Métricas de avaliação de lasso regression\n",
    "print(\"Métricas de avaliação de lasso regression:\")\n",
    "printAvalStat(y_test, lasso_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
