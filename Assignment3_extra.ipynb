{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190fb39a",
   "metadata": {},
   "source": [
    "# Machine Learning 2023/2024\n",
    "\n",
    "## Third Home Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df32ad",
   "metadata": {},
   "source": [
    "**Group Number:** 10\n",
    "\n",
    "**Group Elements:**\n",
    "- André Santos (fc53323)\n",
    "- Filipe Santos (fc53304)\n",
    "- João Martins (fc62532)\n",
    "- Rúben Torres (fc62531)\n",
    "\n",
    "**Hours Worked:**\n",
    "- André Santos (10h)\n",
    "- Filipe Santos (10h)\n",
    "- João Martins (10h)\n",
    "- Rúben Torres (10h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c477d4",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccfa3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "#from statsmodels.api import OLS, add_constant\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, max_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import (PowerTransformer)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da94a3b",
   "metadata": {},
   "source": [
    "### Loading and understanding the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598351a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df = pd.read_pickle(\"drd2_data.pickle\")\n",
    "#unpickled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d77ed14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7337, 2132)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(unpickled_df[0]))\n",
    "unpickled_df[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e8122",
   "metadata": {},
   "source": [
    "Data is splitted upon loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0cb614",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_ivs, y_train, col_names = pickle.load(open(\"drd2_data.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc145a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in (col_names):\n",
    "#    print(\"coluna: \", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130a27dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7337, 2132)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63e19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.tofile('X_train.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64ef73d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(816, 2132)\n"
     ]
    }
   ],
   "source": [
    "print(X_ivs.shape)\n",
    "#X_ivs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431538c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7337,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec9d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_to_discard = []\n",
    "#for col in range (X_train.shape[1]):\n",
    "#    print(\"coluna: \", col_names[col])\n",
    "#    proportion = (X_train[: col] == 0).mean() * 100\n",
    "#    if proportion > 0:\n",
    "#            print(f\"Proportion of missing values in column { col }: { round(proportion, 2) }%\")\n",
    "#        cols_to_discard.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51bc4e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2123</th>\n",
       "      <th>2124</th>\n",
       "      <th>2125</th>\n",
       "      <th>2126</th>\n",
       "      <th>2127</th>\n",
       "      <th>2128</th>\n",
       "      <th>2129</th>\n",
       "      <th>2130</th>\n",
       "      <th>2131</th>\n",
       "      <th>2132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.654947</td>\n",
       "      <td>541.280138</td>\n",
       "      <td>541.656</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.649995</td>\n",
       "      <td>426.197714</td>\n",
       "      <td>426.582</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.154947</td>\n",
       "      <td>348.183778</td>\n",
       "      <td>348.446</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.616176</td>\n",
       "      <td>1455.763803</td>\n",
       "      <td>1456.831</td>\n",
       "      <td>27.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.359725</td>\n",
       "      <td>387.151368</td>\n",
       "      <td>387.886</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>467.149047</td>\n",
       "      <td>467.513</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7333</th>\n",
       "      <td>0.002193</td>\n",
       "      <td>240.162649</td>\n",
       "      <td>240.350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7334</th>\n",
       "      <td>0.293481</td>\n",
       "      <td>510.317874</td>\n",
       "      <td>510.802</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7335</th>\n",
       "      <td>0.596804</td>\n",
       "      <td>393.187483</td>\n",
       "      <td>393.556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7336</th>\n",
       "      <td>0.776976</td>\n",
       "      <td>484.056123</td>\n",
       "      <td>485.462</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7337 rows × 2133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0            1         2     3     4     5     6     7      8     \\\n",
       "0     0.654947   541.280138   541.656  10.0   1.0   8.0   1.0  10.0   40.0   \n",
       "1     0.649995   426.197714   426.582   5.0   1.0   9.0   1.0   4.0   30.0   \n",
       "2     0.154947   348.183778   348.446   4.0   0.0   3.0   0.0   3.0   26.0   \n",
       "3     0.616176  1455.763803  1456.831  27.0  19.0  23.0  17.0  16.0  105.0   \n",
       "4     0.359725   387.151368   387.886   4.0   0.0   4.0   0.0   4.0   27.0   \n",
       "...        ...          ...       ...   ...   ...   ...   ...   ...    ...   \n",
       "7332  0.000000   467.149047   467.513   6.0   0.0   6.0   0.0   5.0   32.0   \n",
       "7333  0.002193   240.162649   240.350   2.0   0.0   3.0   0.0   2.0   18.0   \n",
       "7334  0.293481   510.317874   510.802   4.0   0.0  10.0   0.0   4.0   37.0   \n",
       "7335  0.596804   393.187483   393.556   4.0   2.0   5.0   1.0   5.0   28.0   \n",
       "7336  0.776976   484.056123   485.462   6.0   1.0   7.0   1.0   6.0   30.0   \n",
       "\n",
       "       9     ...  2123  2124  2125  2126  2127  2128  2129  2130  2131  2132  \n",
       "0      75.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1      60.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2      50.0  ...   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3     206.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0  \n",
       "4      50.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...     ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "7332   56.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7333   38.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "7334   79.0  ...   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7335   55.0  ...   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7336   52.0  ...   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[7337 rows x 2133 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N,M=X_train.shape\n",
    "N,M\n",
    "v=np.hstack((y_train.reshape((N,1)), X_train))\n",
    "pd.DataFrame(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd6c76-8b8d-4e79-b6a7-2808c2faaae9",
   "metadata": {},
   "source": [
    "<h3>Data Treatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "272f6f3c-e1e3-4b37-aa43-ad4cfd28c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#powerscaler not ideal for this dataset since it deletes outliers and we believe that the outliers shouldn't be removed from this dataset\n",
    "#scaler = PowerTransformer().fit(X_train)\n",
    "\n",
    "sScaler = StandardScaler()\n",
    "X_train_scaled = sScaler.fit_transform(X_train)\n",
    "X_ivs_scaled = sScaler.transform(X_ivs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc9cff66-ac2c-4074-826c-12ef2947c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.unique(X_train_scaled[: 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90cfa218-0b72-4752-8506-41bb13e457f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_scaled.tofile('X_train_scaled.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf635db9-0490-45b9-9fe7-0c60bf8c3e47",
   "metadata": {},
   "source": [
    "<h3>selecting features by droping the features with correlation score bellow 0.05 to the dependant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaa85ca2-8d13-4e1f-a9e7-aaa4b10d9923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2123</th>\n",
       "      <th>2124</th>\n",
       "      <th>2125</th>\n",
       "      <th>2126</th>\n",
       "      <th>2127</th>\n",
       "      <th>2128</th>\n",
       "      <th>2129</th>\n",
       "      <th>2130</th>\n",
       "      <th>2131</th>\n",
       "      <th>2132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.654947</td>\n",
       "      <td>0.609921</td>\n",
       "      <td>0.608648</td>\n",
       "      <td>1.098414</td>\n",
       "      <td>-0.081854</td>\n",
       "      <td>0.246156</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>1.908209</td>\n",
       "      <td>0.737657</td>\n",
       "      <td>0.614996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.649995</td>\n",
       "      <td>-0.023790</td>\n",
       "      <td>-0.024513</td>\n",
       "      <td>-0.103427</td>\n",
       "      <td>-0.081854</td>\n",
       "      <td>0.413306</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>-0.243126</td>\n",
       "      <td>-0.038629</td>\n",
       "      <td>0.032423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.154947</td>\n",
       "      <td>-0.453381</td>\n",
       "      <td>-0.454433</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.589590</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.601682</td>\n",
       "      <td>-0.349144</td>\n",
       "      <td>-0.355960</td>\n",
       "      <td>...</td>\n",
       "      <td>1.234467</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.616176</td>\n",
       "      <td>5.645607</td>\n",
       "      <td>5.644129</td>\n",
       "      <td>5.184672</td>\n",
       "      <td>7.039406</td>\n",
       "      <td>2.753396</td>\n",
       "      <td>7.153978</td>\n",
       "      <td>4.059544</td>\n",
       "      <td>5.783518</td>\n",
       "      <td>5.702803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>5.029661</td>\n",
       "      <td>3.091413</td>\n",
       "      <td>5.116060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.359725</td>\n",
       "      <td>-0.238802</td>\n",
       "      <td>-0.237426</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.422441</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.243126</td>\n",
       "      <td>-0.271515</td>\n",
       "      <td>-0.355960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201712</td>\n",
       "      <td>0.200698</td>\n",
       "      <td>0.136941</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.088142</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>0.115430</td>\n",
       "      <td>0.116628</td>\n",
       "      <td>-0.122930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7333</th>\n",
       "      <td>0.002193</td>\n",
       "      <td>-1.048209</td>\n",
       "      <td>-1.049199</td>\n",
       "      <td>-0.824531</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.589590</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.960238</td>\n",
       "      <td>-0.970173</td>\n",
       "      <td>-0.822018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>3.091413</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7334</th>\n",
       "      <td>0.293481</td>\n",
       "      <td>0.439425</td>\n",
       "      <td>0.438883</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>0.580455</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.243126</td>\n",
       "      <td>0.504771</td>\n",
       "      <td>0.770349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>3.210153</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7335</th>\n",
       "      <td>0.596804</td>\n",
       "      <td>-0.205564</td>\n",
       "      <td>-0.206229</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>0.313772</td>\n",
       "      <td>-0.255292</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>0.115430</td>\n",
       "      <td>-0.193886</td>\n",
       "      <td>-0.161769</td>\n",
       "      <td>...</td>\n",
       "      <td>1.234467</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7336</th>\n",
       "      <td>0.776976</td>\n",
       "      <td>0.294812</td>\n",
       "      <td>0.299457</td>\n",
       "      <td>0.136941</td>\n",
       "      <td>-0.081854</td>\n",
       "      <td>0.079007</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>0.473986</td>\n",
       "      <td>-0.038629</td>\n",
       "      <td>-0.278283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>5.116060</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7337 rows × 2133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.654947  0.609921  0.608648  1.098414 -0.081854  0.246156 -0.058057   \n",
       "1     0.649995 -0.023790 -0.024513 -0.103427 -0.081854  0.413306 -0.058057   \n",
       "2     0.154947 -0.453381 -0.454433 -0.343795 -0.477479 -0.589590 -0.508809   \n",
       "3     0.616176  5.645607  5.644129  5.184672  7.039406  2.753396  7.153978   \n",
       "4     0.359725 -0.238802 -0.237426 -0.343795 -0.477479 -0.422441 -0.508809   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7332  0.000000  0.201712  0.200698  0.136941 -0.477479 -0.088142 -0.508809   \n",
       "7333  0.002193 -1.048209 -1.049199 -0.824531 -0.477479 -0.589590 -0.508809   \n",
       "7334  0.293481  0.439425  0.438883 -0.343795 -0.477479  0.580455 -0.508809   \n",
       "7335  0.596804 -0.205564 -0.206229 -0.343795  0.313772 -0.255292 -0.058057   \n",
       "7336  0.776976  0.294812  0.299457  0.136941 -0.081854  0.079007 -0.058057   \n",
       "\n",
       "          7         8         9     ...      2123      2124      2125  \\\n",
       "0     1.908209  0.737657  0.614996  ... -0.810066 -0.195463 -0.179087   \n",
       "1    -0.243126 -0.038629  0.032423  ... -0.810066 -0.195463 -0.179087   \n",
       "2    -0.601682 -0.349144 -0.355960  ...  1.234467 -0.195463 -0.179087   \n",
       "3     4.059544  5.783518  5.702803  ... -0.810066 -0.195463 -0.179087   \n",
       "4    -0.243126 -0.271515 -0.355960  ... -0.810066 -0.195463 -0.179087   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7332  0.115430  0.116628 -0.122930  ... -0.810066 -0.195463 -0.179087   \n",
       "7333 -0.960238 -0.970173 -0.822018  ... -0.810066 -0.195463 -0.179087   \n",
       "7334 -0.243126  0.504771  0.770349  ... -0.810066 -0.195463 -0.179087   \n",
       "7335  0.115430 -0.193886 -0.161769  ...  1.234467 -0.195463 -0.179087   \n",
       "7336  0.473986 -0.038629 -0.278283  ... -0.810066  5.116060 -0.179087   \n",
       "\n",
       "          2126      2127      2128      2129      2130      2131      2132  \n",
       "0    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "1    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "2    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "3    -0.311512 -0.168687 -0.105656 -0.136394  5.029661  3.091413  5.116060  \n",
       "4    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7332 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "7333 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821  3.091413 -0.195463  \n",
       "7334  3.210153 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "7335 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "7336 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "\n",
       "[7337 rows x 2133 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new correlation matrix with scaled data\n",
    "v_scaled=np.hstack((y_train.reshape((N,1)), X_train_scaled))\n",
    "pd.DataFrame(v_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4854909b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2123</th>\n",
       "      <th>2124</th>\n",
       "      <th>2125</th>\n",
       "      <th>2126</th>\n",
       "      <th>2127</th>\n",
       "      <th>2128</th>\n",
       "      <th>2129</th>\n",
       "      <th>2130</th>\n",
       "      <th>2131</th>\n",
       "      <th>2132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.160182</td>\n",
       "      <td>0.160216</td>\n",
       "      <td>0.130117</td>\n",
       "      <td>0.084939</td>\n",
       "      <td>0.151968</td>\n",
       "      <td>0.095929</td>\n",
       "      <td>0.126690</td>\n",
       "      <td>0.157545</td>\n",
       "      <td>0.165189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011360</td>\n",
       "      <td>0.095138</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>-0.021728</td>\n",
       "      <td>-0.012977</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.031884</td>\n",
       "      <td>-0.027774</td>\n",
       "      <td>0.058370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.160182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.919389</td>\n",
       "      <td>0.772557</td>\n",
       "      <td>0.888252</td>\n",
       "      <td>0.784495</td>\n",
       "      <td>0.856242</td>\n",
       "      <td>0.992766</td>\n",
       "      <td>0.973177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213406</td>\n",
       "      <td>0.019559</td>\n",
       "      <td>0.127229</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>-0.017119</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.212881</td>\n",
       "      <td>0.342831</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.219727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.160216</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919128</td>\n",
       "      <td>0.772379</td>\n",
       "      <td>0.888072</td>\n",
       "      <td>0.784320</td>\n",
       "      <td>0.855980</td>\n",
       "      <td>0.992623</td>\n",
       "      <td>0.972956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213435</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.127309</td>\n",
       "      <td>0.117351</td>\n",
       "      <td>-0.017201</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.212889</td>\n",
       "      <td>0.342785</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>0.219646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130117</td>\n",
       "      <td>0.919389</td>\n",
       "      <td>0.919128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838103</td>\n",
       "      <td>0.863232</td>\n",
       "      <td>0.843652</td>\n",
       "      <td>0.931528</td>\n",
       "      <td>0.922469</td>\n",
       "      <td>0.905146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196534</td>\n",
       "      <td>-0.008557</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>0.064926</td>\n",
       "      <td>-0.026235</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.181134</td>\n",
       "      <td>0.332638</td>\n",
       "      <td>0.137266</td>\n",
       "      <td>0.205303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.084939</td>\n",
       "      <td>0.772557</td>\n",
       "      <td>0.772379</td>\n",
       "      <td>0.838103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.704868</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>0.653117</td>\n",
       "      <td>0.774714</td>\n",
       "      <td>0.769215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185163</td>\n",
       "      <td>-0.032325</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>0.022735</td>\n",
       "      <td>-0.013150</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.110449</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.210139</td>\n",
       "      <td>0.231743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.036037</td>\n",
       "      <td>0.047958</td>\n",
       "      <td>0.038387</td>\n",
       "      <td>0.059813</td>\n",
       "      <td>0.061989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018420</td>\n",
       "      <td>-0.020652</td>\n",
       "      <td>-0.018922</td>\n",
       "      <td>-0.009945</td>\n",
       "      <td>-0.009870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063520</td>\n",
       "      <td>0.047193</td>\n",
       "      <td>0.032638</td>\n",
       "      <td>0.027846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.212881</td>\n",
       "      <td>0.212889</td>\n",
       "      <td>0.181134</td>\n",
       "      <td>0.110449</td>\n",
       "      <td>0.254921</td>\n",
       "      <td>0.115041</td>\n",
       "      <td>0.202605</td>\n",
       "      <td>0.213051</td>\n",
       "      <td>0.218212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>-0.015847</td>\n",
       "      <td>0.075295</td>\n",
       "      <td>-0.020981</td>\n",
       "      <td>-0.004391</td>\n",
       "      <td>0.063520</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>0.000372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>0.031884</td>\n",
       "      <td>0.342831</td>\n",
       "      <td>0.342785</td>\n",
       "      <td>0.332638</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.276091</td>\n",
       "      <td>0.410854</td>\n",
       "      <td>0.233237</td>\n",
       "      <td>0.342548</td>\n",
       "      <td>0.342609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.017914</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>-0.004214</td>\n",
       "      <td>-0.020504</td>\n",
       "      <td>0.047193</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.123067</td>\n",
       "      <td>0.127682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>-0.027774</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>0.137266</td>\n",
       "      <td>0.210139</td>\n",
       "      <td>0.177572</td>\n",
       "      <td>0.216401</td>\n",
       "      <td>0.081396</td>\n",
       "      <td>0.184924</td>\n",
       "      <td>0.190152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>-0.026145</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>0.051670</td>\n",
       "      <td>-0.029027</td>\n",
       "      <td>0.032638</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>0.123067</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>0.058370</td>\n",
       "      <td>0.219727</td>\n",
       "      <td>0.219646</td>\n",
       "      <td>0.205303</td>\n",
       "      <td>0.231743</td>\n",
       "      <td>0.119145</td>\n",
       "      <td>0.240242</td>\n",
       "      <td>0.151050</td>\n",
       "      <td>0.226627</td>\n",
       "      <td>0.218560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>-0.025196</td>\n",
       "      <td>-0.024145</td>\n",
       "      <td>0.027846</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.127682</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2133 rows × 2133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     1.000000  0.160182  0.160216  0.130117  0.084939  0.151968  0.095929   \n",
       "1     0.160182  1.000000  0.999998  0.919389  0.772557  0.888252  0.784495   \n",
       "2     0.160216  0.999998  1.000000  0.919128  0.772379  0.888072  0.784320   \n",
       "3     0.130117  0.919389  0.919128  1.000000  0.838103  0.863232  0.843652   \n",
       "4     0.084939  0.772557  0.772379  0.838103  1.000000  0.704868  0.993448   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2128  0.001865  0.053576  0.053498  0.043314  0.045537  0.036037  0.047958   \n",
       "2129  0.012468  0.212881  0.212889  0.181134  0.110449  0.254921  0.115041   \n",
       "2130  0.031884  0.342831  0.342785  0.332638  0.409722  0.276091  0.410854   \n",
       "2131 -0.027774  0.173077  0.173016  0.137266  0.210139  0.177572  0.216401   \n",
       "2132  0.058370  0.219727  0.219646  0.205303  0.231743  0.119145  0.240242   \n",
       "\n",
       "          7         8         9     ...      2123      2124      2125  \\\n",
       "0     0.126690  0.157545  0.165189  ...  0.011360  0.095138  0.078900   \n",
       "1     0.856242  0.992766  0.973177  ...  0.213406  0.019559  0.127229   \n",
       "2     0.855980  0.992623  0.972956  ...  0.213435  0.019583  0.127309   \n",
       "3     0.931528  0.922469  0.905146  ...  0.196534 -0.008557  0.095891   \n",
       "4     0.653117  0.774714  0.769215  ...  0.185163 -0.032325  0.083849   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2128  0.038387  0.059813  0.061989  ...  0.018420 -0.020652 -0.018922   \n",
       "2129  0.202605  0.213051  0.218212  ...  0.022700 -0.015847  0.075295   \n",
       "2130  0.233237  0.342548  0.342609  ...  0.006494  0.017914  0.009569   \n",
       "2131  0.081396  0.184924  0.190152  ...  0.004409 -0.026145  0.009127   \n",
       "2132  0.151050  0.226627  0.218560  ...  0.004474  0.000246 -0.018317   \n",
       "\n",
       "          2126      2127      2128      2129      2130      2131      2132  \n",
       "0    -0.021728 -0.012977  0.001865  0.012468  0.031884 -0.027774  0.058370  \n",
       "1     0.117371 -0.017119  0.053576  0.212881  0.342831  0.173077  0.219727  \n",
       "2     0.117351 -0.017201  0.053498  0.212889  0.342785  0.173016  0.219646  \n",
       "3     0.064926 -0.026235  0.043314  0.181134  0.332638  0.137266  0.205303  \n",
       "4     0.022735 -0.013150  0.045537  0.110449  0.409722  0.210139  0.231743  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2128 -0.009945 -0.009870  1.000000  0.063520  0.047193  0.032638  0.027846  \n",
       "2129 -0.020981 -0.004391  0.063520  1.000000  0.026101  0.049729  0.000372  \n",
       "2130 -0.004214 -0.020504  0.047193  0.026101  1.000000  0.123067  0.127682  \n",
       "2131  0.051670 -0.029027  0.032638  0.049729  0.123067  1.000000  0.082630  \n",
       "2132 -0.025196 -0.024145  0.027846  0.000372  0.127682  0.082630  1.000000  \n",
       "\n",
       "[2133 rows x 2133 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.corrcoef(v_scaled.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b92368f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = pd.DataFrame(np.corrcoef(v_scaled.T))\n",
    "\n",
    "# Initialize an empty list to store line numbers and values\n",
    "filtered_values_per_line = []\n",
    "\n",
    "# Iterate through the DataFrame and store line numbers and values\n",
    "#for i, row in enumerate(corr_matrix.values):\n",
    "#    line_values = [(j, value) for j, value in enumerate(row) if abs(value) > 0. and abs(value) < 1]\n",
    "#    if line_values:\n",
    "#        filtered_values_per_line.append(line_values)\n",
    "\n",
    "# Display the filtered values for each line\n",
    "#for line_number, values in enumerate(filtered_values_per_line):\n",
    "#    print(f\"Line {line_number}: {values}\")\n",
    "   \n",
    "for row, value in enumerate(corr_matrix.values[0, 1:]):\n",
    "    if value > 0.05:\n",
    "        line_values = (row, value)\n",
    "        filtered_values_per_line.append(line_values)\n",
    "\n",
    "#for values, line_number in enumerate(filtered_values_per_line):\n",
    "    #print(f\"Line {line_number}: {values}\")\n",
    "\n",
    "len(filtered_values_per_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08cfb4a-f46b-48d0-a0f4-841ab9f71337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1758"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list to store line numbers and values\n",
    "filtered_values_per_line = []\n",
    "\n",
    "for row, value in enumerate(corr_matrix.values[0, 1:]):\n",
    "    if value < 0.05: #0.05 to have a total of 374 features\n",
    "        line_values = (row, value)\n",
    "        filtered_values_per_line.append(line_values)\n",
    "\n",
    "features_to_remove = []\n",
    "for values, line_number in enumerate(filtered_values_per_line):\n",
    "    #print(f\"Feature {line_number}: {values}\")\n",
    "    features_to_remove.append(line_number[0])\n",
    "\n",
    "len(filtered_values_per_line)\n",
    "#print (features_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8435ada5-e330-4eb6-9c6b-97fb6da18e5f",
   "metadata": {},
   "source": [
    "<h4>New X-train with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a5bda67-9628-4370-a4d8-e34edf6cc899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7337, 374)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cut= np.delete(X_train_scaled, features_to_remove, 1)\n",
    "\n",
    "N_cut,M_cut=X_train_cut.shape\n",
    "X_train_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "574ceca3-4ba3-45e9-9ea4-bb69f7108427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(816, 374)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ivs_cut= np.delete(X_ivs_scaled, features_to_remove, 1)\n",
    "\n",
    "X_ivs_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "084e4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.unique(X_train[: 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0975c",
   "metadata": {},
   "source": [
    "<h3>Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e949059",
   "metadata": {},
   "source": [
    "<h4>Stepwise fowards feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "745da56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This approach of feature selection takes to long on big datasets (7000+, 300+)\n",
    "\n",
    "#N,M=X_train_cut.shape\n",
    "\n",
    "#using linear regression for sequential feature selection\n",
    "#lmr=LinearRegression()\n",
    "#sfs = SequentialFeatureSelector(lmr, n_features_to_select=50, n_jobs=16)\n",
    "#sfs.fit(X_train_cut, y_train)\n",
    "\n",
    "#get the relevant columns\n",
    "#features=sfs.get_support()\n",
    "#Features_selected =np.arange(M)[features]\n",
    "#print(\"The features selected are columns: \", Features_selected)\n",
    "\n",
    "#X_train_cut_sfs=sfs.transform(X_train_cut)\n",
    "#X_test_cut_sfs=sfs.transform(X_ivs_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ca76c",
   "metadata": {},
   "source": [
    "<h4>RFs for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3399dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rffeatureSelect(x_train, x_ivs, n_features, t):\n",
    "    rfr=RandomForestRegressor(random_state=0, n_jobs=6) #n_jobs=8\n",
    "    rfr.fit(x_train, y_train)\n",
    "    #for i, imp in enumerate(rfr.feature_importances_):\n",
    "    #    print(\"Feature\", i, \"Importance:\", imp )\n",
    "    \n",
    "    sel = SelectFromModel(estimator=rfr, threshold= t) #Change the threshold! See what happens!\n",
    "    sel.fit(x_train, y_train)\n",
    "    \n",
    "    #print(\"Importances: \", sel.estimator.feature_importances_)\n",
    "    \n",
    "    #print(\"Default threshold: \", sel.threshold_)\n",
    "    \n",
    "    features=sel.get_support()\n",
    "    Features_selected =np.arange(n_features)[features]\n",
    "    print(\"The features selected are columns: \", Features_selected,\".\\n Number of features:\", len(Features_selected))\n",
    "    \n",
    "    X_train_rffs=sel.transform(x_train)\n",
    "    X_test_rffs=sel.transform(x_ivs)\n",
    "    return X_train_rffs, X_test_rffs\n",
    "#naif_model_testingR(nX_train_cut, nX_test_cut, y_train, y_ivs) # y_ivs doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80b03c65-0ab8-449e-b591-ea796349dee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features selected are columns:  [   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "   14   15   16   17   19   22   23   24   25   26   27   28   29   30\n",
      "   31   32   33   34   35   36   37   38   39   40   41   42   96  154\n",
      "  186  216  246  287  303  316  336  348  353  362  364  386  440  442\n",
      "  444  472  494  502  536  546  551  561  675  679  695  769  778  790\n",
      "  815  819  839  853  871  879  886  889  891  909  924  939  956  959\n",
      "  981  982 1013 1020 1029 1037 1041 1054 1063 1087 1088 1139 1147 1161\n",
      " 1167 1176 1214 1248 1283 1284 1285 1333 1377 1397 1440 1454 1499 1527\n",
      " 1540 1575 1598 1604 1641 1665 1667 1687 1709 1722 1752 1778 1811 1841\n",
      " 1857 1868 1893 1916 1921 1922 1931 1934 1968 1992 1998 2004 2017 2069\n",
      " 2076 2080 2122] .\n",
      " Number of features: 143\n"
     ]
    }
   ],
   "source": [
    "#takes around 5 min to execute\n",
    "X_train_rffs, X_test_rffs = rffeatureSelect(X_train, X_ivs, M, .001)#.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4356313-917f-4f30-be4c-a31afc006403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features selected are columns:  [   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "   14   15   16   17   19   22   23   24   25   26   27   28   29   30\n",
      "   31   32   33   34   35   36   37   38   39   40   41   42   96  154\n",
      "  186  216  246  287  303  316  336  348  353  362  364  377  386  440\n",
      "  442  444  472  494  502  546  551  561  675  679  695  769  778  790\n",
      "  815  819  839  853  871  879  886  889  891  909  924  939  956  959\n",
      "  981  982 1013 1020 1029 1037 1041 1054 1063 1087 1088 1139 1147 1161\n",
      " 1167 1176 1214 1248 1283 1284 1285 1333 1397 1440 1454 1499 1527 1540\n",
      " 1575 1598 1604 1641 1649 1652 1665 1667 1686 1687 1709 1752 1778 1811\n",
      " 1841 1857 1868 1893 1916 1921 1922 1931 1934 1968 1992 1998 2004 2017\n",
      " 2069 2076 2122] .\n",
      " Number of features: 143\n"
     ]
    }
   ],
   "source": [
    "#takes around 5 min to execute\n",
    "X_train_scaled_rffs, X_test_scaled_rffs = rffeatureSelect(X_train_scaled, X_ivs_scaled, M, .001)#.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b6a5f93-2c17-4180-ad10-6a5e1004d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features selected are columns:  [  0   1   3   4   5   6   8   9  11  16  19  20  21  22  23  24  25  26\n",
      "  27  28  29  30  31  32  33  34  35  36  50  68  74  78  79  90  92 102\n",
      " 140 161 167 178 187 192 201 240 267 272 298 299 306 327 343] .\n",
      " Number of features: 51\n"
     ]
    }
   ],
   "source": [
    "X_train_cut_rffs, X_test_cut_rffs= rffeatureSelect(X_train_cut, X_ivs_cut, M_cut, .005)#.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1580a5",
   "metadata": {},
   "source": [
    "<h4>Principal component analisys (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49f9a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(x_train, n_comps):\n",
    "    pca = PCA(n_components=n_comps)\n",
    "    pca.fit(x_train)\n",
    "    tve=0\n",
    "    for i, ve in enumerate(pca.explained_variance_ratio_):\n",
    "        tve+=ve\n",
    "        print(\"PC%d - Variance explained: %7.4f - Total Variance: %7.4f\" % (i, ve, tve) )\n",
    "    #print()\n",
    "    #print(\"Actual Eigenvalues:\", pca.singular_values_)\n",
    "    #for i,comp in enumerate(pca.components_):\n",
    "    #    print(\"PC\",i, \"-->\", comp)    \n",
    "    return pca\n",
    "\n",
    "def kpca(x_train, n_comps):\n",
    "    kpca = KernelPCA(n_components=n_comps, kernel='rbf')#, gamma=3)\n",
    "    kpca.fit(x_train)\n",
    "    X_train_kpca = kpca.transform(x_train)\n",
    "    return X_train_kpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6254d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = pca(X_train, 100) \n",
    "X_train_pca = pca1.transform(X_train)\n",
    "#X_test_pca=pca.transform(X_ivs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2aa0f9cf-c6d8-41cd-8a9a-b2e71f11fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_kpca =kpca(X_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "092add44-4336-4e85-bd1b-ad012cd63cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.0259 - Total Variance:  0.0259\n",
      "PC1 - Variance explained:  0.0131 - Total Variance:  0.0389\n",
      "PC2 - Variance explained:  0.0126 - Total Variance:  0.0515\n",
      "PC3 - Variance explained:  0.0116 - Total Variance:  0.0631\n",
      "PC4 - Variance explained:  0.0099 - Total Variance:  0.0730\n",
      "PC5 - Variance explained:  0.0077 - Total Variance:  0.0807\n",
      "PC6 - Variance explained:  0.0074 - Total Variance:  0.0881\n",
      "PC7 - Variance explained:  0.0068 - Total Variance:  0.0949\n",
      "PC8 - Variance explained:  0.0066 - Total Variance:  0.1016\n",
      "PC9 - Variance explained:  0.0064 - Total Variance:  0.1079\n",
      "PC10 - Variance explained:  0.0062 - Total Variance:  0.1141\n",
      "PC11 - Variance explained:  0.0059 - Total Variance:  0.1200\n",
      "PC12 - Variance explained:  0.0057 - Total Variance:  0.1258\n",
      "PC13 - Variance explained:  0.0054 - Total Variance:  0.1312\n",
      "PC14 - Variance explained:  0.0052 - Total Variance:  0.1364\n",
      "PC15 - Variance explained:  0.0051 - Total Variance:  0.1415\n",
      "PC16 - Variance explained:  0.0051 - Total Variance:  0.1465\n",
      "PC17 - Variance explained:  0.0048 - Total Variance:  0.1513\n",
      "PC18 - Variance explained:  0.0048 - Total Variance:  0.1561\n",
      "PC19 - Variance explained:  0.0047 - Total Variance:  0.1608\n",
      "PC20 - Variance explained:  0.0047 - Total Variance:  0.1655\n",
      "PC21 - Variance explained:  0.0046 - Total Variance:  0.1700\n",
      "PC22 - Variance explained:  0.0045 - Total Variance:  0.1745\n",
      "PC23 - Variance explained:  0.0044 - Total Variance:  0.1788\n",
      "PC24 - Variance explained:  0.0043 - Total Variance:  0.1832\n",
      "PC25 - Variance explained:  0.0043 - Total Variance:  0.1874\n",
      "PC26 - Variance explained:  0.0042 - Total Variance:  0.1916\n",
      "PC27 - Variance explained:  0.0042 - Total Variance:  0.1958\n",
      "PC28 - Variance explained:  0.0040 - Total Variance:  0.1997\n",
      "PC29 - Variance explained:  0.0040 - Total Variance:  0.2037\n",
      "PC30 - Variance explained:  0.0039 - Total Variance:  0.2077\n",
      "PC31 - Variance explained:  0.0038 - Total Variance:  0.2115\n",
      "PC32 - Variance explained:  0.0037 - Total Variance:  0.2152\n",
      "PC33 - Variance explained:  0.0037 - Total Variance:  0.2189\n",
      "PC34 - Variance explained:  0.0037 - Total Variance:  0.2226\n",
      "PC35 - Variance explained:  0.0036 - Total Variance:  0.2262\n",
      "PC36 - Variance explained:  0.0035 - Total Variance:  0.2297\n",
      "PC37 - Variance explained:  0.0035 - Total Variance:  0.2332\n",
      "PC38 - Variance explained:  0.0035 - Total Variance:  0.2367\n",
      "PC39 - Variance explained:  0.0034 - Total Variance:  0.2401\n",
      "PC40 - Variance explained:  0.0033 - Total Variance:  0.2434\n",
      "PC41 - Variance explained:  0.0033 - Total Variance:  0.2467\n",
      "PC42 - Variance explained:  0.0033 - Total Variance:  0.2500\n",
      "PC43 - Variance explained:  0.0032 - Total Variance:  0.2532\n",
      "PC44 - Variance explained:  0.0031 - Total Variance:  0.2563\n",
      "PC45 - Variance explained:  0.0031 - Total Variance:  0.2594\n",
      "PC46 - Variance explained:  0.0031 - Total Variance:  0.2625\n",
      "PC47 - Variance explained:  0.0030 - Total Variance:  0.2655\n",
      "PC48 - Variance explained:  0.0030 - Total Variance:  0.2685\n",
      "PC49 - Variance explained:  0.0030 - Total Variance:  0.2715\n",
      "PC50 - Variance explained:  0.0029 - Total Variance:  0.2744\n",
      "PC51 - Variance explained:  0.0029 - Total Variance:  0.2773\n",
      "PC52 - Variance explained:  0.0028 - Total Variance:  0.2801\n",
      "PC53 - Variance explained:  0.0028 - Total Variance:  0.2830\n",
      "PC54 - Variance explained:  0.0028 - Total Variance:  0.2857\n",
      "PC55 - Variance explained:  0.0027 - Total Variance:  0.2885\n",
      "PC56 - Variance explained:  0.0027 - Total Variance:  0.2912\n",
      "PC57 - Variance explained:  0.0027 - Total Variance:  0.2939\n",
      "PC58 - Variance explained:  0.0027 - Total Variance:  0.2966\n",
      "PC59 - Variance explained:  0.0026 - Total Variance:  0.2992\n",
      "PC60 - Variance explained:  0.0026 - Total Variance:  0.3019\n",
      "PC61 - Variance explained:  0.0026 - Total Variance:  0.3044\n",
      "PC62 - Variance explained:  0.0026 - Total Variance:  0.3070\n",
      "PC63 - Variance explained:  0.0025 - Total Variance:  0.3095\n",
      "PC64 - Variance explained:  0.0025 - Total Variance:  0.3120\n",
      "PC65 - Variance explained:  0.0025 - Total Variance:  0.3145\n",
      "PC66 - Variance explained:  0.0025 - Total Variance:  0.3170\n",
      "PC67 - Variance explained:  0.0024 - Total Variance:  0.3194\n",
      "PC68 - Variance explained:  0.0024 - Total Variance:  0.3219\n",
      "PC69 - Variance explained:  0.0024 - Total Variance:  0.3243\n",
      "PC70 - Variance explained:  0.0024 - Total Variance:  0.3266\n",
      "PC71 - Variance explained:  0.0024 - Total Variance:  0.3290\n",
      "PC72 - Variance explained:  0.0023 - Total Variance:  0.3314\n",
      "PC73 - Variance explained:  0.0023 - Total Variance:  0.3337\n",
      "PC74 - Variance explained:  0.0023 - Total Variance:  0.3360\n",
      "PC75 - Variance explained:  0.0023 - Total Variance:  0.3383\n",
      "PC76 - Variance explained:  0.0023 - Total Variance:  0.3406\n",
      "PC77 - Variance explained:  0.0023 - Total Variance:  0.3428\n",
      "PC78 - Variance explained:  0.0022 - Total Variance:  0.3451\n",
      "PC79 - Variance explained:  0.0022 - Total Variance:  0.3473\n",
      "PC80 - Variance explained:  0.0022 - Total Variance:  0.3495\n",
      "PC81 - Variance explained:  0.0022 - Total Variance:  0.3517\n",
      "PC82 - Variance explained:  0.0022 - Total Variance:  0.3539\n",
      "PC83 - Variance explained:  0.0022 - Total Variance:  0.3561\n",
      "PC84 - Variance explained:  0.0021 - Total Variance:  0.3582\n",
      "PC85 - Variance explained:  0.0021 - Total Variance:  0.3603\n",
      "PC86 - Variance explained:  0.0021 - Total Variance:  0.3624\n",
      "PC87 - Variance explained:  0.0021 - Total Variance:  0.3645\n",
      "PC88 - Variance explained:  0.0021 - Total Variance:  0.3666\n",
      "PC89 - Variance explained:  0.0021 - Total Variance:  0.3687\n",
      "PC90 - Variance explained:  0.0021 - Total Variance:  0.3708\n",
      "PC91 - Variance explained:  0.0020 - Total Variance:  0.3728\n",
      "PC92 - Variance explained:  0.0020 - Total Variance:  0.3748\n",
      "PC93 - Variance explained:  0.0020 - Total Variance:  0.3768\n",
      "PC94 - Variance explained:  0.0020 - Total Variance:  0.3788\n",
      "PC95 - Variance explained:  0.0020 - Total Variance:  0.3808\n",
      "PC96 - Variance explained:  0.0020 - Total Variance:  0.3828\n",
      "PC97 - Variance explained:  0.0020 - Total Variance:  0.3848\n",
      "PC98 - Variance explained:  0.0020 - Total Variance:  0.3867\n",
      "PC99 - Variance explained:  0.0019 - Total Variance:  0.3887\n",
      "PC100 - Variance explained:  0.0019 - Total Variance:  0.3906\n",
      "PC101 - Variance explained:  0.0019 - Total Variance:  0.3925\n",
      "PC102 - Variance explained:  0.0019 - Total Variance:  0.3944\n",
      "PC103 - Variance explained:  0.0019 - Total Variance:  0.3963\n",
      "PC104 - Variance explained:  0.0019 - Total Variance:  0.3982\n",
      "PC105 - Variance explained:  0.0018 - Total Variance:  0.4000\n",
      "PC106 - Variance explained:  0.0018 - Total Variance:  0.4019\n",
      "PC107 - Variance explained:  0.0018 - Total Variance:  0.4037\n",
      "PC108 - Variance explained:  0.0018 - Total Variance:  0.4055\n",
      "PC109 - Variance explained:  0.0018 - Total Variance:  0.4073\n",
      "PC110 - Variance explained:  0.0018 - Total Variance:  0.4091\n",
      "PC111 - Variance explained:  0.0018 - Total Variance:  0.4109\n",
      "PC112 - Variance explained:  0.0018 - Total Variance:  0.4126\n",
      "PC113 - Variance explained:  0.0017 - Total Variance:  0.4144\n",
      "PC114 - Variance explained:  0.0017 - Total Variance:  0.4161\n",
      "PC115 - Variance explained:  0.0017 - Total Variance:  0.4178\n",
      "PC116 - Variance explained:  0.0017 - Total Variance:  0.4195\n",
      "PC117 - Variance explained:  0.0017 - Total Variance:  0.4213\n",
      "PC118 - Variance explained:  0.0017 - Total Variance:  0.4230\n",
      "PC119 - Variance explained:  0.0017 - Total Variance:  0.4247\n",
      "PC120 - Variance explained:  0.0017 - Total Variance:  0.4263\n",
      "PC121 - Variance explained:  0.0017 - Total Variance:  0.4280\n",
      "PC122 - Variance explained:  0.0017 - Total Variance:  0.4296\n",
      "PC123 - Variance explained:  0.0016 - Total Variance:  0.4313\n",
      "PC124 - Variance explained:  0.0016 - Total Variance:  0.4329\n",
      "PC125 - Variance explained:  0.0016 - Total Variance:  0.4345\n",
      "PC126 - Variance explained:  0.0016 - Total Variance:  0.4361\n",
      "PC127 - Variance explained:  0.0016 - Total Variance:  0.4378\n",
      "PC128 - Variance explained:  0.0016 - Total Variance:  0.4393\n",
      "PC129 - Variance explained:  0.0016 - Total Variance:  0.4409\n",
      "PC130 - Variance explained:  0.0016 - Total Variance:  0.4425\n",
      "PC131 - Variance explained:  0.0016 - Total Variance:  0.4441\n",
      "PC132 - Variance explained:  0.0016 - Total Variance:  0.4456\n",
      "PC133 - Variance explained:  0.0016 - Total Variance:  0.4472\n",
      "PC134 - Variance explained:  0.0016 - Total Variance:  0.4487\n",
      "PC135 - Variance explained:  0.0015 - Total Variance:  0.4503\n",
      "PC136 - Variance explained:  0.0015 - Total Variance:  0.4518\n",
      "PC137 - Variance explained:  0.0015 - Total Variance:  0.4533\n",
      "PC138 - Variance explained:  0.0015 - Total Variance:  0.4548\n",
      "PC139 - Variance explained:  0.0015 - Total Variance:  0.4563\n",
      "PC140 - Variance explained:  0.0015 - Total Variance:  0.4578\n",
      "PC141 - Variance explained:  0.0015 - Total Variance:  0.4592\n",
      "PC142 - Variance explained:  0.0014 - Total Variance:  0.4607\n",
      "PC143 - Variance explained:  0.0014 - Total Variance:  0.4621\n",
      "PC144 - Variance explained:  0.0014 - Total Variance:  0.4635\n",
      "PC145 - Variance explained:  0.0014 - Total Variance:  0.4649\n",
      "PC146 - Variance explained:  0.0014 - Total Variance:  0.4664\n",
      "PC147 - Variance explained:  0.0014 - Total Variance:  0.4678\n",
      "PC148 - Variance explained:  0.0014 - Total Variance:  0.4692\n",
      "PC149 - Variance explained:  0.0014 - Total Variance:  0.4705\n"
     ]
    }
   ],
   "source": [
    "pca_scaled = pca(X_train_scaled, 150) \n",
    "X_train_scaled_pca = pca_scaled.transform(X_train_scaled)\n",
    "#X_test_scaled_pca=pca_scaled.transform(X_ivs_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "20afd6f6-5429-45c6-946b-2406a4e5bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_kpca =kpca(X_train_scaled, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67ec5b00-3398-4861-a1b2-afe4355c2ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.0862 - Total Variance:  0.0862\n",
      "PC1 - Variance explained:  0.0406 - Total Variance:  0.1268\n",
      "PC2 - Variance explained:  0.0323 - Total Variance:  0.1591\n",
      "PC3 - Variance explained:  0.0293 - Total Variance:  0.1884\n",
      "PC4 - Variance explained:  0.0272 - Total Variance:  0.2156\n",
      "PC5 - Variance explained:  0.0256 - Total Variance:  0.2412\n",
      "PC6 - Variance explained:  0.0245 - Total Variance:  0.2657\n",
      "PC7 - Variance explained:  0.0191 - Total Variance:  0.2848\n",
      "PC8 - Variance explained:  0.0169 - Total Variance:  0.3017\n",
      "PC9 - Variance explained:  0.0163 - Total Variance:  0.3180\n",
      "PC10 - Variance explained:  0.0137 - Total Variance:  0.3317\n",
      "PC11 - Variance explained:  0.0133 - Total Variance:  0.3450\n",
      "PC12 - Variance explained:  0.0124 - Total Variance:  0.3573\n",
      "PC13 - Variance explained:  0.0112 - Total Variance:  0.3685\n",
      "PC14 - Variance explained:  0.0109 - Total Variance:  0.3794\n",
      "PC15 - Variance explained:  0.0107 - Total Variance:  0.3901\n",
      "PC16 - Variance explained:  0.0099 - Total Variance:  0.4000\n",
      "PC17 - Variance explained:  0.0095 - Total Variance:  0.4095\n",
      "PC18 - Variance explained:  0.0089 - Total Variance:  0.4183\n",
      "PC19 - Variance explained:  0.0086 - Total Variance:  0.4269\n",
      "PC20 - Variance explained:  0.0083 - Total Variance:  0.4352\n",
      "PC21 - Variance explained:  0.0079 - Total Variance:  0.4431\n",
      "PC22 - Variance explained:  0.0074 - Total Variance:  0.4504\n",
      "PC23 - Variance explained:  0.0073 - Total Variance:  0.4577\n",
      "PC24 - Variance explained:  0.0071 - Total Variance:  0.4648\n",
      "PC25 - Variance explained:  0.0069 - Total Variance:  0.4718\n",
      "PC26 - Variance explained:  0.0066 - Total Variance:  0.4784\n",
      "PC27 - Variance explained:  0.0065 - Total Variance:  0.4849\n",
      "PC28 - Variance explained:  0.0063 - Total Variance:  0.4912\n",
      "PC29 - Variance explained:  0.0061 - Total Variance:  0.4973\n",
      "PC30 - Variance explained:  0.0060 - Total Variance:  0.5033\n",
      "PC31 - Variance explained:  0.0059 - Total Variance:  0.5092\n",
      "PC32 - Variance explained:  0.0056 - Total Variance:  0.5148\n",
      "PC33 - Variance explained:  0.0054 - Total Variance:  0.5202\n",
      "PC34 - Variance explained:  0.0052 - Total Variance:  0.5254\n",
      "PC35 - Variance explained:  0.0051 - Total Variance:  0.5305\n",
      "PC36 - Variance explained:  0.0051 - Total Variance:  0.5355\n",
      "PC37 - Variance explained:  0.0049 - Total Variance:  0.5405\n",
      "PC38 - Variance explained:  0.0047 - Total Variance:  0.5452\n",
      "PC39 - Variance explained:  0.0047 - Total Variance:  0.5498\n",
      "PC40 - Variance explained:  0.0046 - Total Variance:  0.5544\n",
      "PC41 - Variance explained:  0.0045 - Total Variance:  0.5589\n",
      "PC42 - Variance explained:  0.0043 - Total Variance:  0.5632\n",
      "PC43 - Variance explained:  0.0042 - Total Variance:  0.5674\n",
      "PC44 - Variance explained:  0.0042 - Total Variance:  0.5716\n",
      "PC45 - Variance explained:  0.0042 - Total Variance:  0.5758\n",
      "PC46 - Variance explained:  0.0040 - Total Variance:  0.5798\n",
      "PC47 - Variance explained:  0.0039 - Total Variance:  0.5837\n",
      "PC48 - Variance explained:  0.0039 - Total Variance:  0.5876\n",
      "PC49 - Variance explained:  0.0039 - Total Variance:  0.5915\n",
      "PC50 - Variance explained:  0.0038 - Total Variance:  0.5953\n",
      "PC51 - Variance explained:  0.0037 - Total Variance:  0.5991\n",
      "PC52 - Variance explained:  0.0037 - Total Variance:  0.6028\n",
      "PC53 - Variance explained:  0.0037 - Total Variance:  0.6064\n",
      "PC54 - Variance explained:  0.0036 - Total Variance:  0.6100\n",
      "PC55 - Variance explained:  0.0036 - Total Variance:  0.6136\n",
      "PC56 - Variance explained:  0.0035 - Total Variance:  0.6172\n",
      "PC57 - Variance explained:  0.0034 - Total Variance:  0.6206\n",
      "PC58 - Variance explained:  0.0034 - Total Variance:  0.6240\n",
      "PC59 - Variance explained:  0.0034 - Total Variance:  0.6274\n",
      "PC60 - Variance explained:  0.0033 - Total Variance:  0.6307\n",
      "PC61 - Variance explained:  0.0033 - Total Variance:  0.6340\n",
      "PC62 - Variance explained:  0.0032 - Total Variance:  0.6372\n",
      "PC63 - Variance explained:  0.0032 - Total Variance:  0.6404\n",
      "PC64 - Variance explained:  0.0032 - Total Variance:  0.6436\n",
      "PC65 - Variance explained:  0.0032 - Total Variance:  0.6468\n",
      "PC66 - Variance explained:  0.0031 - Total Variance:  0.6499\n",
      "PC67 - Variance explained:  0.0030 - Total Variance:  0.6529\n",
      "PC68 - Variance explained:  0.0030 - Total Variance:  0.6559\n",
      "PC69 - Variance explained:  0.0030 - Total Variance:  0.6589\n",
      "PC70 - Variance explained:  0.0029 - Total Variance:  0.6618\n",
      "PC71 - Variance explained:  0.0029 - Total Variance:  0.6647\n",
      "PC72 - Variance explained:  0.0029 - Total Variance:  0.6676\n",
      "PC73 - Variance explained:  0.0029 - Total Variance:  0.6705\n",
      "PC74 - Variance explained:  0.0028 - Total Variance:  0.6733\n",
      "PC75 - Variance explained:  0.0028 - Total Variance:  0.6761\n",
      "PC76 - Variance explained:  0.0028 - Total Variance:  0.6788\n",
      "PC77 - Variance explained:  0.0027 - Total Variance:  0.6816\n",
      "PC78 - Variance explained:  0.0027 - Total Variance:  0.6842\n",
      "PC79 - Variance explained:  0.0027 - Total Variance:  0.6869\n",
      "PC80 - Variance explained:  0.0026 - Total Variance:  0.6895\n",
      "PC81 - Variance explained:  0.0026 - Total Variance:  0.6921\n",
      "PC82 - Variance explained:  0.0026 - Total Variance:  0.6947\n",
      "PC83 - Variance explained:  0.0026 - Total Variance:  0.6973\n",
      "PC84 - Variance explained:  0.0025 - Total Variance:  0.6998\n",
      "PC85 - Variance explained:  0.0025 - Total Variance:  0.7023\n",
      "PC86 - Variance explained:  0.0025 - Total Variance:  0.7048\n",
      "PC87 - Variance explained:  0.0025 - Total Variance:  0.7073\n",
      "PC88 - Variance explained:  0.0024 - Total Variance:  0.7097\n",
      "PC89 - Variance explained:  0.0024 - Total Variance:  0.7121\n",
      "PC90 - Variance explained:  0.0024 - Total Variance:  0.7145\n",
      "PC91 - Variance explained:  0.0024 - Total Variance:  0.7168\n",
      "PC92 - Variance explained:  0.0023 - Total Variance:  0.7192\n",
      "PC93 - Variance explained:  0.0023 - Total Variance:  0.7214\n",
      "PC94 - Variance explained:  0.0023 - Total Variance:  0.7237\n",
      "PC95 - Variance explained:  0.0022 - Total Variance:  0.7260\n",
      "PC96 - Variance explained:  0.0022 - Total Variance:  0.7282\n",
      "PC97 - Variance explained:  0.0022 - Total Variance:  0.7303\n",
      "PC98 - Variance explained:  0.0021 - Total Variance:  0.7325\n",
      "PC99 - Variance explained:  0.0021 - Total Variance:  0.7346\n"
     ]
    }
   ],
   "source": [
    "pca_cut = pca(X_train_cut, 100)\n",
    "X_train_cut_pca = pca_cut.transform(X_train_cut)\n",
    "#X_test_cut_pca=pca_cut.transform(X_ivs_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e8fee1ee-28b4-4594-8249-2c974586ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cut_kpca = kpca(X_train_cut, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327c040-ab6c-4f7b-904f-5faa56139899",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_rffs = pca(X_train_rffs, 70)\n",
    "X_train_rffs_pca = pca_rffs.transform(X_train_rffs)\n",
    "#X_test_rffs_pca=pca_rffs.transform(X_test_rffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b2ffb0a2-051e-4669-9941-3ba3b78b1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rffs_kpca =kpca(X_train_rffs, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3bdee35-ff0a-423d-930c-551ad7b0d17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.1980 - Total Variance:  0.1980\n",
      "PC1 - Variance explained:  0.0486 - Total Variance:  0.2466\n",
      "PC2 - Variance explained:  0.0421 - Total Variance:  0.2886\n",
      "PC3 - Variance explained:  0.0387 - Total Variance:  0.3273\n",
      "PC4 - Variance explained:  0.0298 - Total Variance:  0.3571\n",
      "PC5 - Variance explained:  0.0253 - Total Variance:  0.3824\n",
      "PC6 - Variance explained:  0.0217 - Total Variance:  0.4042\n",
      "PC7 - Variance explained:  0.0211 - Total Variance:  0.4252\n",
      "PC8 - Variance explained:  0.0205 - Total Variance:  0.4457\n",
      "PC9 - Variance explained:  0.0192 - Total Variance:  0.4649\n",
      "PC10 - Variance explained:  0.0178 - Total Variance:  0.4828\n",
      "PC11 - Variance explained:  0.0164 - Total Variance:  0.4991\n",
      "PC12 - Variance explained:  0.0152 - Total Variance:  0.5144\n",
      "PC13 - Variance explained:  0.0142 - Total Variance:  0.5286\n",
      "PC14 - Variance explained:  0.0132 - Total Variance:  0.5417\n",
      "PC15 - Variance explained:  0.0128 - Total Variance:  0.5546\n",
      "PC16 - Variance explained:  0.0118 - Total Variance:  0.5664\n",
      "PC17 - Variance explained:  0.0116 - Total Variance:  0.5780\n",
      "PC18 - Variance explained:  0.0114 - Total Variance:  0.5894\n",
      "PC19 - Variance explained:  0.0110 - Total Variance:  0.6004\n",
      "PC20 - Variance explained:  0.0105 - Total Variance:  0.6109\n",
      "PC21 - Variance explained:  0.0103 - Total Variance:  0.6212\n",
      "PC22 - Variance explained:  0.0098 - Total Variance:  0.6310\n",
      "PC23 - Variance explained:  0.0097 - Total Variance:  0.6407\n",
      "PC24 - Variance explained:  0.0093 - Total Variance:  0.6500\n",
      "PC25 - Variance explained:  0.0091 - Total Variance:  0.6591\n",
      "PC26 - Variance explained:  0.0089 - Total Variance:  0.6680\n",
      "PC27 - Variance explained:  0.0085 - Total Variance:  0.6765\n",
      "PC28 - Variance explained:  0.0082 - Total Variance:  0.6846\n",
      "PC29 - Variance explained:  0.0080 - Total Variance:  0.6926\n",
      "PC30 - Variance explained:  0.0079 - Total Variance:  0.7005\n",
      "PC31 - Variance explained:  0.0077 - Total Variance:  0.7081\n",
      "PC32 - Variance explained:  0.0076 - Total Variance:  0.7157\n",
      "PC33 - Variance explained:  0.0076 - Total Variance:  0.7233\n",
      "PC34 - Variance explained:  0.0073 - Total Variance:  0.7307\n",
      "PC35 - Variance explained:  0.0072 - Total Variance:  0.7378\n",
      "PC36 - Variance explained:  0.0070 - Total Variance:  0.7449\n",
      "PC37 - Variance explained:  0.0068 - Total Variance:  0.7517\n",
      "PC38 - Variance explained:  0.0067 - Total Variance:  0.7584\n",
      "PC39 - Variance explained:  0.0066 - Total Variance:  0.7650\n",
      "PC40 - Variance explained:  0.0063 - Total Variance:  0.7713\n",
      "PC41 - Variance explained:  0.0062 - Total Variance:  0.7775\n",
      "PC42 - Variance explained:  0.0061 - Total Variance:  0.7836\n",
      "PC43 - Variance explained:  0.0060 - Total Variance:  0.7896\n",
      "PC44 - Variance explained:  0.0058 - Total Variance:  0.7954\n",
      "PC45 - Variance explained:  0.0056 - Total Variance:  0.8010\n",
      "PC46 - Variance explained:  0.0055 - Total Variance:  0.8066\n",
      "PC47 - Variance explained:  0.0054 - Total Variance:  0.8120\n",
      "PC48 - Variance explained:  0.0053 - Total Variance:  0.8172\n",
      "PC49 - Variance explained:  0.0052 - Total Variance:  0.8225\n",
      "PC50 - Variance explained:  0.0051 - Total Variance:  0.8276\n",
      "PC51 - Variance explained:  0.0050 - Total Variance:  0.8326\n",
      "PC52 - Variance explained:  0.0049 - Total Variance:  0.8375\n",
      "PC53 - Variance explained:  0.0048 - Total Variance:  0.8424\n",
      "PC54 - Variance explained:  0.0048 - Total Variance:  0.8471\n",
      "PC55 - Variance explained:  0.0046 - Total Variance:  0.8518\n",
      "PC56 - Variance explained:  0.0046 - Total Variance:  0.8564\n",
      "PC57 - Variance explained:  0.0044 - Total Variance:  0.8608\n",
      "PC58 - Variance explained:  0.0044 - Total Variance:  0.8652\n",
      "PC59 - Variance explained:  0.0043 - Total Variance:  0.8696\n",
      "PC60 - Variance explained:  0.0043 - Total Variance:  0.8738\n",
      "PC61 - Variance explained:  0.0041 - Total Variance:  0.8779\n",
      "PC62 - Variance explained:  0.0040 - Total Variance:  0.8819\n",
      "PC63 - Variance explained:  0.0040 - Total Variance:  0.8858\n",
      "PC64 - Variance explained:  0.0039 - Total Variance:  0.8898\n",
      "PC65 - Variance explained:  0.0039 - Total Variance:  0.8936\n",
      "PC66 - Variance explained:  0.0037 - Total Variance:  0.8974\n",
      "PC67 - Variance explained:  0.0036 - Total Variance:  0.9010\n",
      "PC68 - Variance explained:  0.0036 - Total Variance:  0.9046\n",
      "PC69 - Variance explained:  0.0035 - Total Variance:  0.9081\n"
     ]
    }
   ],
   "source": [
    "#pca_scaled_rffs\n",
    "pca_scaled_rffs = pca(X_train_scaled_rffs, 70)\n",
    "X_train_scaled_rffs_pca = pca_scaled_rffs.transform(X_train_scaled_rffs)\n",
    "#X_test_scaled_rffs_pca = pca_scaled_rffs.transform(X_test_scaled_rffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1514c36a-1da2-40a6-a0af-3043ea458c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_rffs_kpca = kpca(X_train_scaled_rffs, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bedba7e6-6974-4758-a3b2-409215c41e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.4475 - Total Variance:  0.4475\n",
      "PC1 - Variance explained:  0.0565 - Total Variance:  0.5040\n",
      "PC2 - Variance explained:  0.0497 - Total Variance:  0.5537\n",
      "PC3 - Variance explained:  0.0418 - Total Variance:  0.5955\n",
      "PC4 - Variance explained:  0.0317 - Total Variance:  0.6272\n",
      "PC5 - Variance explained:  0.0297 - Total Variance:  0.6568\n",
      "PC6 - Variance explained:  0.0259 - Total Variance:  0.6827\n",
      "PC7 - Variance explained:  0.0241 - Total Variance:  0.7068\n",
      "PC8 - Variance explained:  0.0210 - Total Variance:  0.7278\n",
      "PC9 - Variance explained:  0.0203 - Total Variance:  0.7481\n",
      "PC10 - Variance explained:  0.0201 - Total Variance:  0.7682\n",
      "PC11 - Variance explained:  0.0197 - Total Variance:  0.7879\n",
      "PC12 - Variance explained:  0.0188 - Total Variance:  0.8067\n",
      "PC13 - Variance explained:  0.0187 - Total Variance:  0.8253\n",
      "PC14 - Variance explained:  0.0180 - Total Variance:  0.8433\n",
      "PC15 - Variance explained:  0.0168 - Total Variance:  0.8601\n",
      "PC16 - Variance explained:  0.0158 - Total Variance:  0.8760\n",
      "PC17 - Variance explained:  0.0149 - Total Variance:  0.8909\n",
      "PC18 - Variance explained:  0.0135 - Total Variance:  0.9044\n",
      "PC19 - Variance explained:  0.0129 - Total Variance:  0.9173\n",
      "PC20 - Variance explained:  0.0119 - Total Variance:  0.9292\n",
      "PC21 - Variance explained:  0.0115 - Total Variance:  0.9407\n",
      "PC22 - Variance explained:  0.0108 - Total Variance:  0.9515\n",
      "PC23 - Variance explained:  0.0100 - Total Variance:  0.9615\n",
      "PC24 - Variance explained:  0.0089 - Total Variance:  0.9703\n"
     ]
    }
   ],
   "source": [
    "pca_cut_rffs = pca(X_train_cut_rffs, 25)\n",
    "X_train_cut_rffs_pca=pca_cut_rffs.transform(X_train_cut_rffs)\n",
    "#X_test_cut_rffs_pca=pca_cut.transform(X_test_cut_rffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4b68e82c-1499-40a6-948c-df2d93f22835",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cut_rffs_kpca = kpca(X_train_cut_rffs, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b50ca8",
   "metadata": {},
   "source": [
    "<h3>Cross-validation. Evaluation of the (models) and data configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ba74a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAvalStat(truth, preds):\n",
    "    avalStats = []\n",
    "    print(\"The RVE is: \", explained_variance_score(truth, preds))\n",
    "    print(\"The rmse is: \", mean_squared_error(truth, preds, squared=False))\n",
    "    corr, pval=pearsonr(truth, preds)\n",
    "    print(\"The Correlation Score is: %6.4f (p-value=%e)\"%(corr,pval))\n",
    "\n",
    "    print(\"The Maximum Error is: \", max_error(truth, preds))\n",
    "    print(\"The Mean Absolute Error is:\", mean_absolute_error(truth, preds),\"\\n\")\n",
    "    avalStats.append(explained_variance_score(truth, preds))\n",
    "    avalStats.append(mean_squared_error(truth, preds, squared=False))\n",
    "    avalStats.append(pearsonr(truth, preds))\n",
    "    #avalStats.append( %(corr,pval) )\n",
    "    return avalStats\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec27ca",
   "metadata": {},
   "source": [
    "<h4>N-Fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "511ffe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfold_valid(X_train_Valids):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=23)\n",
    "    kf.get_n_splits(X_train_Valids)\n",
    "    TRUTH_nfold=None\n",
    "    PREDS_nfold=None\n",
    "    for train_index, test_index in kf.split(X_train_Valids):\n",
    "        X_train_nfold, X_ivs_nfold = X_train_Valids[train_index], X_train_Valids[test_index]\n",
    "        y_train_nfold, y_ivs_nfold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        #mdl = DecisionTreeRegressor()#max_depth = 5)\n",
    "        mdl = RandomForestRegressor(n_estimators=10, random_state=0, min_samples_leaf=3, max_depth = 10, n_jobs=6)\n",
    "        mdl.fit(X_train_nfold, y_train_nfold)\n",
    "        preds = mdl.predict(X_ivs_nfold)\n",
    "        if TRUTH_nfold is None:\n",
    "            PREDS_nfold=preds\n",
    "            TRUTH_nfold=y_ivs_nfold\n",
    "        else:\n",
    "            PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
    "            TRUTH_nfold=np.hstack((TRUTH_nfold, y_ivs_nfold))\n",
    "        \n",
    "    printAvalStat(TRUTH_nfold, PREDS_nfold)\n",
    "\n",
    "#print(\"N-fold cross validation of nXf_train, after sequencial reduction feature selection\")\n",
    "#nfold_valid(nXf_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9fa3d324-7c5e-46a6-ad04-2de4dfe661dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-fold cross validation of X_train\n",
      "The RVE is:  0.4306622630357596\n",
      "The rmse is:  0.2087354179051696\n",
      "The Correlation Score is: 0.6606 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8645408990344696\n",
      "The Mean Absolute Error is: 0.165311491822563 \n",
      "\n",
      "N-fold cross validation of X_train_scaled\n",
      "The RVE is:  0.43079787092361854\n",
      "The rmse is:  0.20871057962911607\n",
      "The Correlation Score is: 0.6607 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8645408990344698\n",
      "The Mean Absolute Error is: 0.16528339261716604 \n",
      "\n",
      "N-fold cross validation of X_train_cut(already scaled!)\n",
      "The RVE is:  0.4262619972524192\n",
      "The rmse is:  0.20955020213931275\n",
      "The Correlation Score is: 0.6565 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8327437900923893\n",
      "The Mean Absolute Error is: 0.16607190021223064 \n",
      "\n",
      "N-fold cross validation of X_train_rffs\n",
      "The RVE is:  0.43427492253919797\n",
      "The rmse is:  0.20807215006462745\n",
      "The Correlation Score is: 0.6632 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8638021741678932\n",
      "The Mean Absolute Error is: 0.16425370405744452 \n",
      "\n",
      "N-fold cross validation of X_train_scaled_rffs\n",
      "The RVE is:  0.43150998780382954\n",
      "The rmse is:  0.20858014037515837\n",
      "The Correlation Score is: 0.6609 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8638021741678932\n",
      "The Mean Absolute Error is: 0.16470707805119614 \n",
      "\n",
      "N-fold cross validation of X_train_cut_rffs(already scaled!)\n",
      "The RVE is:  0.4107818261103744\n",
      "The rmse is:  0.21234864282301666\n",
      "The Correlation Score is: 0.6429 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8977471674351751\n",
      "The Mean Absolute Error is: 0.1679458154947112 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first unchanged data evaluations\n",
    "print(\"N-fold cross validation of X_train\")\n",
    "nfold_valid(X_train)\n",
    "print(\"N-fold cross validation of X_train_scaled\")\n",
    "nfold_valid(X_train_scaled)\n",
    "\n",
    "#manually cut features data evaluations\n",
    "print(\"N-fold cross validation of X_train_cut(already scaled!)\")\n",
    "nfold_valid(X_train_cut)\n",
    "\n",
    "#feature selected data evaluations\n",
    "print(\"N-fold cross validation of X_train_rffs\")\n",
    "nfold_valid(X_train_rffs)\n",
    "print(\"N-fold cross validation of X_train_scaled_rffs\")\n",
    "nfold_valid(X_train_scaled_rffs)\n",
    "\n",
    "#cut x feature selected data evaluation\n",
    "print(\"N-fold cross validation of X_train_cut_rffs(already scaled!)\")\n",
    "nfold_valid(X_train_cut_rffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ccab7bff-953a-4ecb-ac4c-f34d1a6c8e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-fold cross validation of X_train_pca\n",
      "The RVE is:  0.40243392524173227\n",
      "The rmse is:  0.21384983525808984\n",
      "The Correlation Score is: 0.6452 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8305012606991558\n",
      "The Mean Absolute Error is: 0.16896091646769132 \n",
      "\n",
      "N-fold cross validation of X_train_scaled_pca\n",
      "The RVE is:  0.4255149728155324\n",
      "The rmse is:  0.20968213972802616\n",
      "The Correlation Score is: 0.6580 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.7897435841724622\n",
      "The Mean Absolute Error is: 0.1659543645522143 \n",
      "\n",
      "N-fold cross validation of X_train_cut_pca(already scaled!)\n",
      "The RVE is:  0.42626166497173934\n",
      "The rmse is:  0.20956660157745494\n",
      "The Correlation Score is: 0.6584 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.767632344439466\n",
      "The Mean Absolute Error is: 0.1665529998847385 \n",
      "\n",
      "N-fold cross validation of X_train_rffs_pca\n",
      "The RVE is:  0.38835837104581816\n",
      "The rmse is:  0.21635179699757218\n",
      "The Correlation Score is: 0.6346 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8187096958541058\n",
      "The Mean Absolute Error is: 0.17149474658330946 \n",
      "\n",
      "N-fold cross validation of X_train_scaled_rffs_pca\n",
      "The RVE is:  0.40899101768392154\n",
      "The rmse is:  0.21267182661355669\n",
      "The Correlation Score is: 0.6431 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8458664188563916\n",
      "The Mean Absolute Error is: 0.16876612462897453 \n",
      "\n",
      "N-fold cross validation of X_train_cut_rffs_pca(already scaled!)\n",
      "The RVE is:  0.4000572834623738\n",
      "The rmse is:  0.2142730736027166\n",
      "The Correlation Score is: 0.6340 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.9226528118457142\n",
      "The Mean Absolute Error is: 0.16909038420177197 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pca evaluations\n",
    "print(\"N-fold cross validation of X_train_pca\")\n",
    "nfold_valid(X_train_pca)\n",
    "print(\"N-fold cross validation of X_train_scaled_pca\")\n",
    "nfold_valid(X_train_scaled_pca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_cut_pca(already scaled!)\")\n",
    "nfold_valid(X_train_cut_pca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_rffs_pca\")\n",
    "nfold_valid(X_train_rffs_pca)\n",
    "print(\"N-fold cross validation of X_train_scaled_rffs_pca\")\n",
    "nfold_valid(X_train_scaled_rffs_pca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_cut_rffs_pca(already scaled!)\")\n",
    "nfold_valid(X_train_cut_rffs_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f2e13f58-251a-4684-81d2-77a478f34ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-fold cross validation of X_train_kpca\n",
      "The RVE is:  0.20766765108781393\n",
      "The rmse is:  0.24624797080869948\n",
      "The Correlation Score is: 0.4561 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8488884855533345\n",
      "The Mean Absolute Error is: 0.19842078800306723 \n",
      "\n",
      "N-fold cross validation of X_train_scaled_kpca\n",
      "The RVE is:  0.4264363712451361\n",
      "The rmse is:  0.20951208647911054\n",
      "The Correlation Score is: 0.6596 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.7810330062802083\n",
      "The Mean Absolute Error is: 0.16492001818943736 \n",
      "\n",
      "N-fold cross validation of X_train_cut_kpca(already scaled!)\n",
      "The RVE is:  0.47009611765177906\n",
      "The rmse is:  0.20139019003514333\n",
      "The Correlation Score is: 0.6865 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.7975236460222147\n",
      "The Mean Absolute Error is: 0.15847636729303838 \n",
      "\n",
      "N-fold cross validation of X_train_rffs_kpca\n",
      "The RVE is:  0.1320321961525056\n",
      "The rmse is:  0.2577308860063787\n",
      "The Correlation Score is: 0.3635 (p-value=4.967061e-228)\n",
      "The Maximum Error is:  0.8473072336692589\n",
      "The Mean Absolute Error is: 0.20877379474471752 \n",
      "\n",
      "N-fold cross validation of X_train_scaled_rffs_kpca\n",
      "The RVE is:  0.4593788291913644\n",
      "The rmse is:  0.2034075786201149\n",
      "The Correlation Score is: 0.6822 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8113019215119311\n",
      "The Mean Absolute Error is: 0.16031001650865007 \n",
      "\n",
      "N-fold cross validation of X_train_cut_rffs_kpca(already scaled!)\n",
      "The RVE is:  0.40625886166383895\n",
      "The rmse is:  0.21316748405476388\n",
      "The Correlation Score is: 0.6385 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.879456118001907\n",
      "The Mean Absolute Error is: 0.1675023694343489 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Kpca evaluations\n",
    "print(\"N-fold cross validation of X_train_kpca\")\n",
    "nfold_valid(X_train_kpca)\n",
    "print(\"N-fold cross validation of X_train_scaled_kpca\")\n",
    "nfold_valid(X_train_scaled_kpca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_cut_kpca(already scaled!)\")\n",
    "nfold_valid(X_train_cut_kpca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_rffs_kpca\")\n",
    "nfold_valid(X_train_rffs_kpca)\n",
    "print(\"N-fold cross validation of X_train_scaled_rffs_kpca\")\n",
    "nfold_valid(X_train_scaled_rffs_kpca)\n",
    "\n",
    "print(\"N-fold cross validation of X_train_cut_rffs_kpca(already scaled!)\")\n",
    "nfold_valid(X_train_cut_rffs_kpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13965578-d709-4b43-9de7-35e3b72788b9",
   "metadata": {},
   "source": [
    "<h5>\n",
    "Com estes resultados concluímos que o data tretment que efetuámos não afeta consideravelmente a qualidade dos modelos resultantes.\n",
    "<br>\n",
    "De salientar que os modelos resultantes dos dados manualmente cortados, que resultou numa redução de features de 2000+ para 377, tem resultados muito semelhantes aos modelos que usam todas as 2000+ features.\n",
    "<br>\n",
    "Por isso achamos que só o uso de mais e diferentes modelos para além de random forest é que poderemos melhorar o resultado das previsões.\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1293df2-324b-41de-98e0-c2af51f4bf0f",
   "metadata": {},
   "source": [
    "#### Daqui para a frente escolhemos o dataset X_train_scaled_pca por ter bom ratio de scores/ nº features usadas e ser mais simples envolvendo menos processamento do dataset original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8179ff-fe50-41e9-8b09-9b07389443d5",
   "metadata": {},
   "source": [
    "<h3>New Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51b9b653-d6d6-4279-a6ab-8ec28386ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfold_evaluate(mdl ,X_train_Valids):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=23)\n",
    "    kf.get_n_splits(X_train_Valids)\n",
    "    TRUTH_nfold=None\n",
    "    PREDS_nfold=None\n",
    "    for train_index, test_index in kf.split(X_train_Valids):\n",
    "        X_train_nfold, X_ivs_nfold = X_train_Valids[train_index], X_train_Valids[test_index]\n",
    "        y_train_nfold, y_ivs_nfold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        #mdl = DecisionTreeRegressor()#max_depth = 5)\n",
    "        mdl.fit(X_train_nfold, y_train_nfold)\n",
    "        preds = mdl.predict(X_ivs_nfold)\n",
    "        if TRUTH_nfold is None:\n",
    "            PREDS_nfold=preds\n",
    "            TRUTH_nfold=y_ivs_nfold\n",
    "        else:\n",
    "            PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
    "            TRUTH_nfold=np.hstack((TRUTH_nfold, y_ivs_nfold))\n",
    "        \n",
    "    return printAvalStat(TRUTH_nfold, PREDS_nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cdbed44d-6782-4d05-8962-705a35aa3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR_mdl = RandomForestRegressor(n_estimators=10, random_state=0, min_samples_leaf=3, max_depth = 8, n_jobs=6)\n",
    "LR_mdl = LinearRegression()\n",
    "Ridge_mdl = Ridge(alpha=1, max_iter=9999999)#10\n",
    "Lasso_mdl = Lasso(alpha=1, max_iter=9999999)#10\n",
    "KNN_mdl =KNeighborsRegressor()\n",
    "SVR_mdl =SVR()\n",
    "XGB_mdl = XGBRegressor()\n",
    "ABR_mdl = AdaBoostRegressor(n_estimators=10) #n_estimators=10\n",
    "GBR_mdl = GradientBoostingRegressor(n_estimators=10, learning_rate=1.0, random_state=0) #n_estimators=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f10acac-a39c-4d92-8919-33d319397176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set de controlo\n",
      "result of linear regression model on the X_train_scaled dataset\n",
      "The RVE is:  0.43036631730088293\n",
      "The rmse is:  0.20880393821899282\n",
      "The Correlation Score is: 0.7026 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.9797748650935948\n",
      "The Mean Absolute Error is: 0.15853068724620642 \n",
      "\n",
      "result of Ridge model on the X_train_scaled dataset\n",
      "The RVE is:  0.43384489082402156\n",
      "The rmse is:  0.20816564760571085\n",
      "The Correlation Score is: 0.7033 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.9999158704211308\n",
      "The Mean Absolute Error is: 0.15771812938893398 \n",
      "\n",
      "result of Lasso regression model on the X_train_scaled dataset\n",
      "The RVE is:  -0.0001864913956102221\n",
      "The rmse is:  0.2766632727709075\n",
      "The Correlation Score is: -0.0182 (p-value=1.189339e-01)\n",
      "The Maximum Error is:  0.613232559385074\n",
      "The Mean Absolute Error is: 0.22794516065455514 \n",
      "\n",
      "result of K-means regression model on the X_train_scaled dataset\n",
      "The RVE is:  0.5199069084981629\n",
      "The rmse is:  0.19505966472165026\n",
      "The Correlation Score is: 0.7228 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8345336654000001\n",
      "The Mean Absolute Error is: 0.14329407524056154 \n",
      "\n",
      "result of support vector regressor model on the X_train_scaled dataset\n",
      "The RVE is:  0.6187615697791591\n",
      "The rmse is:  0.17093377535090984\n",
      "The Correlation Score is: 0.7933 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.7665129888379658\n",
      "The Mean Absolute Error is: 0.13297410531303153 \n",
      "\n",
      "result of XGBoost regression model on the X_train_scaled dataset\n",
      "The RVE is:  0.629029850192303\n",
      "The rmse is:  0.1685051413813602\n",
      "The Correlation Score is: 0.7932 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.9021875262260437\n",
      "The Mean Absolute Error is: 0.12525926569993068 \n",
      "\n",
      "result of AdaBoost regression model on the X_train_scaled dataset\n",
      "The RVE is:  0.21433371141628332\n",
      "The rmse is:  0.2456664252212313\n",
      "The Correlation Score is: 0.5128 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.6988577604189605\n",
      "The Mean Absolute Error is: 0.20124639038692435 \n",
      "\n",
      "result of GradientBoost regression model on the X_train_scaled dataset\n",
      "The RVE is:  0.36747339296780823\n",
      "The rmse is:  0.2200154892406923\n",
      "The Correlation Score is: 0.6177 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.9009394138904565\n",
      "The Mean Absolute Error is: 0.17081644509538188 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set de controlo\n",
    "print(\"Set de controlo\")\n",
    "print(\"result of linear regression model on the X_train_scaled dataset\")\n",
    "LR_ce = nfold_evaluate(LR_mdl ,X_train_scaled)\n",
    "print(\"result of Ridge model on the X_train_scaled dataset\")\n",
    "Ridge_ce= nfold_evaluate(Ridge_mdl ,X_train_scaled)\n",
    "print(\"result of Lasso regression model on the X_train_scaled dataset\")\n",
    "Lasso_ce=nfold_evaluate(Lasso_mdl ,X_train_scaled)\n",
    "print(\"result of K-means regression model on the X_train_scaled dataset\")\n",
    "KNN_ce=nfold_evaluate(KNN_mdl ,X_train_scaled)\n",
    "print(\"result of support vector regressor model on the X_train_scaled dataset\")\n",
    "SVR_ce=nfold_evaluate(SVR_mdl ,X_train_scaled)\n",
    "print(\"result of XGBoost regression model on the X_train_scaled dataset\")\n",
    "XGB_ce=nfold_evaluate(XGB_mdl ,X_train_scaled)\n",
    "print(\"result of AdaBoost regression model on the X_train_scaled dataset\")\n",
    "ABR_ce=nfold_evaluate(ABR_mdl ,X_train_scaled)\n",
    "print(\"result of GradientBoost regression model on the X_train_scaled dataset\")\n",
    "GBR_ce=nfold_evaluate(GBR_mdl ,X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7224d971-43ff-4dd8-a8d0-94aaa8852331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of linear regression model on the X_train_scaled_pca dataset\n",
      "The RVE is:  0.4426068220827363\n",
      "The rmse is:  0.2065341378581706\n",
      "The Correlation Score is: 0.6656 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8298692557546726\n",
      "The Mean Absolute Error is: 0.1619193956563455 \n",
      "\n",
      "result of Ridge model on the X_train_scaled_pca dataset\n",
      "The RVE is:  0.442607790452041\n",
      "The rmse is:  0.20653395845085465\n",
      "The Correlation Score is: 0.6656 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8298651762089456\n",
      "The Mean Absolute Error is: 0.16191941596420703 \n",
      "\n",
      "result of Lasso regression model on the X_train_scaled_pca dataset\n",
      "The RVE is:  -0.0001864913956102221\n",
      "The rmse is:  0.2766632727709075\n",
      "The Correlation Score is: -0.0182 (p-value=1.189339e-01)\n",
      "The Maximum Error is:  0.613232559385074\n",
      "The Mean Absolute Error is: 0.22794516065455514 \n",
      "\n",
      "result of K-means regression model on the X_train_scaled_pca dataset\n",
      "The RVE is:  0.6019847011031803\n",
      "The rmse is:  0.17452784489554513\n",
      "The Correlation Score is: 0.7786 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8635003800000001\n",
      "The Mean Absolute Error is: 0.12617676470254874 \n",
      "\n",
      "result of support vector regressor model on the X_train_scaled_pca dataset\n",
      "The RVE is:  0.6428671527356806\n",
      "The rmse is:  0.16533133407498407\n",
      "The Correlation Score is: 0.8022 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8684079887737278\n",
      "The Mean Absolute Error is: 0.1253342604910048 \n",
      "\n",
      "result of XGBoost regression model on the X_train_scaled_pca dataset\n",
      "The RVE is:  0.5482463950253171\n",
      "The rmse is:  0.18593533413857838\n",
      "The Correlation Score is: 0.7421 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.8486411571502686\n",
      "The Mean Absolute Error is: 0.14133905116282383 \n",
      "\n",
      "result of AdaBoost regression model on the X_train_scaled_pca dataset\n",
      "The RVE is:  0.24156869012423676\n",
      "The rmse is:  0.2412908436482676\n",
      "The Correlation Score is: 0.5172 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.6960859738325896\n",
      "The Mean Absolute Error is: 0.19712264850518701 \n",
      "\n",
      "result of GradientBoost regression model on the X_train_scaled_pca dataset\n",
      "The RVE is:  0.3439311476257254\n",
      "The rmse is:  0.2240897538103631\n",
      "The Correlation Score is: 0.6051 (p-value=0.000000e+00)\n",
      "The Maximum Error is:  0.9218533425397304\n",
      "The Mean Absolute Error is: 0.17549796677253404 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set para testing do X_train_scaled_rffs_pca\n",
    "print(\"result of linear regression model on the X_train_scaled_pca dataset\")\n",
    "LR_me = nfold_evaluate(LR_mdl ,X_train_scaled_pca)\n",
    "print(\"result of Ridge model on the X_train_scaled_pca dataset\")\n",
    "Ridge_me = nfold_evaluate(Ridge_mdl ,X_train_scaled_pca)\n",
    "print(\"result of Lasso regression model on the X_train_scaled_pca dataset\")\n",
    "Lasso_me =nfold_evaluate(Lasso_mdl ,X_train_scaled_pca)\n",
    "print(\"result of K-means regression model on the X_train_scaled_pca dataset\")\n",
    "KNN_me =nfold_evaluate(KNN_mdl ,X_train_scaled_pca)\n",
    "print(\"result of support vector regressor model on the X_train_scaled_pca dataset\")\n",
    "SVR_me =nfold_evaluate(SVR_mdl ,X_train_scaled_pca)\n",
    "print(\"result of XGBoost regression model on the X_train_scaled_pca dataset\")\n",
    "XGB_me =nfold_evaluate(XGB_mdl ,X_train_scaled_pca)\n",
    "print(\"result of AdaBoost regression model on the X_train_scaled_pca dataset\")\n",
    "ABR_me =nfold_evaluate(ABR_mdl ,X_train_scaled_pca)\n",
    "print(\"result of GradientBoost regression model on the X_train_scaled_pca dataset\")\n",
    "GBR_me =nfold_evaluate(GBR_mdl ,X_train_scaled_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "972dc5e0-5da7-471f-9897-32c95658b46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imputers = [\"LR\", \"Ridge\",\"Lasso\", \"KNN\", \"SVR\", \"XGB\", \"AdaB\", \"GrB\"]\n",
    "RVEs = [LR_me[0], Ridge_me[0], Lasso_me[0], KNN_me[0], SVR_me[0], XGB_me[0], ABR_me[0], GBR_me[0]]\n",
    "\n",
    "plt.plot(imputers, RVEs, marker = \"o\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
