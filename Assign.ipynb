{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, max_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAvalStat(truth, preds):\n",
    "    print(\" The RVE is: \", explained_variance_score(truth, preds))\n",
    "    print(\" The rmse is: \", mean_squared_error(truth, preds, squared=False))\n",
    "    corr, pval=pearsonr(truth, preds)\n",
    "    print(\" The Correlation Score is: %6.4f (p-value=%e)\\n\"%(corr,pval))\n",
    "\n",
    "    print(\" The Maximum Error is: \", max_error(truth, preds))\n",
    "    print(\" The Mean Absolute Error is:\", mean_absolute_error(truth, preds),\"\\n\")\n",
    "\n",
    "def nfold_valid(X_train_Valids, y_train, mdl):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=23)\n",
    "    kf.get_n_splits(X_train_Valids)\n",
    "    TRUTH_nfold=None\n",
    "    PREDS_nfold=None\n",
    "    for train_index, test_index in kf.split(X_train_Valids):\n",
    "        X_train_nfold, X_ivs_nfold = X_train_Valids[train_index], X_train_Valids[test_index]\n",
    "        y_train_nfold, y_ivs_nfold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        mdl.fit(X_train_nfold, y_train_nfold)\n",
    "        preds = mdl.predict(X_ivs_nfold)\n",
    "        if TRUTH_nfold is None:\n",
    "            PREDS_nfold=preds\n",
    "            TRUTH_nfold=y_ivs_nfold\n",
    "        else:\n",
    "            PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
    "            TRUTH_nfold=np.hstack((TRUTH_nfold, y_ivs_nfold))\n",
    "        \n",
    "    printAvalStat(TRUTH_nfold, PREDS_nfold)\n",
    "\n",
    "#print(\"N-fold cross validation of nXf_train, after sequencial reduction feature selection\")\n",
    "#nfold_valid(nXf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and understanding the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7337, 2132)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_ivs, y_train, col_names = pickle.load(open(\"drd2_data.pickle\", \"rb\"))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(816, 2132)\n"
     ]
    }
   ],
   "source": [
    "print(X_ivs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7337,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2123</th>\n",
       "      <th>2124</th>\n",
       "      <th>2125</th>\n",
       "      <th>2126</th>\n",
       "      <th>2127</th>\n",
       "      <th>2128</th>\n",
       "      <th>2129</th>\n",
       "      <th>2130</th>\n",
       "      <th>2131</th>\n",
       "      <th>2132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.654947</td>\n",
       "      <td>541.280138</td>\n",
       "      <td>541.656</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.649995</td>\n",
       "      <td>426.197714</td>\n",
       "      <td>426.582</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.154947</td>\n",
       "      <td>348.183778</td>\n",
       "      <td>348.446</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.616176</td>\n",
       "      <td>1455.763803</td>\n",
       "      <td>1456.831</td>\n",
       "      <td>27.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.359725</td>\n",
       "      <td>387.151368</td>\n",
       "      <td>387.886</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>467.149047</td>\n",
       "      <td>467.513</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7333</th>\n",
       "      <td>0.002193</td>\n",
       "      <td>240.162649</td>\n",
       "      <td>240.350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7334</th>\n",
       "      <td>0.293481</td>\n",
       "      <td>510.317874</td>\n",
       "      <td>510.802</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7335</th>\n",
       "      <td>0.596804</td>\n",
       "      <td>393.187483</td>\n",
       "      <td>393.556</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7336</th>\n",
       "      <td>0.776976</td>\n",
       "      <td>484.056123</td>\n",
       "      <td>485.462</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7337 rows × 2133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0            1         2     3     4     5     6     7      8     \\\n",
       "0     0.654947   541.280138   541.656  10.0   1.0   8.0   1.0  10.0   40.0   \n",
       "1     0.649995   426.197714   426.582   5.0   1.0   9.0   1.0   4.0   30.0   \n",
       "2     0.154947   348.183778   348.446   4.0   0.0   3.0   0.0   3.0   26.0   \n",
       "3     0.616176  1455.763803  1456.831  27.0  19.0  23.0  17.0  16.0  105.0   \n",
       "4     0.359725   387.151368   387.886   4.0   0.0   4.0   0.0   4.0   27.0   \n",
       "...        ...          ...       ...   ...   ...   ...   ...   ...    ...   \n",
       "7332  0.000000   467.149047   467.513   6.0   0.0   6.0   0.0   5.0   32.0   \n",
       "7333  0.002193   240.162649   240.350   2.0   0.0   3.0   0.0   2.0   18.0   \n",
       "7334  0.293481   510.317874   510.802   4.0   0.0  10.0   0.0   4.0   37.0   \n",
       "7335  0.596804   393.187483   393.556   4.0   2.0   5.0   1.0   5.0   28.0   \n",
       "7336  0.776976   484.056123   485.462   6.0   1.0   7.0   1.0   6.0   30.0   \n",
       "\n",
       "       9     ...  2123  2124  2125  2126  2127  2128  2129  2130  2131  2132  \n",
       "0      75.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1      60.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2      50.0  ...   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3     206.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0  \n",
       "4      50.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...     ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "7332   56.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7333   38.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "7334   79.0  ...   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7335   55.0  ...   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7336   52.0  ...   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[7337 rows x 2133 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N,M=X_train.shape\n",
    "v=np.hstack((y_train.reshape((N,1)), X_train))\n",
    "pd.DataFrame(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2123</th>\n",
       "      <th>2124</th>\n",
       "      <th>2125</th>\n",
       "      <th>2126</th>\n",
       "      <th>2127</th>\n",
       "      <th>2128</th>\n",
       "      <th>2129</th>\n",
       "      <th>2130</th>\n",
       "      <th>2131</th>\n",
       "      <th>2132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.160182</td>\n",
       "      <td>0.160216</td>\n",
       "      <td>0.130117</td>\n",
       "      <td>0.084939</td>\n",
       "      <td>0.151968</td>\n",
       "      <td>0.095929</td>\n",
       "      <td>0.126690</td>\n",
       "      <td>0.157545</td>\n",
       "      <td>0.165189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011360</td>\n",
       "      <td>0.095138</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>-0.021728</td>\n",
       "      <td>-0.012977</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.031884</td>\n",
       "      <td>-0.027774</td>\n",
       "      <td>0.058370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.160182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.919389</td>\n",
       "      <td>0.772557</td>\n",
       "      <td>0.888252</td>\n",
       "      <td>0.784495</td>\n",
       "      <td>0.856242</td>\n",
       "      <td>0.992766</td>\n",
       "      <td>0.973177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213406</td>\n",
       "      <td>0.019559</td>\n",
       "      <td>0.127229</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>-0.017119</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.212881</td>\n",
       "      <td>0.342831</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.219727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.160216</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919128</td>\n",
       "      <td>0.772379</td>\n",
       "      <td>0.888072</td>\n",
       "      <td>0.784320</td>\n",
       "      <td>0.855980</td>\n",
       "      <td>0.992623</td>\n",
       "      <td>0.972956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213435</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.127309</td>\n",
       "      <td>0.117351</td>\n",
       "      <td>-0.017201</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.212889</td>\n",
       "      <td>0.342785</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>0.219646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130117</td>\n",
       "      <td>0.919389</td>\n",
       "      <td>0.919128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838103</td>\n",
       "      <td>0.863232</td>\n",
       "      <td>0.843652</td>\n",
       "      <td>0.931528</td>\n",
       "      <td>0.922469</td>\n",
       "      <td>0.905146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196534</td>\n",
       "      <td>-0.008557</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>0.064926</td>\n",
       "      <td>-0.026235</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.181134</td>\n",
       "      <td>0.332638</td>\n",
       "      <td>0.137266</td>\n",
       "      <td>0.205303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.084939</td>\n",
       "      <td>0.772557</td>\n",
       "      <td>0.772379</td>\n",
       "      <td>0.838103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.704868</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>0.653117</td>\n",
       "      <td>0.774714</td>\n",
       "      <td>0.769215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185163</td>\n",
       "      <td>-0.032325</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>0.022735</td>\n",
       "      <td>-0.013150</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.110449</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.210139</td>\n",
       "      <td>0.231743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.036037</td>\n",
       "      <td>0.047958</td>\n",
       "      <td>0.038387</td>\n",
       "      <td>0.059813</td>\n",
       "      <td>0.061989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018420</td>\n",
       "      <td>-0.020652</td>\n",
       "      <td>-0.018922</td>\n",
       "      <td>-0.009945</td>\n",
       "      <td>-0.009870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063520</td>\n",
       "      <td>0.047193</td>\n",
       "      <td>0.032638</td>\n",
       "      <td>0.027846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.212881</td>\n",
       "      <td>0.212889</td>\n",
       "      <td>0.181134</td>\n",
       "      <td>0.110449</td>\n",
       "      <td>0.254921</td>\n",
       "      <td>0.115041</td>\n",
       "      <td>0.202605</td>\n",
       "      <td>0.213051</td>\n",
       "      <td>0.218212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>-0.015847</td>\n",
       "      <td>0.075295</td>\n",
       "      <td>-0.020981</td>\n",
       "      <td>-0.004391</td>\n",
       "      <td>0.063520</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>0.000372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>0.031884</td>\n",
       "      <td>0.342831</td>\n",
       "      <td>0.342785</td>\n",
       "      <td>0.332638</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.276091</td>\n",
       "      <td>0.410854</td>\n",
       "      <td>0.233237</td>\n",
       "      <td>0.342548</td>\n",
       "      <td>0.342609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.017914</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>-0.004214</td>\n",
       "      <td>-0.020504</td>\n",
       "      <td>0.047193</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.123067</td>\n",
       "      <td>0.127682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>-0.027774</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>0.137266</td>\n",
       "      <td>0.210139</td>\n",
       "      <td>0.177572</td>\n",
       "      <td>0.216401</td>\n",
       "      <td>0.081396</td>\n",
       "      <td>0.184924</td>\n",
       "      <td>0.190152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>-0.026145</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>0.051670</td>\n",
       "      <td>-0.029027</td>\n",
       "      <td>0.032638</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>0.123067</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>0.058370</td>\n",
       "      <td>0.219727</td>\n",
       "      <td>0.219646</td>\n",
       "      <td>0.205303</td>\n",
       "      <td>0.231743</td>\n",
       "      <td>0.119145</td>\n",
       "      <td>0.240242</td>\n",
       "      <td>0.151050</td>\n",
       "      <td>0.226627</td>\n",
       "      <td>0.218560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>-0.025196</td>\n",
       "      <td>-0.024145</td>\n",
       "      <td>0.027846</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.127682</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2133 rows × 2133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     1.000000  0.160182  0.160216  0.130117  0.084939  0.151968  0.095929   \n",
       "1     0.160182  1.000000  0.999998  0.919389  0.772557  0.888252  0.784495   \n",
       "2     0.160216  0.999998  1.000000  0.919128  0.772379  0.888072  0.784320   \n",
       "3     0.130117  0.919389  0.919128  1.000000  0.838103  0.863232  0.843652   \n",
       "4     0.084939  0.772557  0.772379  0.838103  1.000000  0.704868  0.993448   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2128  0.001865  0.053576  0.053498  0.043314  0.045537  0.036037  0.047958   \n",
       "2129  0.012468  0.212881  0.212889  0.181134  0.110449  0.254921  0.115041   \n",
       "2130  0.031884  0.342831  0.342785  0.332638  0.409722  0.276091  0.410854   \n",
       "2131 -0.027774  0.173077  0.173016  0.137266  0.210139  0.177572  0.216401   \n",
       "2132  0.058370  0.219727  0.219646  0.205303  0.231743  0.119145  0.240242   \n",
       "\n",
       "          7         8         9     ...      2123      2124      2125  \\\n",
       "0     0.126690  0.157545  0.165189  ...  0.011360  0.095138  0.078900   \n",
       "1     0.856242  0.992766  0.973177  ...  0.213406  0.019559  0.127229   \n",
       "2     0.855980  0.992623  0.972956  ...  0.213435  0.019583  0.127309   \n",
       "3     0.931528  0.922469  0.905146  ...  0.196534 -0.008557  0.095891   \n",
       "4     0.653117  0.774714  0.769215  ...  0.185163 -0.032325  0.083849   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2128  0.038387  0.059813  0.061989  ...  0.018420 -0.020652 -0.018922   \n",
       "2129  0.202605  0.213051  0.218212  ...  0.022700 -0.015847  0.075295   \n",
       "2130  0.233237  0.342548  0.342609  ...  0.006494  0.017914  0.009569   \n",
       "2131  0.081396  0.184924  0.190152  ...  0.004409 -0.026145  0.009127   \n",
       "2132  0.151050  0.226627  0.218560  ...  0.004474  0.000246 -0.018317   \n",
       "\n",
       "          2126      2127      2128      2129      2130      2131      2132  \n",
       "0    -0.021728 -0.012977  0.001865  0.012468  0.031884 -0.027774  0.058370  \n",
       "1     0.117371 -0.017119  0.053576  0.212881  0.342831  0.173077  0.219727  \n",
       "2     0.117351 -0.017201  0.053498  0.212889  0.342785  0.173016  0.219646  \n",
       "3     0.064926 -0.026235  0.043314  0.181134  0.332638  0.137266  0.205303  \n",
       "4     0.022735 -0.013150  0.045537  0.110449  0.409722  0.210139  0.231743  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2128 -0.009945 -0.009870  1.000000  0.063520  0.047193  0.032638  0.027846  \n",
       "2129 -0.020981 -0.004391  0.063520  1.000000  0.026101  0.049729  0.000372  \n",
       "2130 -0.004214 -0.020504  0.047193  0.026101  1.000000  0.123067  0.127682  \n",
       "2131  0.051670 -0.029027  0.032638  0.049729  0.123067  1.000000  0.082630  \n",
       "2132 -0.025196 -0.024145  0.027846  0.000372  0.127682  0.082630  1.000000  \n",
       "\n",
       "[2133 rows x 2133 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.corrcoef(v.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_remove_with_threshold(threshold):\n",
    "    corr_matrix = pd.DataFrame(np.corrcoef(v.T))\n",
    "    selected_values_lines = [(row, value) for row, value in enumerate(corr_matrix.iloc[1:, 0]) if abs(value) < threshold]\n",
    "\n",
    "    # print(len(selected_values_lines))\n",
    "    # Display the selected values and lines\n",
    "    # for row, value in selected_values_lines:\n",
    "    #     print(f\"Row: {row}, Value: {value}\")\n",
    "\n",
    "    features_to_remove = []\n",
    "    for values, line_number in enumerate(selected_values_lines):\n",
    "        #print(f\"Feature {line_number}: {values}\")\n",
    "        features_to_remove.append(line_number[0])\n",
    "\n",
    "    return features_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "threshold = np.arange(0.05, 0.16, 0.02)\n",
    "\n",
    "for i in threshold:\n",
    "    features_to_remove = features_to_remove_with_threshold(i)\n",
    "    X_train_cut= np.delete(X_train, features_to_remove, 1)\n",
    "\n",
    "    print(f\"N-fold cross validation of X_train for {X_train_cut.shape[1]}\")\n",
    "    print(\"Random Forest\")\n",
    "    RFR_mdl = RandomForestRegressor(n_estimators=10, random_state=0, min_samples_leaf=3, max_depth = 8, n_jobs=6)\n",
    "    nfold_valid(X_train_cut, y_train, RFR_mdl)\n",
    "\n",
    "    print(\"Linear Regressor\")\n",
    "    LR_mdl = LinearRegression(n_jobs=6)\n",
    "    nfold_valid(X_train_cut, y_train, LR_mdl)  \n",
    "\n",
    "    print(\"Decision Tree\")\n",
    "    DTR_mdl = DecisionTreeRegressor(max_depth=5)\n",
    "    nfold_valid(X_train_cut, y_train, DTR_mdl)\n",
    "\n",
    "    print(\"KNN\")\n",
    "    KNN_mdl = KNeighborsRegressor(n_jobs=6)\n",
    "    nfold_valid(X_train_cut, y_train, KNN_mdl)\n",
    "\n",
    "    print(\"SVR\")\n",
    "    SVR_mdl = SVR(kernel='rbf', C=10, gamma=0.1)\n",
    "    nfold_valid(X_train_cut, y_train, SVR_mdl)\n",
    "\n",
    "    print(\"MLP\")\n",
    "    MLP_mdl = MLPRegressor()\n",
    "    nfold_valid(X_train_cut, y_train, MLP_mdl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Treatement, scaling data and looking to features with most correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaom\\Documents\\FCUL\\1ºSemestre\\Aprendizagem Automática\\Home assignments\\.env\\Lib\\site-packages\\numpy\\core\\_methods.py:176: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "c:\\Users\\joaom\\Documents\\FCUL\\1ºSemestre\\Aprendizagem Automática\\Home assignments\\.env\\Lib\\site-packages\\numpy\\core\\_methods.py:187: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2123</th>\n",
       "      <th>2124</th>\n",
       "      <th>2125</th>\n",
       "      <th>2126</th>\n",
       "      <th>2127</th>\n",
       "      <th>2128</th>\n",
       "      <th>2129</th>\n",
       "      <th>2130</th>\n",
       "      <th>2131</th>\n",
       "      <th>2132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.654947</td>\n",
       "      <td>0.609921</td>\n",
       "      <td>0.608648</td>\n",
       "      <td>1.098414</td>\n",
       "      <td>-0.081854</td>\n",
       "      <td>0.246156</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>1.908209</td>\n",
       "      <td>0.737657</td>\n",
       "      <td>0.614996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.649995</td>\n",
       "      <td>-0.023790</td>\n",
       "      <td>-0.024513</td>\n",
       "      <td>-0.103427</td>\n",
       "      <td>-0.081854</td>\n",
       "      <td>0.413306</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>-0.243126</td>\n",
       "      <td>-0.038629</td>\n",
       "      <td>0.032423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.154947</td>\n",
       "      <td>-0.453381</td>\n",
       "      <td>-0.454433</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.589590</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.601682</td>\n",
       "      <td>-0.349144</td>\n",
       "      <td>-0.355960</td>\n",
       "      <td>...</td>\n",
       "      <td>1.234467</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.616176</td>\n",
       "      <td>5.645607</td>\n",
       "      <td>5.644129</td>\n",
       "      <td>5.184672</td>\n",
       "      <td>7.039406</td>\n",
       "      <td>2.753396</td>\n",
       "      <td>7.153978</td>\n",
       "      <td>4.059544</td>\n",
       "      <td>5.783518</td>\n",
       "      <td>5.702803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>5.029661</td>\n",
       "      <td>3.091413</td>\n",
       "      <td>5.116060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.359725</td>\n",
       "      <td>-0.238802</td>\n",
       "      <td>-0.237426</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.422441</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.243126</td>\n",
       "      <td>-0.271515</td>\n",
       "      <td>-0.355960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201712</td>\n",
       "      <td>0.200698</td>\n",
       "      <td>0.136941</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.088142</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>0.115430</td>\n",
       "      <td>0.116628</td>\n",
       "      <td>-0.122930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7333</th>\n",
       "      <td>0.002193</td>\n",
       "      <td>-1.048209</td>\n",
       "      <td>-1.049199</td>\n",
       "      <td>-0.824531</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>-0.589590</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.960238</td>\n",
       "      <td>-0.970173</td>\n",
       "      <td>-0.822018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>3.091413</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7334</th>\n",
       "      <td>0.293481</td>\n",
       "      <td>0.439425</td>\n",
       "      <td>0.438883</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>-0.477479</td>\n",
       "      <td>0.580455</td>\n",
       "      <td>-0.508809</td>\n",
       "      <td>-0.243126</td>\n",
       "      <td>0.504771</td>\n",
       "      <td>0.770349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>3.210153</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7335</th>\n",
       "      <td>0.596804</td>\n",
       "      <td>-0.205564</td>\n",
       "      <td>-0.206229</td>\n",
       "      <td>-0.343795</td>\n",
       "      <td>0.313772</td>\n",
       "      <td>-0.255292</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>0.115430</td>\n",
       "      <td>-0.193886</td>\n",
       "      <td>-0.161769</td>\n",
       "      <td>...</td>\n",
       "      <td>1.234467</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7336</th>\n",
       "      <td>0.776976</td>\n",
       "      <td>0.294812</td>\n",
       "      <td>0.299457</td>\n",
       "      <td>0.136941</td>\n",
       "      <td>-0.081854</td>\n",
       "      <td>0.079007</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>0.473986</td>\n",
       "      <td>-0.038629</td>\n",
       "      <td>-0.278283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810066</td>\n",
       "      <td>5.116060</td>\n",
       "      <td>-0.179087</td>\n",
       "      <td>-0.311512</td>\n",
       "      <td>-0.168687</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.136394</td>\n",
       "      <td>-0.198821</td>\n",
       "      <td>-0.323477</td>\n",
       "      <td>-0.195463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7337 rows × 2133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.654947  0.609921  0.608648  1.098414 -0.081854  0.246156 -0.058057   \n",
       "1     0.649995 -0.023790 -0.024513 -0.103427 -0.081854  0.413306 -0.058057   \n",
       "2     0.154947 -0.453381 -0.454433 -0.343795 -0.477479 -0.589590 -0.508809   \n",
       "3     0.616176  5.645607  5.644129  5.184672  7.039406  2.753396  7.153978   \n",
       "4     0.359725 -0.238802 -0.237426 -0.343795 -0.477479 -0.422441 -0.508809   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7332  0.000000  0.201712  0.200698  0.136941 -0.477479 -0.088142 -0.508809   \n",
       "7333  0.002193 -1.048209 -1.049199 -0.824531 -0.477479 -0.589590 -0.508809   \n",
       "7334  0.293481  0.439425  0.438883 -0.343795 -0.477479  0.580455 -0.508809   \n",
       "7335  0.596804 -0.205564 -0.206229 -0.343795  0.313772 -0.255292 -0.058057   \n",
       "7336  0.776976  0.294812  0.299457  0.136941 -0.081854  0.079007 -0.058057   \n",
       "\n",
       "          7         8         9     ...      2123      2124      2125  \\\n",
       "0     1.908209  0.737657  0.614996  ... -0.810066 -0.195463 -0.179087   \n",
       "1    -0.243126 -0.038629  0.032423  ... -0.810066 -0.195463 -0.179087   \n",
       "2    -0.601682 -0.349144 -0.355960  ...  1.234467 -0.195463 -0.179087   \n",
       "3     4.059544  5.783518  5.702803  ... -0.810066 -0.195463 -0.179087   \n",
       "4    -0.243126 -0.271515 -0.355960  ... -0.810066 -0.195463 -0.179087   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7332  0.115430  0.116628 -0.122930  ... -0.810066 -0.195463 -0.179087   \n",
       "7333 -0.960238 -0.970173 -0.822018  ... -0.810066 -0.195463 -0.179087   \n",
       "7334 -0.243126  0.504771  0.770349  ... -0.810066 -0.195463 -0.179087   \n",
       "7335  0.115430 -0.193886 -0.161769  ...  1.234467 -0.195463 -0.179087   \n",
       "7336  0.473986 -0.038629 -0.278283  ... -0.810066  5.116060 -0.179087   \n",
       "\n",
       "          2126      2127      2128      2129      2130      2131      2132  \n",
       "0    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "1    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "2    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "3    -0.311512 -0.168687 -0.105656 -0.136394  5.029661  3.091413  5.116060  \n",
       "4    -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7332 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "7333 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821  3.091413 -0.195463  \n",
       "7334  3.210153 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "7335 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "7336 -0.311512 -0.168687 -0.105656 -0.136394 -0.198821 -0.323477 -0.195463  \n",
       "\n",
       "[7337 rows x 2133 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sScaler = StandardScaler()\n",
    "sScaler.fit(X_train)\n",
    "X_train_scaled = sScaler.transform(X_train)\n",
    "X_ivs_scaled = sScaler.transform(X_ivs)\n",
    "\n",
    "pT_scaler = PowerTransformer()\n",
    "pT_scaler.fit(X_train)\n",
    "X_train_power_t = pT_scaler.transform(X_train)\n",
    "X_ivs_power_t = pT_scaler.transform(X_train)\n",
    "\n",
    "v_scaled=np.hstack((y_train.reshape((N,1)), X_train_scaled))\n",
    "pd.DataFrame(v_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2123</th>\n",
       "      <th>2124</th>\n",
       "      <th>2125</th>\n",
       "      <th>2126</th>\n",
       "      <th>2127</th>\n",
       "      <th>2128</th>\n",
       "      <th>2129</th>\n",
       "      <th>2130</th>\n",
       "      <th>2131</th>\n",
       "      <th>2132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.160182</td>\n",
       "      <td>0.160216</td>\n",
       "      <td>0.130117</td>\n",
       "      <td>0.084939</td>\n",
       "      <td>0.151968</td>\n",
       "      <td>0.095929</td>\n",
       "      <td>0.126690</td>\n",
       "      <td>0.157545</td>\n",
       "      <td>0.165189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011360</td>\n",
       "      <td>0.095138</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>-0.021728</td>\n",
       "      <td>-0.012977</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.031884</td>\n",
       "      <td>-0.027774</td>\n",
       "      <td>0.058370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.160182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.919389</td>\n",
       "      <td>0.772557</td>\n",
       "      <td>0.888252</td>\n",
       "      <td>0.784495</td>\n",
       "      <td>0.856242</td>\n",
       "      <td>0.992766</td>\n",
       "      <td>0.973177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213406</td>\n",
       "      <td>0.019559</td>\n",
       "      <td>0.127229</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>-0.017119</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.212881</td>\n",
       "      <td>0.342831</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.219727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.160216</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919128</td>\n",
       "      <td>0.772379</td>\n",
       "      <td>0.888072</td>\n",
       "      <td>0.784320</td>\n",
       "      <td>0.855980</td>\n",
       "      <td>0.992623</td>\n",
       "      <td>0.972956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213435</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.127309</td>\n",
       "      <td>0.117351</td>\n",
       "      <td>-0.017201</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.212889</td>\n",
       "      <td>0.342785</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>0.219646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130117</td>\n",
       "      <td>0.919389</td>\n",
       "      <td>0.919128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838103</td>\n",
       "      <td>0.863232</td>\n",
       "      <td>0.843652</td>\n",
       "      <td>0.931528</td>\n",
       "      <td>0.922469</td>\n",
       "      <td>0.905146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196534</td>\n",
       "      <td>-0.008557</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>0.064926</td>\n",
       "      <td>-0.026235</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.181134</td>\n",
       "      <td>0.332638</td>\n",
       "      <td>0.137266</td>\n",
       "      <td>0.205303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.084939</td>\n",
       "      <td>0.772557</td>\n",
       "      <td>0.772379</td>\n",
       "      <td>0.838103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.704868</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>0.653117</td>\n",
       "      <td>0.774714</td>\n",
       "      <td>0.769215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185163</td>\n",
       "      <td>-0.032325</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>0.022735</td>\n",
       "      <td>-0.013150</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.110449</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.210139</td>\n",
       "      <td>0.231743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.036037</td>\n",
       "      <td>0.047958</td>\n",
       "      <td>0.038387</td>\n",
       "      <td>0.059813</td>\n",
       "      <td>0.061989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018420</td>\n",
       "      <td>-0.020652</td>\n",
       "      <td>-0.018922</td>\n",
       "      <td>-0.009945</td>\n",
       "      <td>-0.009870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063520</td>\n",
       "      <td>0.047193</td>\n",
       "      <td>0.032638</td>\n",
       "      <td>0.027846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.212881</td>\n",
       "      <td>0.212889</td>\n",
       "      <td>0.181134</td>\n",
       "      <td>0.110449</td>\n",
       "      <td>0.254921</td>\n",
       "      <td>0.115041</td>\n",
       "      <td>0.202605</td>\n",
       "      <td>0.213051</td>\n",
       "      <td>0.218212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>-0.015847</td>\n",
       "      <td>0.075295</td>\n",
       "      <td>-0.020981</td>\n",
       "      <td>-0.004391</td>\n",
       "      <td>0.063520</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>0.000372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>0.031884</td>\n",
       "      <td>0.342831</td>\n",
       "      <td>0.342785</td>\n",
       "      <td>0.332638</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.276091</td>\n",
       "      <td>0.410854</td>\n",
       "      <td>0.233237</td>\n",
       "      <td>0.342548</td>\n",
       "      <td>0.342609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.017914</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>-0.004214</td>\n",
       "      <td>-0.020504</td>\n",
       "      <td>0.047193</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.123067</td>\n",
       "      <td>0.127682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>-0.027774</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>0.137266</td>\n",
       "      <td>0.210139</td>\n",
       "      <td>0.177572</td>\n",
       "      <td>0.216401</td>\n",
       "      <td>0.081396</td>\n",
       "      <td>0.184924</td>\n",
       "      <td>0.190152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>-0.026145</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>0.051670</td>\n",
       "      <td>-0.029027</td>\n",
       "      <td>0.032638</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>0.123067</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>0.058370</td>\n",
       "      <td>0.219727</td>\n",
       "      <td>0.219646</td>\n",
       "      <td>0.205303</td>\n",
       "      <td>0.231743</td>\n",
       "      <td>0.119145</td>\n",
       "      <td>0.240242</td>\n",
       "      <td>0.151050</td>\n",
       "      <td>0.226627</td>\n",
       "      <td>0.218560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>-0.018317</td>\n",
       "      <td>-0.025196</td>\n",
       "      <td>-0.024145</td>\n",
       "      <td>0.027846</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.127682</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2133 rows × 2133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     1.000000  0.160182  0.160216  0.130117  0.084939  0.151968  0.095929   \n",
       "1     0.160182  1.000000  0.999998  0.919389  0.772557  0.888252  0.784495   \n",
       "2     0.160216  0.999998  1.000000  0.919128  0.772379  0.888072  0.784320   \n",
       "3     0.130117  0.919389  0.919128  1.000000  0.838103  0.863232  0.843652   \n",
       "4     0.084939  0.772557  0.772379  0.838103  1.000000  0.704868  0.993448   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2128  0.001865  0.053576  0.053498  0.043314  0.045537  0.036037  0.047958   \n",
       "2129  0.012468  0.212881  0.212889  0.181134  0.110449  0.254921  0.115041   \n",
       "2130  0.031884  0.342831  0.342785  0.332638  0.409722  0.276091  0.410854   \n",
       "2131 -0.027774  0.173077  0.173016  0.137266  0.210139  0.177572  0.216401   \n",
       "2132  0.058370  0.219727  0.219646  0.205303  0.231743  0.119145  0.240242   \n",
       "\n",
       "          7         8         9     ...      2123      2124      2125  \\\n",
       "0     0.126690  0.157545  0.165189  ...  0.011360  0.095138  0.078900   \n",
       "1     0.856242  0.992766  0.973177  ...  0.213406  0.019559  0.127229   \n",
       "2     0.855980  0.992623  0.972956  ...  0.213435  0.019583  0.127309   \n",
       "3     0.931528  0.922469  0.905146  ...  0.196534 -0.008557  0.095891   \n",
       "4     0.653117  0.774714  0.769215  ...  0.185163 -0.032325  0.083849   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2128  0.038387  0.059813  0.061989  ...  0.018420 -0.020652 -0.018922   \n",
       "2129  0.202605  0.213051  0.218212  ...  0.022700 -0.015847  0.075295   \n",
       "2130  0.233237  0.342548  0.342609  ...  0.006494  0.017914  0.009569   \n",
       "2131  0.081396  0.184924  0.190152  ...  0.004409 -0.026145  0.009127   \n",
       "2132  0.151050  0.226627  0.218560  ...  0.004474  0.000246 -0.018317   \n",
       "\n",
       "          2126      2127      2128      2129      2130      2131      2132  \n",
       "0    -0.021728 -0.012977  0.001865  0.012468  0.031884 -0.027774  0.058370  \n",
       "1     0.117371 -0.017119  0.053576  0.212881  0.342831  0.173077  0.219727  \n",
       "2     0.117351 -0.017201  0.053498  0.212889  0.342785  0.173016  0.219646  \n",
       "3     0.064926 -0.026235  0.043314  0.181134  0.332638  0.137266  0.205303  \n",
       "4     0.022735 -0.013150  0.045537  0.110449  0.409722  0.210139  0.231743  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2128 -0.009945 -0.009870  1.000000  0.063520  0.047193  0.032638  0.027846  \n",
       "2129 -0.020981 -0.004391  0.063520  1.000000  0.026101  0.049729  0.000372  \n",
       "2130 -0.004214 -0.020504  0.047193  0.026101  1.000000  0.123067  0.127682  \n",
       "2131  0.051670 -0.029027  0.032638  0.049729  0.123067  1.000000  0.082630  \n",
       "2132 -0.025196 -0.024145  0.027846  0.000372  0.127682  0.082630  1.000000  \n",
       "\n",
       "[2133 rows x 2133 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.corrcoef(v_scaled.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-fold cross validation of X_train for 2132\n",
      "Random Forest\n",
      " The RVE is:  0.43083638196535623\n",
      " The rmse is:  0.208703513879559\n",
      " The Correlation Score is: 0.6608 (p-value=0.000000e+00)\n",
      "\n",
      " The Maximum Error is:  0.8645408990344696\n",
      " The Mean Absolute Error is: 0.1652836905537773 \n",
      "\n",
      "Linear Regressor\n",
      " The RVE is:  0.4270934090524995\n",
      " The rmse is:  0.2094028980287027\n",
      " The Correlation Score is: 0.7004 (p-value=0.000000e+00)\n",
      "\n",
      " The Maximum Error is:  1.0548827375254515\n",
      " The Mean Absolute Error is: 0.15847652396570752 \n",
      "\n",
      "Decision Tree\n",
      " The RVE is:  0.2859254385612089\n",
      " The rmse is:  0.23376700864205158\n",
      " The Correlation Score is: 0.5359 (p-value=0.000000e+00)\n",
      "\n",
      " The Maximum Error is:  0.8883078161666667\n",
      " The Mean Absolute Error is: 0.18556654545452797 \n",
      "\n",
      "KNN\n",
      " The RVE is:  0.5169615407967393\n",
      " The rmse is:  0.19592788060403094\n",
      " The Correlation Score is: 0.7213 (p-value=0.000000e+00)\n",
      "\n",
      " The Maximum Error is:  0.8476752418\n",
      " The Mean Absolute Error is: 0.1433638550345373 \n",
      "\n",
      "SVR\n",
      " The RVE is:  0.6195917293083877\n",
      " The rmse is:  0.17075048391992462\n",
      " The Correlation Score is: 0.7938 (p-value=0.000000e+00)\n",
      "\n",
      " The Maximum Error is:  0.7697676937530821\n",
      " The Mean Absolute Error is: 0.13273991462590579 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "threshold = np.arange(0.05, 0.16, 0.02)\n",
    "\n",
    "for i in threshold:\n",
    "    features_to_remove = features_to_remove_with_threshold(i)\n",
    "    X_train_cut= np.delete(X_train_scaled, features_to_remove, 1)\n",
    "\n",
    "    print(f\"N-fold cross validation of X_train for {X_train_cut.shape[1]}\")\n",
    "    print(\"Random Forest\")\n",
    "    RFR_mdl = RandomForestRegressor(n_estimators=10, random_state=0, min_samples_leaf=3, max_depth = 8, n_jobs=6)\n",
    "    nfold_valid(X_train_cut, y_train, RFR_mdl)\n",
    "\n",
    "    print(\"Linear Regressor\")\n",
    "    LR_mdl = LinearRegression(n_jobs=6)\n",
    "    nfold_valid(X_train_cut, y_train, LR_mdl)  \n",
    "\n",
    "    print(\"Decision Tree\")\n",
    "    DTR_mdl = DecisionTreeRegressor(max_depth=5)\n",
    "    nfold_valid(X_train_cut, y_train, DTR_mdl)\n",
    "\n",
    "    print(\"KNN\")\n",
    "    KNN_mdl = KNeighborsRegressor(n_jobs=6)\n",
    "    nfold_valid(X_train_cut, y_train, KNN_mdl)\n",
    "\n",
    "    print(\"SVR\")\n",
    "    SVR_mdl = SVR()\n",
    "    nfold_valid(X_train_cut, y_train, SVR_mdl)\n",
    "\n",
    "    # print(\"MLP\")\n",
    "    # MLP_mdl = MLPRegressor()\n",
    "    # nfold_valid(X_train_cut, y_train, MLP_mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting features by droping the features with correlation and by selection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelect(x_train, n_features, t):\n",
    "    rfr=RandomForestRegressor(random_state=0, n_jobs=6)\n",
    "    rfr.fit(x_train, y_train)\n",
    "\n",
    "    \n",
    "    sel = SelectFromModel(estimator=rfr, threshold= t) #Change the threshold! See what happens!\n",
    "    sel.fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    features=sel.get_support()\n",
    "    Features_selected =np.arange(n_features)[features]\n",
    "    print(\"The features selected are columns: \", Features_selected,\".\\n Number of features:\", len(Features_selected))\n",
    "    \n",
    "    X_train_rffs=sel.transform(x_train)\n",
    "    return X_train_rffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "threshold = np.arange(0.05, 0.16, 0.02)\n",
    "\n",
    "for i in threshold:\n",
    "    features_to_remove = features_to_remove_with_threshold(i)\n",
    "    X_train_cut= np.delete(X_train_scaled, features_to_remove, 1)\n",
    "    X_train_rffs = featureSelect(X_train_cut, X_train_cut.shape[1], .003)\n",
    "\n",
    "    print(f\"\\nN-fold cross validation of X_train for {X_train_cut.shape[1]} and {i} as threshold value\")\n",
    "    print(\"Random Forest\")\n",
    "    RFR_mdl = RandomForestRegressor(n_estimators=10, random_state=0, min_samples_leaf=3, max_depth = 8, n_jobs=6)\n",
    "    nfold_valid(X_train_rffs, y_train, RFR_mdl)\n",
    "\n",
    "    print(\"Linear Regressor\")\n",
    "    LR_mdl = LinearRegression(n_jobs=6)\n",
    "    nfold_valid(X_train_rffs, y_train, LR_mdl)  \n",
    "\n",
    "    print(\"Decision Tree\")\n",
    "    DTR_mdl = DecisionTreeRegressor(max_depth=5)\n",
    "    nfold_valid(X_train_rffs, y_train, DTR_mdl)\n",
    "\n",
    "    print(\"KNN\")\n",
    "    KNN_mdl = KNeighborsRegressor(n_jobs=6)\n",
    "    nfold_valid(X_train_rffs, y_train, KNN_mdl)\n",
    "    \n",
    "    print(\"SVR\")\n",
    "    SVR_mdl = SVR()\n",
    "    nfold_valid(X_train_rffs, y_train, SVR_mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component analizys (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(x_train, n_comps):\n",
    "    pca = PCA(n_components=n_comps)\n",
    "    pca.fit(x_train)\n",
    "    tve=0\n",
    "    for i, ve in enumerate(pca.explained_variance_ratio_):\n",
    "        tve+=ve\n",
    "        print(\"PC%d - Variance explained: %7.4f - Total Variance: %7.4f\" % (i, ve, tve) )\n",
    "    print()\n",
    "    # print(\"Actual Eigenvalues:\", pca.singular_values_)\n",
    "    # for i,comp in enumerate(pca.components_):\n",
    "    #    print(\"PC\",i, \"-->\", comp)   \n",
    "    X_train_pca = pca.transform(x_train) \n",
    "    return X_train_pca\n",
    "\n",
    "def kpca(x_train, n_comps):\n",
    "    kpca = KernelPCA(n_components=n_comps, kernel='rbf', n_jobs=4)#, gamma=3)\n",
    "    kpca.fit(x_train)\n",
    "    X_train_kpca = kpca.transform(x_train)\n",
    "    return X_train_kpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.0395 - Total Variance:  0.0395\n",
      "PC1 - Variance explained:  0.0212 - Total Variance:  0.0606\n",
      "PC2 - Variance explained:  0.0197 - Total Variance:  0.0803\n",
      "PC3 - Variance explained:  0.0170 - Total Variance:  0.0973\n",
      "PC4 - Variance explained:  0.0124 - Total Variance:  0.1096\n",
      "PC5 - Variance explained:  0.0121 - Total Variance:  0.1218\n",
      "PC6 - Variance explained:  0.0117 - Total Variance:  0.1334\n",
      "PC7 - Variance explained:  0.0109 - Total Variance:  0.1443\n",
      "PC8 - Variance explained:  0.0105 - Total Variance:  0.1548\n",
      "PC9 - Variance explained:  0.0101 - Total Variance:  0.1649\n",
      "PC10 - Variance explained:  0.0095 - Total Variance:  0.1745\n",
      "PC11 - Variance explained:  0.0091 - Total Variance:  0.1836\n",
      "PC12 - Variance explained:  0.0085 - Total Variance:  0.1921\n",
      "PC13 - Variance explained:  0.0077 - Total Variance:  0.1998\n",
      "PC14 - Variance explained:  0.0076 - Total Variance:  0.2073\n",
      "PC15 - Variance explained:  0.0072 - Total Variance:  0.2145\n",
      "PC16 - Variance explained:  0.0071 - Total Variance:  0.2217\n",
      "PC17 - Variance explained:  0.0070 - Total Variance:  0.2287\n",
      "PC18 - Variance explained:  0.0065 - Total Variance:  0.2352\n",
      "PC19 - Variance explained:  0.0064 - Total Variance:  0.2416\n",
      "PC20 - Variance explained:  0.0062 - Total Variance:  0.2478\n",
      "PC21 - Variance explained:  0.0061 - Total Variance:  0.2539\n",
      "PC22 - Variance explained:  0.0057 - Total Variance:  0.2596\n",
      "PC23 - Variance explained:  0.0055 - Total Variance:  0.2651\n",
      "PC24 - Variance explained:  0.0054 - Total Variance:  0.2706\n",
      "PC25 - Variance explained:  0.0053 - Total Variance:  0.2759\n",
      "PC26 - Variance explained:  0.0051 - Total Variance:  0.2810\n",
      "PC27 - Variance explained:  0.0050 - Total Variance:  0.2861\n",
      "PC28 - Variance explained:  0.0050 - Total Variance:  0.2910\n",
      "PC29 - Variance explained:  0.0048 - Total Variance:  0.2958\n",
      "PC30 - Variance explained:  0.0048 - Total Variance:  0.3006\n",
      "PC31 - Variance explained:  0.0047 - Total Variance:  0.3053\n",
      "PC32 - Variance explained:  0.0046 - Total Variance:  0.3099\n",
      "PC33 - Variance explained:  0.0045 - Total Variance:  0.3144\n",
      "PC34 - Variance explained:  0.0044 - Total Variance:  0.3188\n",
      "PC35 - Variance explained:  0.0043 - Total Variance:  0.3231\n",
      "PC36 - Variance explained:  0.0042 - Total Variance:  0.3273\n",
      "PC37 - Variance explained:  0.0042 - Total Variance:  0.3315\n",
      "PC38 - Variance explained:  0.0041 - Total Variance:  0.3356\n",
      "PC39 - Variance explained:  0.0041 - Total Variance:  0.3397\n",
      "PC40 - Variance explained:  0.0040 - Total Variance:  0.3437\n",
      "PC41 - Variance explained:  0.0039 - Total Variance:  0.3476\n",
      "PC42 - Variance explained:  0.0039 - Total Variance:  0.3515\n",
      "PC43 - Variance explained:  0.0038 - Total Variance:  0.3553\n",
      "PC44 - Variance explained:  0.0038 - Total Variance:  0.3590\n",
      "PC45 - Variance explained:  0.0037 - Total Variance:  0.3627\n",
      "PC46 - Variance explained:  0.0036 - Total Variance:  0.3663\n",
      "PC47 - Variance explained:  0.0036 - Total Variance:  0.3699\n",
      "PC48 - Variance explained:  0.0035 - Total Variance:  0.3734\n",
      "PC49 - Variance explained:  0.0035 - Total Variance:  0.3769\n",
      "PC50 - Variance explained:  0.0035 - Total Variance:  0.3804\n",
      "PC51 - Variance explained:  0.0034 - Total Variance:  0.3838\n",
      "PC52 - Variance explained:  0.0034 - Total Variance:  0.3872\n",
      "PC53 - Variance explained:  0.0033 - Total Variance:  0.3905\n",
      "PC54 - Variance explained:  0.0032 - Total Variance:  0.3937\n",
      "PC55 - Variance explained:  0.0032 - Total Variance:  0.3969\n",
      "PC56 - Variance explained:  0.0032 - Total Variance:  0.4001\n",
      "PC57 - Variance explained:  0.0031 - Total Variance:  0.4032\n",
      "PC58 - Variance explained:  0.0031 - Total Variance:  0.4064\n",
      "PC59 - Variance explained:  0.0031 - Total Variance:  0.4094\n",
      "PC60 - Variance explained:  0.0030 - Total Variance:  0.4124\n",
      "PC61 - Variance explained:  0.0030 - Total Variance:  0.4154\n",
      "PC62 - Variance explained:  0.0029 - Total Variance:  0.4184\n",
      "PC63 - Variance explained:  0.0029 - Total Variance:  0.4213\n",
      "PC64 - Variance explained:  0.0029 - Total Variance:  0.4241\n",
      "PC65 - Variance explained:  0.0029 - Total Variance:  0.4270\n",
      "PC66 - Variance explained:  0.0028 - Total Variance:  0.4299\n",
      "PC67 - Variance explained:  0.0028 - Total Variance:  0.4327\n",
      "PC68 - Variance explained:  0.0028 - Total Variance:  0.4355\n",
      "PC69 - Variance explained:  0.0028 - Total Variance:  0.4382\n",
      "PC70 - Variance explained:  0.0027 - Total Variance:  0.4410\n",
      "PC71 - Variance explained:  0.0027 - Total Variance:  0.4437\n",
      "PC72 - Variance explained:  0.0027 - Total Variance:  0.4464\n",
      "PC73 - Variance explained:  0.0027 - Total Variance:  0.4490\n",
      "PC74 - Variance explained:  0.0026 - Total Variance:  0.4517\n",
      "PC75 - Variance explained:  0.0026 - Total Variance:  0.4543\n",
      "PC76 - Variance explained:  0.0026 - Total Variance:  0.4569\n",
      "PC77 - Variance explained:  0.0026 - Total Variance:  0.4595\n",
      "PC78 - Variance explained:  0.0026 - Total Variance:  0.4620\n",
      "PC79 - Variance explained:  0.0025 - Total Variance:  0.4645\n",
      "PC80 - Variance explained:  0.0025 - Total Variance:  0.4670\n",
      "PC81 - Variance explained:  0.0025 - Total Variance:  0.4695\n",
      "PC82 - Variance explained:  0.0025 - Total Variance:  0.4720\n",
      "PC83 - Variance explained:  0.0024 - Total Variance:  0.4744\n",
      "PC84 - Variance explained:  0.0024 - Total Variance:  0.4768\n",
      "PC85 - Variance explained:  0.0024 - Total Variance:  0.4792\n",
      "PC86 - Variance explained:  0.0024 - Total Variance:  0.4816\n",
      "PC87 - Variance explained:  0.0023 - Total Variance:  0.4839\n",
      "PC88 - Variance explained:  0.0023 - Total Variance:  0.4862\n",
      "PC89 - Variance explained:  0.0023 - Total Variance:  0.4885\n",
      "PC90 - Variance explained:  0.0023 - Total Variance:  0.4908\n",
      "PC91 - Variance explained:  0.0023 - Total Variance:  0.4931\n",
      "PC92 - Variance explained:  0.0023 - Total Variance:  0.4954\n",
      "PC93 - Variance explained:  0.0022 - Total Variance:  0.4976\n",
      "PC94 - Variance explained:  0.0022 - Total Variance:  0.4998\n",
      "PC95 - Variance explained:  0.0022 - Total Variance:  0.5020\n",
      "PC96 - Variance explained:  0.0022 - Total Variance:  0.5042\n",
      "PC97 - Variance explained:  0.0022 - Total Variance:  0.5064\n",
      "PC98 - Variance explained:  0.0022 - Total Variance:  0.5086\n",
      "PC99 - Variance explained:  0.0021 - Total Variance:  0.5107\n",
      "PC100 - Variance explained:  0.0021 - Total Variance:  0.5128\n",
      "PC101 - Variance explained:  0.0021 - Total Variance:  0.5149\n",
      "PC102 - Variance explained:  0.0021 - Total Variance:  0.5170\n",
      "PC103 - Variance explained:  0.0021 - Total Variance:  0.5191\n",
      "PC104 - Variance explained:  0.0021 - Total Variance:  0.5212\n",
      "PC105 - Variance explained:  0.0020 - Total Variance:  0.5232\n",
      "PC106 - Variance explained:  0.0020 - Total Variance:  0.5252\n",
      "PC107 - Variance explained:  0.0020 - Total Variance:  0.5273\n",
      "PC108 - Variance explained:  0.0020 - Total Variance:  0.5293\n",
      "PC109 - Variance explained:  0.0020 - Total Variance:  0.5313\n",
      "PC110 - Variance explained:  0.0020 - Total Variance:  0.5333\n",
      "PC111 - Variance explained:  0.0020 - Total Variance:  0.5353\n",
      "PC112 - Variance explained:  0.0020 - Total Variance:  0.5372\n",
      "PC113 - Variance explained:  0.0020 - Total Variance:  0.5392\n",
      "PC114 - Variance explained:  0.0019 - Total Variance:  0.5411\n",
      "PC115 - Variance explained:  0.0019 - Total Variance:  0.5430\n",
      "PC116 - Variance explained:  0.0019 - Total Variance:  0.5449\n",
      "PC117 - Variance explained:  0.0019 - Total Variance:  0.5468\n",
      "PC118 - Variance explained:  0.0019 - Total Variance:  0.5487\n",
      "PC119 - Variance explained:  0.0019 - Total Variance:  0.5506\n",
      "PC120 - Variance explained:  0.0019 - Total Variance:  0.5524\n",
      "PC121 - Variance explained:  0.0019 - Total Variance:  0.5543\n",
      "PC122 - Variance explained:  0.0018 - Total Variance:  0.5561\n",
      "PC123 - Variance explained:  0.0018 - Total Variance:  0.5580\n",
      "PC124 - Variance explained:  0.0018 - Total Variance:  0.5598\n",
      "PC125 - Variance explained:  0.0018 - Total Variance:  0.5616\n",
      "PC126 - Variance explained:  0.0018 - Total Variance:  0.5634\n",
      "PC127 - Variance explained:  0.0018 - Total Variance:  0.5652\n",
      "PC128 - Variance explained:  0.0018 - Total Variance:  0.5670\n",
      "PC129 - Variance explained:  0.0018 - Total Variance:  0.5687\n",
      "PC130 - Variance explained:  0.0018 - Total Variance:  0.5705\n",
      "PC131 - Variance explained:  0.0017 - Total Variance:  0.5722\n",
      "PC132 - Variance explained:  0.0017 - Total Variance:  0.5739\n",
      "PC133 - Variance explained:  0.0017 - Total Variance:  0.5757\n",
      "PC134 - Variance explained:  0.0017 - Total Variance:  0.5774\n",
      "PC135 - Variance explained:  0.0017 - Total Variance:  0.5790\n",
      "PC136 - Variance explained:  0.0017 - Total Variance:  0.5807\n",
      "PC137 - Variance explained:  0.0017 - Total Variance:  0.5824\n",
      "PC138 - Variance explained:  0.0017 - Total Variance:  0.5841\n",
      "PC139 - Variance explained:  0.0017 - Total Variance:  0.5858\n",
      "PC140 - Variance explained:  0.0017 - Total Variance:  0.5874\n",
      "PC141 - Variance explained:  0.0016 - Total Variance:  0.5890\n",
      "PC142 - Variance explained:  0.0016 - Total Variance:  0.5907\n",
      "PC143 - Variance explained:  0.0016 - Total Variance:  0.5923\n",
      "PC144 - Variance explained:  0.0016 - Total Variance:  0.5939\n",
      "PC145 - Variance explained:  0.0016 - Total Variance:  0.5955\n",
      "PC146 - Variance explained:  0.0016 - Total Variance:  0.5971\n",
      "PC147 - Variance explained:  0.0016 - Total Variance:  0.5987\n",
      "PC148 - Variance explained:  0.0016 - Total Variance:  0.6003\n",
      "PC149 - Variance explained:  0.0016 - Total Variance:  0.6019\n",
      "PC150 - Variance explained:  0.0016 - Total Variance:  0.6034\n",
      "PC151 - Variance explained:  0.0015 - Total Variance:  0.6050\n",
      "PC152 - Variance explained:  0.0015 - Total Variance:  0.6065\n",
      "PC153 - Variance explained:  0.0015 - Total Variance:  0.6080\n",
      "PC154 - Variance explained:  0.0015 - Total Variance:  0.6096\n",
      "PC155 - Variance explained:  0.0015 - Total Variance:  0.6111\n",
      "PC156 - Variance explained:  0.0015 - Total Variance:  0.6126\n",
      "PC157 - Variance explained:  0.0015 - Total Variance:  0.6141\n",
      "PC158 - Variance explained:  0.0015 - Total Variance:  0.6156\n",
      "PC159 - Variance explained:  0.0015 - Total Variance:  0.6170\n",
      "PC160 - Variance explained:  0.0015 - Total Variance:  0.6185\n",
      "PC161 - Variance explained:  0.0015 - Total Variance:  0.6200\n",
      "PC162 - Variance explained:  0.0015 - Total Variance:  0.6215\n",
      "PC163 - Variance explained:  0.0015 - Total Variance:  0.6229\n",
      "PC164 - Variance explained:  0.0014 - Total Variance:  0.6244\n",
      "PC165 - Variance explained:  0.0014 - Total Variance:  0.6258\n",
      "PC166 - Variance explained:  0.0014 - Total Variance:  0.6272\n",
      "PC167 - Variance explained:  0.0014 - Total Variance:  0.6286\n",
      "PC168 - Variance explained:  0.0014 - Total Variance:  0.6301\n",
      "PC169 - Variance explained:  0.0014 - Total Variance:  0.6315\n",
      "PC170 - Variance explained:  0.0014 - Total Variance:  0.6329\n",
      "PC171 - Variance explained:  0.0014 - Total Variance:  0.6343\n",
      "PC172 - Variance explained:  0.0014 - Total Variance:  0.6357\n",
      "PC173 - Variance explained:  0.0014 - Total Variance:  0.6370\n",
      "PC174 - Variance explained:  0.0014 - Total Variance:  0.6384\n",
      "PC175 - Variance explained:  0.0014 - Total Variance:  0.6398\n",
      "PC176 - Variance explained:  0.0014 - Total Variance:  0.6411\n",
      "PC177 - Variance explained:  0.0014 - Total Variance:  0.6425\n",
      "PC178 - Variance explained:  0.0014 - Total Variance:  0.6439\n",
      "PC179 - Variance explained:  0.0013 - Total Variance:  0.6452\n",
      "PC180 - Variance explained:  0.0013 - Total Variance:  0.6465\n",
      "PC181 - Variance explained:  0.0013 - Total Variance:  0.6479\n",
      "PC182 - Variance explained:  0.0013 - Total Variance:  0.6492\n",
      "PC183 - Variance explained:  0.0013 - Total Variance:  0.6505\n",
      "PC184 - Variance explained:  0.0013 - Total Variance:  0.6518\n",
      "PC185 - Variance explained:  0.0013 - Total Variance:  0.6531\n",
      "PC186 - Variance explained:  0.0013 - Total Variance:  0.6544\n",
      "PC187 - Variance explained:  0.0013 - Total Variance:  0.6557\n",
      "PC188 - Variance explained:  0.0013 - Total Variance:  0.6570\n",
      "PC189 - Variance explained:  0.0013 - Total Variance:  0.6582\n",
      "PC190 - Variance explained:  0.0013 - Total Variance:  0.6595\n",
      "PC191 - Variance explained:  0.0013 - Total Variance:  0.6608\n",
      "PC192 - Variance explained:  0.0013 - Total Variance:  0.6620\n",
      "PC193 - Variance explained:  0.0012 - Total Variance:  0.6633\n",
      "PC194 - Variance explained:  0.0012 - Total Variance:  0.6645\n",
      "PC195 - Variance explained:  0.0012 - Total Variance:  0.6657\n",
      "PC196 - Variance explained:  0.0012 - Total Variance:  0.6670\n",
      "PC197 - Variance explained:  0.0012 - Total Variance:  0.6682\n",
      "PC198 - Variance explained:  0.0012 - Total Variance:  0.6694\n",
      "PC199 - Variance explained:  0.0012 - Total Variance:  0.6706\n",
      "PC200 - Variance explained:  0.0012 - Total Variance:  0.6718\n",
      "PC201 - Variance explained:  0.0012 - Total Variance:  0.6730\n",
      "PC202 - Variance explained:  0.0012 - Total Variance:  0.6742\n",
      "PC203 - Variance explained:  0.0012 - Total Variance:  0.6754\n",
      "PC204 - Variance explained:  0.0012 - Total Variance:  0.6766\n",
      "PC205 - Variance explained:  0.0012 - Total Variance:  0.6777\n",
      "PC206 - Variance explained:  0.0012 - Total Variance:  0.6789\n",
      "PC207 - Variance explained:  0.0012 - Total Variance:  0.6801\n",
      "PC208 - Variance explained:  0.0012 - Total Variance:  0.6812\n",
      "PC209 - Variance explained:  0.0011 - Total Variance:  0.6824\n",
      "PC210 - Variance explained:  0.0011 - Total Variance:  0.6835\n",
      "PC211 - Variance explained:  0.0011 - Total Variance:  0.6846\n",
      "PC212 - Variance explained:  0.0011 - Total Variance:  0.6858\n",
      "PC213 - Variance explained:  0.0011 - Total Variance:  0.6869\n",
      "PC214 - Variance explained:  0.0011 - Total Variance:  0.6880\n",
      "PC215 - Variance explained:  0.0011 - Total Variance:  0.6891\n",
      "PC216 - Variance explained:  0.0011 - Total Variance:  0.6902\n",
      "PC217 - Variance explained:  0.0011 - Total Variance:  0.6913\n",
      "PC218 - Variance explained:  0.0011 - Total Variance:  0.6924\n",
      "PC219 - Variance explained:  0.0011 - Total Variance:  0.6935\n",
      "PC220 - Variance explained:  0.0011 - Total Variance:  0.6946\n",
      "PC221 - Variance explained:  0.0011 - Total Variance:  0.6957\n",
      "PC222 - Variance explained:  0.0011 - Total Variance:  0.6967\n",
      "PC223 - Variance explained:  0.0011 - Total Variance:  0.6978\n",
      "PC224 - Variance explained:  0.0011 - Total Variance:  0.6989\n",
      "PC225 - Variance explained:  0.0010 - Total Variance:  0.6999\n",
      "PC226 - Variance explained:  0.0010 - Total Variance:  0.7010\n",
      "PC227 - Variance explained:  0.0010 - Total Variance:  0.7020\n",
      "PC228 - Variance explained:  0.0010 - Total Variance:  0.7030\n",
      "PC229 - Variance explained:  0.0010 - Total Variance:  0.7040\n",
      "PC230 - Variance explained:  0.0010 - Total Variance:  0.7051\n",
      "PC231 - Variance explained:  0.0010 - Total Variance:  0.7061\n",
      "PC232 - Variance explained:  0.0010 - Total Variance:  0.7071\n",
      "PC233 - Variance explained:  0.0010 - Total Variance:  0.7081\n",
      "PC234 - Variance explained:  0.0010 - Total Variance:  0.7091\n",
      "PC235 - Variance explained:  0.0010 - Total Variance:  0.7101\n",
      "PC236 - Variance explained:  0.0010 - Total Variance:  0.7111\n",
      "PC237 - Variance explained:  0.0010 - Total Variance:  0.7120\n",
      "PC238 - Variance explained:  0.0010 - Total Variance:  0.7130\n",
      "PC239 - Variance explained:  0.0010 - Total Variance:  0.7140\n",
      "PC240 - Variance explained:  0.0010 - Total Variance:  0.7150\n",
      "PC241 - Variance explained:  0.0010 - Total Variance:  0.7159\n",
      "PC242 - Variance explained:  0.0010 - Total Variance:  0.7169\n",
      "PC243 - Variance explained:  0.0009 - Total Variance:  0.7178\n",
      "PC244 - Variance explained:  0.0009 - Total Variance:  0.7188\n",
      "PC245 - Variance explained:  0.0009 - Total Variance:  0.7197\n",
      "PC246 - Variance explained:  0.0009 - Total Variance:  0.7206\n",
      "PC247 - Variance explained:  0.0009 - Total Variance:  0.7215\n",
      "PC248 - Variance explained:  0.0009 - Total Variance:  0.7225\n",
      "PC249 - Variance explained:  0.0009 - Total Variance:  0.7234\n",
      "\n",
      "\n",
      "N-fold cross validation of X_train for 1012\n",
      "Random Forest\n",
      " The RVE is:  0.4309844987695156\n",
      " The rmse is:  0.20867997327895102\n",
      " The Correlation Score is: 0.6618 (p-value=0.000000e+00)\n",
      "\n",
      " The Maximum Error is:  0.7816668719623496\n",
      " The Mean Absolute Error is: 0.16484590117633124 \n",
      "\n",
      "KNN\n",
      " The RVE is:  0.6020642959235939\n",
      " The rmse is:  0.17451490393860217\n",
      " The Correlation Score is: 0.7783 (p-value=0.000000e+00)\n",
      "\n",
      " The Maximum Error is:  1.0\n",
      " The Mean Absolute Error is: 0.12601007097216846 \n",
      "\n",
      "SVR\n",
      " The RVE is:  0.6414153650071439\n",
      " The rmse is:  0.16572330554320724\n",
      " The Correlation Score is: 0.8021 (p-value=0.000000e+00)\n",
      "\n",
      " The Maximum Error is:  0.8744093289073627\n",
      " The Mean Absolute Error is: 0.12657557017235707 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = np.arange(0.05, 0.16, 0.02)\n",
    "\n",
    "n_components_pca = [30, 60, 100]\n",
    "\n",
    "# for t in threshold:\n",
    "    \n",
    "#     features_to_remove = features_to_remove_with_threshold(t)\n",
    "#     X_train_cut= np.delete(X_train_scaled, features_to_remove, 1)\n",
    "#     X_train_rffs = featureSelect(X_train_cut, X_train_cut.shape[1], .003)\n",
    "    \n",
    "#     for i in n_components_pca:\n",
    "        # pca = pca(X_train_scaled, i) \n",
    "        # X_train_pca = pca.transform(X_train_scaled)\n",
    "\n",
    "N,M = X_train_scaled.shape\n",
    "\n",
    "features_to_remove = features_to_remove_with_threshold(0.03)\n",
    "X_train_cut= np.delete(X_train_scaled, features_to_remove, 1)\n",
    "# X_train_rffs = featureSelect(X_train_cut, X_train_cut.shape[1], .003)\n",
    "# X_train_rffs = featureSelect(X_train_scaled, M, .003)\n",
    "X_train_pca = pca(X_train_cut, 250)\n",
    "\n",
    "\n",
    "print(f\"\\nN-fold cross validation of X_train for {X_train_cut.shape[1]}\")\n",
    "print(\"Random Forest\")\n",
    "RFR_mdl = RandomForestRegressor(n_estimators=10, random_state=0, min_samples_leaf=3, max_depth = 8, n_jobs=6)\n",
    "nfold_valid(X_train_pca, y_train, RFR_mdl)\n",
    "\n",
    "# print(\"Linear Regressor\")\n",
    "# LR_mdl = LinearRegression(n_jobs=6)\n",
    "# nfold_valid(X_train_pca, y_train, LR_mdl)  \n",
    "\n",
    "# print(\"Decision Tree\")\n",
    "# DTR_mdl = DecisionTreeRegressor(max_depth=5)\n",
    "# nfold_valid(X_train_pca, y_train, DTR_mdl)\n",
    "\n",
    "print(\"KNN\")\n",
    "KNN_mdl = KNeighborsRegressor(n_jobs=6)\n",
    "nfold_valid(X_train_pca, y_train, KNN_mdl)\n",
    "\n",
    "print(\"SVR\")\n",
    "SVR_mdl = SVR()\n",
    "nfold_valid(X_train_pca, y_train, SVR_mdl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
